{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJHy04_R86Xg"
      },
      "outputs": [],
      "source": [
        "!pip install spacy==2.3.4\n",
        "!python -m spacy download pt_core_news_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NswmrXxt890p",
        "outputId": "66ddb362-e95f-4cc2-ab72-0a1740f1a692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 17.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.3.4\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import re\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import json\n",
        "from random import randint\n",
        "from spacy.matcher import Matcher\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from tqdm import tqdm, trange\n",
        "import nltk\n",
        "import string\n",
        "from datetime import datetime, timezone\n",
        "from lxml.html.clean import Cleaner\n",
        "cleaner = Cleaner(page_structure=True)\n",
        "\n",
        "!pip install Unidecode\n",
        "from unidecode import unidecode\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "\n",
        "from pathlib import Path\n",
        "diretorio = Path('/content/gdrive/My Drive/NER/DataFrames/URLS')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yC2qREY5Dsi"
      },
      "source": [
        "# DataFrames com URLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axMgQVtbsOaH"
      },
      "outputs": [],
      "source": [
        "df_sentences = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuN3V3VfDslv"
      },
      "outputs": [],
      "source": [
        "df_sentences['preço NER'] = 'NENHUM'\n",
        "df_sentences['dorms NER'] = 'NENHUM'\n",
        "df_sentences['quartos NER'] = 'NENHUM'\n",
        "df_sentences['vagas NER'] = 'NENHUM'\n",
        "df_sentences['area NER'] = 'NENHUM'\n",
        "df_sentences['suites NER'] = 'NENHUM'\n",
        "df_sentences['banheiros NER'] = 'NENHUM'\n",
        "\n",
        "df_sentences['preco REGEX'] = 'NENHUM'\n",
        "df_sentences['quartos REGEX'] = 'NENHUM'\n",
        "df_sentences['dorms REGEX'] = 'NENHUM'\n",
        "df_sentences['vagas REGEX'] = 'NENHUM'\n",
        "df_sentences['area REGEX'] = 'NENHUM'\n",
        "df_sentences['suites REGEX'] = 'NENHUM'\n",
        "df_sentences['banheiros REGEX'] = 'NENHUM'\n",
        "\n",
        "df_sentences['preco REGEX correct'] = 'NENHUM'\n",
        "df_sentences['quartos REGEX correct'] = 'NENHUM'\n",
        "df_sentences['dorms REGEX correct'] = 'NENHUM'\n",
        "df_sentences['vagas REGEX correct'] = 'NENHUM'\n",
        "df_sentences['area REGEX correct'] = 'NENHUM'\n",
        "df_sentences['area priv REGEX correct'] = 'NENHUM'\n",
        "df_sentences['suites REGEX correct'] = 'NENHUM'\n",
        "df_sentences['banheiros REGEX correct'] = 'NENHUM'\n",
        "\n",
        "df_sentences['only area REGEX'] = 'NENHUM'\n",
        "df_sentences['only area priv REGEX'] = 'NENHUM'\n",
        "df_sentences['only area terreno REGEX'] = 'NENHUM'\n",
        "df_sentences['only dorms REGEX'] = 'NENHUM'\n",
        "df_sentences['only quartos REGEX'] = 'NENHUM'\n",
        "df_sentences['only suites REGEX'] = 'NENHUM'\n",
        "df_sentences['only vagas REGEX'] = 'NENHUM'\n",
        "df_sentences['only banheiros REGEX'] = 'NENHUM'\n",
        "df_sentences['only garagem REGEX'] = 'NENHUM'\n",
        "df_sentences['only carros REGEX'] = 'NENHUM'\n",
        "\n",
        "df_sentences['only dorms REGEX 2'] = 'NENHUM'\n",
        "df_sentences['only quartos REGEX 2'] = 'NENHUM'\n",
        "df_sentences['only suites REGEX 2'] = 'NENHUM'\n",
        "df_sentences['only vagas REGEX 2'] = 'NENHUM'\n",
        "df_sentences['only banheiros REGEX 2'] = 'NENHUM'\n",
        "df_sentences['only garagem REGEX 2'] = 'NENHUM'\n",
        "df_sentences['only carros REGEX 2'] = 'NENHUM'\n",
        "\n",
        "df_sentences['only dorms REGEX 3'] = 'NENHUM'\n",
        "df_sentences['only quartos REGEX 3'] = 'NENHUM'\n",
        "df_sentences['only suites REGEX 3'] = 'NENHUM'\n",
        "df_sentences['only vagas REGEX 3'] = 'NENHUM'\n",
        "df_sentences['only banheiros REGEX 3'] = 'NENHUM'\n",
        "df_sentences['only garagem REGEX 3'] = 'NENHUM'\n",
        "df_sentences['only carros REGEX 3'] = 'NENHUM'\n",
        "\n",
        "df_sentences['dorms REGEX sentence'] = 'NENHUM'\n",
        "df_sentences['quartos REGEX sentence'] = 'NENHUM'\n",
        "df_sentences['suites REGEX sentence'] = 'NENHUM'\n",
        "df_sentences['vagas REGEX sentence'] = 'NENHUM'\n",
        "df_sentences['carro REGEX sentence'] = 'NENHUM'\n",
        "df_sentences['garagem REGEX sentence'] = 'NENHUM'\n",
        "df_sentences['banheiros REGEX sentence'] = 'NENHUM'\n",
        "\n",
        "df_sentences['dorms unique REGEX'] = 'NENHUM'\n",
        "df_sentences['quartos unique REGEX'] = 'NENHUM'\n",
        "df_sentences['suites unique REGEX'] = 'NENHUM'\n",
        "df_sentences['vagas unique REGEX'] = 'NENHUM'\n",
        "df_sentences['banheiros unique REGEX'] = 'NENHUM'\n",
        "df_sentences['garagem unique REGEX'] = 'NENHUM'\n",
        "df_sentences['carros unique REGEX'] = 'NENHUM'\n",
        "#df_sentences['box unique REGEX'] = 'NENHUM'\n",
        "\n",
        "df_sentences['Dorms Show'] = 'NENHUM'\n",
        "df_sentences['Quartos Show'] = 'NENHUM'\n",
        "df_sentences['Suites Show'] = 'NENHUM'\n",
        "df_sentences['Vagas Show'] = 'NENHUM'\n",
        "df_sentences['Banheiros Show'] = 'NENHUM'\n",
        "df_sentences['Garagem Show'] = 'NENHUM'\n",
        "df_sentences['Carros Show'] = 'NENHUM'\n",
        "\n",
        "df_sentences['Area Show'] = 'NENHUM'\n",
        "df_sentences['Area Priv Show'] = 'NENHUM'\n",
        "df_sentences['Area Terreno Show'] = 'NENHUM'\n",
        "df_sentences['Area Global Show'] = 'NENHUM'\n",
        "df_sentences['Area Útil Show'] = 'NENHUM'\n",
        "df_sentences['Area Construída Show'] = 'NENHUM'\n",
        "df_sentences['Area Total Show'] = 'NENHUM'\n",
        "\n",
        "df_sentences['Dorms OU'] = 'NENHUM'\n",
        "df_sentences['Quartos OU'] = 'NENHUM'\n",
        "df_sentences['Suites OU'] = 'NENHUM'\n",
        "df_sentences['Banheiros OU'] = 'NENHUM'\n",
        "df_sentences['Vagas OU'] = 'NENHUM'\n",
        "df_sentences['Garagem OU'] = 'NENHUM'\n",
        "df_sentences['Carros OU'] = 'NENHUM'\n",
        "df_sentences['Box OU'] = 'NENHUM'\n",
        "\n",
        "df_sentences['Preço All'] ='NENHUM'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRLYQkPadU7G"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('pt_core_news_lg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2E_GShgrD0d",
        "outputId": "f1d2d98e-bc42-45ea-b23f-9a0973572b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/_ml.py:304: UserWarning: [W019] Changing vectors name from pt_core_news_lg.vectors to pt_core_news_lg.vectors_0, to avoid clash with previously loaded vectors. See Issue #3853.\n",
            "  warnings.warn(Warnings.W019.format(old=old_name, new=new_name))\n"
          ]
        }
      ],
      "source": [
        "nlp4 = spacy.load('/content/gdrive/My Drive/NER/spacy_NER4')\n",
        "nlp5 = spacy.load('/content/gdrive/My Drive/NER/spacy_NER5')\n",
        "nlp6 = spacy.load('/content/gdrive/My Drive/NER/spacy_NER6')\n",
        "nlp7 = spacy.load('/content/gdrive/My Drive/NER/spacy_NER7')\n",
        "nlp71 = spacy.load('/content/gdrive/My Drive/NER/spacy_NER7.1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHlHEvO89BFQ"
      },
      "outputs": [],
      "source": [
        "nlp8 = spacy.load('/content/gdrive/My Drive/NER/spacy_NER8') #preço\n",
        "nlp9 = spacy.load('/content/gdrive/My Drive/NER/spacy_NER9') #dorms e quartos\n",
        "nlp10 = spacy.load('/content/gdrive/My Drive/NER/spacy_NER10') #vagas\n",
        "nlp11 = spacy.load('/content/gdrive/My Drive/NER/spacy_NER11') #área\n",
        "nlp12 = spacy.load('/content/gdrive/My Drive/NER/spacy_NER12') #suítes e banheiros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EktlSO82SWPs"
      },
      "source": [
        "# Funções para executar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J20-9PlVFNcT"
      },
      "outputs": [],
      "source": [
        "def returnComodosJuntos(s):\n",
        "  s = re.sub(',', '', re.sub('\\s+', ' ', unidecode(s.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0',''))))\n",
        "  list_correct = []\n",
        "  for regex in ['\\w+\\s*\\d{1,2}\\s*', '\\d{1,2}\\s*\\w+\\s*']:\n",
        "    rgx = regex\n",
        "    for qnt_vezes in range(1,5):\n",
        "      rgx += regex\n",
        "      values = re.findall(rgx, s)\n",
        "      for v in values:\n",
        "        v = v.strip()\n",
        "        cont_ok = 0\n",
        "        list_tags_ok = []\n",
        "        for tag in ['dorm', 'suit', 'vaga', 'banheiro', 'quarto', 'garage']: #palavras chaves que estou procurando\n",
        "          #if tag in v: cont_ok +=1\n",
        "          #verifica se sobra letras/palavras além das tags procuradas. Se retornar, então não é válido\n",
        "          if tag in v:\n",
        "            list_tags_ok.append(tag)\n",
        "            cont_ok +=1\n",
        "        sr = v\n",
        "        for tags_ok in list_tags_ok:\n",
        "          sr = re.sub(tags_ok+'\\w*\\D', '', sr)\n",
        "        sr = re.sub('\\s+', '', sr)\n",
        "        if len(re.findall('\\D', sr)) > 0:\n",
        "          #print(\"OPAAAAAA, NÃO DÁ\", v, \"SR\", sr)\n",
        "          cont_ok = -1\n",
        "        list_correct.append((cont_ok, v, regex))\n",
        "\n",
        "  list_correct.sort(reverse=True)\n",
        "  #print(\"correct\", list_correct)\n",
        "  dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quarto_rgx_only, garagem_rgx_only = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM'\n",
        "  frase_correct, regex_correct = '',''\n",
        "  if len(list_correct) > 0:\n",
        "    frase_correct = list_correct[0][1]\n",
        "    regex_correct = list_correct[0][2]\n",
        "    if regex_correct == '\\\\d{1,2}\\\\s*\\\\w+\\\\s*':\n",
        "      if (re.findall('\\d{1,2}\\s*dorm', frase_correct)) != []:\n",
        "        dorms_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*dorm', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*suit', frase_correct)) != []:\n",
        "        suites_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*suit', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*vaga', frase_correct)) != []:\n",
        "        vagas_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*vaga', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*banheiro', frase_correct)) != []:\n",
        "        banheiro_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*banheiro', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*quarto', frase_correct)) != []:\n",
        "        quarto_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*quarto', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*garage', frase_correct)) != []:\n",
        "        garagem_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*garage', frase_correct)[0])[0]\n",
        "    elif regex_correct == '\\\\w+\\\\s*\\\\d{1,2}\\\\s*':\n",
        "      if (re.findall('dorm\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        dorms_rgx_only = re.findall('\\d{1,2}', re.findall('dorm\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('suit\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        suites_rgx_only = re.findall('\\d{1,2}', re.findall('suit\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('vaga\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        vagas_rgx_only = re.findall('\\d{1,2}', re.findall('vaga\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('banheiro\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        banheiro_rgx_only = re.findall('\\d{1,2}', re.findall('banheiro\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('quarto\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        quarto_rgx_only = re.findall('\\d{1,2}', re.findall('quarto\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('garage\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        garagem_rgx_only = re.findall('\\d{1,2}', re.findall('garage\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "    \n",
        "  return frase_correct, regex_correct, dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quarto_rgx_only, garagem_rgx_only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2_pPBjhSeHO"
      },
      "outputs": [],
      "source": [
        "def indicesArea(s):\n",
        "  doc = nlp8(s)\n",
        "\n",
        "  indices_area = []\n",
        "  for regex_area in ['\\d+.?,?\\d+?\\s*?m²', '\\d+.?,?\\d+?\\s*?m2', '\\d+.?,?\\d+?\\s*?m ²', '\\d+.?,?\\d+?\\s*?m 2']:\n",
        "    for match in re.finditer(regex_area, doc.text):\n",
        "      start, end = match.span()\n",
        "      span = doc.char_span(start, end, label='area')\n",
        "      if span:\n",
        "        indices_area += range(start, end+1)\n",
        "  return indices_area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dO26k9cShlV"
      },
      "outputs": [],
      "source": [
        "def indicesPreco(s):\n",
        "  doc = nlp8(s)\n",
        "\n",
        "  indices_preco = []\n",
        "  for match in re.finditer('\\(?r\\$\\)?[:\\s*]*\\d+[.,]?[\\d+]*?[.,]?[\\d+]*[.,]?[\\d+]*[.,]?[\\d+]*', doc.text):\n",
        "    start, end = match.span()\n",
        "    span = doc.char_span(start, end, label='preco')\n",
        "    if span:\n",
        "      indices_preco += range(start, end+1)\n",
        "  return indices_preco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF7q6uitSmr3"
      },
      "outputs": [],
      "source": [
        "#AAAAAAAAQQQQQQQQQUIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII ------------------------------\n",
        "def comodosUnique(s):\n",
        "  s = re.sub('\\s+', ' ', unidecode(s.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0','').replace('.', ' .')))\n",
        "  doc = nlp8(s)\n",
        "  for tag in ['dorm', 'suit', 'vaga', 'banheiro', 'quarto', 'garage', 'carro', 'box']:\n",
        "    regex_tag = '\\w*?'+tag+'\\w*'\n",
        "    list_values_rigth, list_values_left = [], []\n",
        "    parte_direita_numero, parte_esquerda_numero = 'NENHUM', 'NENHUM'\n",
        "    number_direita, number_esquerda = [], []\n",
        "    indices_area = indicesArea(s)\n",
        "    indices_preco = indicesPreco(s)\n",
        "    value_dorms_unique = 'NENHUM'\n",
        "    for match in re.finditer(regex_tag, doc.text):\n",
        "      start, end = match.span()\n",
        "      span = doc.char_span(start, end, label=tag)\n",
        "      if span:\n",
        "        #print(span)\n",
        "        for k in range(1,10):\n",
        "          if re.findall('\\d+', s[start:end]) != []:                 #dorms2\n",
        "            value_dorms_unique = re.findall('\\d+', s[start:end])[0]\n",
        "            break\n",
        "          if end+k > len(s)-1:break\n",
        "          frase = s[start:end+k]\n",
        "          #print(\"frase\", frase, frase[-1])\n",
        "          if frase[-1] == '.' or frase[-1] == ',':\n",
        "            break\n",
        "          if frase[-1].isspace() == False:\n",
        "            if frase[-1].isdigit() == True and s[start:end+k+1][-1].isdigit() == False:  #dorms 2\n",
        "              if end+k in indices_preco or end+k in indices_area: break\n",
        "              number_direita.append((re.findall('\\d+', s[start:end+k])[0], [x for x in range(start, end+k+1)]))\n",
        "              #print(s[start:end+k])\n",
        "              parte_direita_numero = 'ok'\n",
        "            elif frase[-1].isdigit() == True and s[start:end+k+1][-1].isdigit() == True: #dorms 02\n",
        "              if end+k in indices_preco or end+k in indices_area: break\n",
        "              number_direita.append((re.findall('\\d+', s[start:end+k+1])[0], [x for x in range(start,end+k+2)]))\n",
        "              #print(s[start:end+k+1])\n",
        "              parte_direita_numero = 'ok'\n",
        "            break\n",
        "        for j in range(1,10):\n",
        "          if start-j < 0:break\n",
        "          if start-j-1 < 0:j +=1\n",
        "          frase = s[start-j:end]\n",
        "          if len(frase) == 0:break\n",
        "          if frase[0] == '.' or frase[0] == ',':break\n",
        "          if frase[0].isspace() == False:\n",
        "            if start-j in indices_preco or start-j in indices_area: break\n",
        "            if frase[0].isdigit() == True and s[start-j-1:end][0].isdigit() == False:  #2 dorms \n",
        "              number_esquerda.append( (re.findall('\\d+', s[start-j:end])[0], [x for x in range(start-j, end+1)]))\n",
        "              #print(s[start-j:end])\n",
        "              parte_esquerda_numero = 'ok'\n",
        "            if frase[0].isdigit() == True and s[start-j-1:end][0].isdigit() == True:  #02 dorms \n",
        "              if start-j in indices_preco or start-j in indices_area: break\n",
        "              number_esquerda.append( (re.findall('\\d+', s[start-j-1:end])[0], [x for x in range(start-j-1, end+1)]))\n",
        "              #print(s[start-j-1:end])\n",
        "              parte_esquerda_numero = 'ok'\n",
        "            break\n",
        "    \n",
        "    number_unique_ok = 'NENHUM'\n",
        "\n",
        "    dorms_unique, quartos_unique, suites_unique, vagas_unique, banheiros_unique, garagem_unique, carros_unique, box_unique = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM'\n",
        "\n",
        "    if value_dorms_unique != 'NENHUM' and parte_esquerda_numero != 'ok':\n",
        "      number_unique_ok = value_dorms_unique\n",
        "\n",
        "    if number_direita != [] and number_esquerda == []:\n",
        "      #number_unique_ok = number_direita[0]\n",
        "      for v in number_direita:\n",
        "        v = v[0]\n",
        "        if number_unique_ok == 'NENHUM':number_unique_ok = v\n",
        "        else: number_unique_ok +='&-&'+v\n",
        "    elif number_direita == [] and number_esquerda != []:\n",
        "      #number_unique_ok = number_esquerda[0]\n",
        "      for v in number_esquerda:\n",
        "        v = v[0]\n",
        "        if number_unique_ok == 'NENHUM':number_unique_ok = v\n",
        "        else: number_unique_ok +='&-&'+v\n",
        "    elif number_direita != [] and number_esquerda != []:\n",
        "      #print(number_direita, number_esquerda)\n",
        "      list_indices_inserido, list_final = [],[]\n",
        "      for v1 in number_direita:\n",
        "        dentro = 0\n",
        "        for v2 in number_esquerda:\n",
        "          if (v1[1][0] in v2[1] or v1[1][-1] in v2[1]):\n",
        "            dentro = 1\n",
        "            break\n",
        "        if dentro == 0:\n",
        "          list_indices_inserido.append(v1[1])\n",
        "          list_final.append((v1[0], v1[1]))\n",
        "          #print(\"ADD\", v1[0], v1[1])\n",
        "          for b1 in number_esquerda:\n",
        "            dentro = 0\n",
        "            #print(list_indices_inserido)\n",
        "            for b2 in number_direita:\n",
        "              if b1[1][0] in b2[1] or b1[1][-1] in b2[1] or b1[1] in list_indices_inserido:\n",
        "                dentro = 1\n",
        "                break\n",
        "            if dentro == 0:\n",
        "              #print(\"ADD\", b1[0], b1[1])\n",
        "              list_final.append((b1[0], b1[1]))\n",
        "              break\n",
        "          #print(\"ok\", b1[1][0], b1[1][-1], b2[1], b1[0], b2[0])\n",
        "        else:\n",
        "          continue\n",
        "      #print(list_final)\n",
        "      list_final = [t[0] for t in list(set([( x[0], str(x[1])) for x in list_final]))]\n",
        "\n",
        "      for v in list_final:\n",
        "        if number_unique_ok == 'NENHUM':number_unique_ok = v\n",
        "        else: number_unique_ok +='&-&'+v\n",
        "      #print(list_final)\n",
        "\n",
        "    if tag == 'dorm' and number_unique_ok != 'NENHUM':dorms_unique = number_unique_ok\n",
        "    if tag == 'quarto' and number_unique_ok != 'NENHUM':quartos_unique = number_unique_ok\n",
        "    if tag == 'banheiro' and number_unique_ok != 'NENHUM':banheiros_unique = number_unique_ok\n",
        "    if tag == 'suit' and number_unique_ok != 'NENHUM':suites_unique = number_unique_ok\n",
        "    if tag == 'vaga' and number_unique_ok != 'NENHUM':vagas_unique = number_unique_ok\n",
        "    if tag == 'garage' and number_unique_ok != 'NENHUM':garagem_unique = number_unique_ok\n",
        "    if tag == 'carro' and number_unique_ok != 'NENHUM':carros_unique = number_unique_ok\n",
        "    if tag == 'box' and number_unique_ok != 'NENHUM':box_unique = number_unique_ok\n",
        "\n",
        "    #print(tag, number_unique_ok)\n",
        "    return dorms_unique, quartos_unique, suites_unique, vagas_unique, banheiros_unique, garagem_unique, carros_unique, box_unique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvVKze7CSqsd"
      },
      "outputs": [],
      "source": [
        "list_tags_search = ['dorm', 'suit', 'vaga', 'banheiro', 'quarto', 'garage', 'carro', 'box']\n",
        "def verify_tags_and_numbers(s, word):\n",
        "  for tag in list_tags_search:\n",
        "    if tag in word: \n",
        "      #print(\"word\", word)\n",
        "      return 1\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f9HHgkSYrBu"
      },
      "outputs": [],
      "source": [
        "list_tags_search = ['dorm', 'suit', 'vaga', 'banheiro', 'quarto', 'garage', 'carro', 'box']\n",
        "def verify_tags_and_numbers(s, word):\n",
        "  for tag in list_tags_search:\n",
        "    if tag in word: \n",
        "      #print(\"word\", word)\n",
        "      return 1\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFD-sn2OSvnV"
      },
      "outputs": [],
      "source": [
        "def comodosChavesJuntos(s):\n",
        "  indices_preco, indices_area = [], []\n",
        "  s = re.sub('\\s+', ' ', unidecode(s.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0','')))\n",
        "  list_tags_search = ['dorm', 'suit', 'vaga', 'banheiro', 'quarto', 'garage', 'carro', 'box']\n",
        "  list_return_frase = []\n",
        "  doc = nlp8(s) \n",
        "  indices_preco = indicesPreco(s)\n",
        "  indices_area = indicesArea(s)\n",
        "  for tag in list_tags_search:\n",
        "    regex_tag = tag+'\\w*'\n",
        "    number_direita, number_esquerda = [], []\n",
        "    for match in re.finditer(regex_tag, doc.text):\n",
        "      start, end = match.span()\n",
        "      span = doc.char_span(start, end, label=tag)\n",
        "      parte_direita_numero, parte_esquerda_numero = [], []\n",
        "      if span:\n",
        "        #print(\"SPAN\", span)\n",
        "        for k in range(1,50):                           #NÚMERO PARA A DIREITA\n",
        "          if re.findall('\\d+', s[start:end]) != []:\n",
        "            parte_direita_numero.append((start, end))\n",
        "            break\n",
        "          if end+k > len(s)-1:break\n",
        "          frase = s[start:end+k]\n",
        "          #print(\"frase 1\", frase)\n",
        "          if frase[-1] == '.' or frase[-1] == ',':break\n",
        "          if frase[-1].isspace() == False:\n",
        "            if frase[-1].isdigit() == True and s[start:end+k+1][-1].isdigit() == False:  #dorms 2\n",
        "              if end+k in indices_preco or end+k in indices_area: break\n",
        "              parte_direita_numero.append((start, end+k))\n",
        "            elif frase[-1].isdigit() == True and s[start:end+k+1][-1].isdigit() == True: #dorms 02\n",
        "              if end+k in indices_preco or end+k in indices_area: break\n",
        "              parte_direita_numero.append((start, end+k+1))\n",
        "            break\n",
        "        for j in range(1,50):\n",
        "          if start-j < 0:break\n",
        "          if start-j-1 < 0: j+=1\n",
        "          frase = s[start-j:end]\n",
        "          #print(\"frase2\", frase)\n",
        "          try:\n",
        "            if frase[0] == '.' or frase[0] == ',':break\n",
        "          except:break\n",
        "          if frase[0].isspace() == False:\n",
        "            if start-j in indices_preco or start-j in indices_area: break\n",
        "            if frase[0].isdigit() == True and s[start-j-1:end][0].isdigit() == False:  #2 dorms \n",
        "              parte_esquerda_numero.append( (start-j, end) )\n",
        "            if frase[0].isdigit() == True and s[start-j-1:end][0].isdigit() == True:  #02 dorms \n",
        "              parte_esquerda_numero.append( (start-j-2, end) )\n",
        "            break\n",
        "      #print(parte_direita_numero, parte_esquerda_numero)\n",
        "      if parte_direita_numero != [] and parte_esquerda_numero != []:\n",
        "        #s = s.replace(',','').replace('.','')\n",
        "        inicio = parte_esquerda_numero[0][0]\n",
        "        inicio_frase = inicio\n",
        "        fim = parte_esquerda_numero[0][1]\n",
        "        fim_parte_direita = parte_direita_numero[0][1]\n",
        "        fim2 = fim_parte_direita\n",
        "        #print(\"ref\", s[inicio:fim2])\n",
        "        indice_space, space = inicio, 0\n",
        "        inicio_frase = -1\n",
        "        #inicio_frase = inicio\n",
        "        k = 0\n",
        "        for _ in range(1,25):   #para trás\n",
        "          brk = 0\n",
        "          k +=1\n",
        "          if inicio-k < 0:break\n",
        "          if inicio_frase != -1:inicio = inicio_frase\n",
        "          if s[inicio-k].isspace():space +=1 #CONFERIR SE TEM UMA OUTRA PALAVRA ANTES DO NÚMERO ANTERIOR\n",
        "          if s[inicio-k]==',' or s[inicio-k] == '.':\n",
        "            inicio_frase = inicio - k\n",
        "            break\n",
        "          if verify_tags_and_numbers(s, s[inicio-k:inicio]) == 1:\n",
        "            space_ant = 0\n",
        "            for j in range(1,5):\n",
        "              if s[inicio-k-j].isdigit():\n",
        "                inicio_frase = inicio-k-j\n",
        "                #print(\"ENCONTROU NÚMERO\", s[inicio_frase:fim])\n",
        "                k = 0\n",
        "                break\n",
        "              if s[inicio-k-j].isspace():\n",
        "                space_ant +=1\n",
        "              if space_ant == 1 and re.findall('\\w', s[inicio-k-j]) != []:\n",
        "                #print(\"NÂO TEM NÚMERO ANTES\", s[inicio-k:fim])\n",
        "                inicio_frase = inicio-k\n",
        "                brk = 1\n",
        "                break\n",
        "            if brk == 1:\n",
        "              break\n",
        "        fim_frase = -1\n",
        "        digit_next, space_next, f, fim2 = 0, 0, -1, fim_parte_direita+1\n",
        "        #print(\"inicio\", s[inicio:fim2])\n",
        "        for _ in range(1,25): #para frente\n",
        "          f +=1\n",
        "          number_with_word = 0\n",
        "          if fim2+f > len(s) -1:\n",
        "            #print(fim2+1, len(s))\n",
        "            break\n",
        "          if s[fim2+f].isspace():\n",
        "            space_next +=1\n",
        "          \n",
        "          #print(\"--\", s[fim2:fim2+f+1])\n",
        "          #if ' ' in s[fim2:fim2+f+1]:\n",
        "          #  if verify_tags_and_numbers(s, s[fim2:fim2+f+1]) == 0:\n",
        "          #    print(fim_parte_direita, fim2)\n",
        "          #    while fim_parte_direita >= fim2:\n",
        "          #      fim2 -=1\n",
        "          #    print(s[start:end])\n",
        "          #    while  s[fim2] != s[start:end][-1]: fim2 -=1\n",
        "          #    print(\"break aqio\")\n",
        "          #    break\n",
        "          #if space_next >= 2 and verify_tags_and_numbers(s, s[fim2:fim2+f+1]) == 0:\n",
        "          #  fim2 -=1\n",
        "            #print(\"break aqui\")\n",
        "          #  break\n",
        "\n",
        "          if space_next == 1 and re.findall('\\d', s[fim2:fim2+f+1]) != []:\n",
        "            #print(\"aqui\", s[fim2:fim2+f+1])\n",
        "            number_with_word = 1\n",
        "          if (number_with_word == 1 or s[fim2+f].isdigit() ):#and verify_tags_and_numbers(s, s[fim2:fim2+f+1]) == 1:\n",
        "            if verify_tags_and_numbers(s, s[fim2:fim2+f+1]) == 1:\n",
        "              #print(\"encontrou\", s[fim2:fim2+f+1])\n",
        "              fim2 = fim2+f+1\n",
        "              f = 0 #carros 3\n",
        "              space_next = 0\n",
        "            else:break\n",
        "\n",
        "        #while re.findall('\\w', s[inicio_frase:fim2][-1]) != [] and s[inicio_frase:fim2].strip()[0].isdigit():\n",
        "        #  fim2 -=1\n",
        "        #if s[inicio_frase:fim2].strip()[0].isdigit():\n",
        "        #  space = 0\n",
        "        #  for p in range(1,20):\n",
        "        #    if fim2+p+1 > len(s)-1:break\n",
        "        #    if s[fim2+p].isspace(): space +=1\n",
        "        #    if space >= 1:\n",
        "        #      if verify_tags_and_numbers(s, s[fim2:fim2+p+1]) == 0:\n",
        "        #        fim2 -=1\n",
        "        #      break\n",
        "        #print(\"print(\", s[inicio_frase:fim2])\n",
        "        list_return_frase.append( ( s[inicio_frase:fim2], len(s[inicio_frase:fim2]) ) )\n",
        "      #break\n",
        "    #break\n",
        "\n",
        "  dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM'\n",
        "  if len(list_return_frase) == 0: return dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only\n",
        "  try:\n",
        "    list_return_frase.sort(reverse=True)\n",
        "    frase_correct = list_return_frase[0][0].strip()\n",
        "    #if frase_correct[0] == '':return dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only\n",
        "    #if frase_correct[0][0].isdigit() and frase_correct[0][-1].isdigit():\n",
        "      #frase_correct = frase_correct[1:].strip()\n",
        "  except:return dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only\n",
        "  #print(frase_correct)\n",
        "  try:\n",
        "    if frase_correct[0].isdigit():\n",
        "      if (re.findall('\\d{1,2}\\s*dorm', frase_correct)) != []:\n",
        "        dorms_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*dorm', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*suit', frase_correct)) != []:\n",
        "        suites_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*suit', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*vaga', frase_correct)) != []:\n",
        "        vagas_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*vaga', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*banheiro', frase_correct)) != []:\n",
        "        banheiro_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*banheiro', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*quarto', frase_correct)) != []:\n",
        "        quartos_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*quarto', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*garage', frase_correct)) != []:\n",
        "        garagem_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*garage', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*carro', frase_correct)) != []:\n",
        "        carros_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*carro', frase_correct)[0])[0]\n",
        "    else:\n",
        "      if (re.findall('dorm\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        dorms_rgx_only = re.findall('\\d{1,2}', re.findall('dorm\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('suit\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        suites_rgx_only = re.findall('\\d{1,2}', re.findall('suit\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('vaga\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        vagas_rgx_only = re.findall('\\d{1,2}', re.findall('vaga\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('banheiro\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        banheiro_rgx_only = re.findall('\\d{1,2}', re.findall('banheiro\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('quarto\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        quartos_rgx_only = re.findall('\\d{1,2}', re.findall('quarto\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('garage\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        garagem_rgx_only = re.findall('\\d{1,2}', re.findall('garage\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('carro\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        carros_rgx_only = re.findall('\\d{1,2}', re.findall('carro\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "  except:pass\n",
        "  return dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGP2orPaw5hB"
      },
      "outputs": [],
      "source": [
        "def comodosChavesJuntos(s):\n",
        "  indices_preco, indices_area = [], []\n",
        "  s = re.sub('\\s+', ' ', unidecode(s.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0','')))\n",
        "  list_tags_search = ['dorm', 'suit', 'vaga', 'banheiro', 'quarto', 'garage', 'carro', 'box']\n",
        "  list_return_frase = []\n",
        "  doc = nlp8(s) \n",
        "  indices_preco = indicesPreco(s)\n",
        "  indices_area = indicesArea(s)\n",
        "  for tag in list_tags_search:\n",
        "    regex_tag = tag+'\\w*'\n",
        "    number_direita, number_esquerda = [], []\n",
        "    for match in re.finditer(regex_tag, doc.text):\n",
        "      start, end = match.span()\n",
        "      span = doc.char_span(start, end, label=tag)\n",
        "      parte_direita_numero, parte_esquerda_numero = [], []\n",
        "      if span:\n",
        "        #print(\"SPAN\", span)\n",
        "        for k in range(1,50):                           #NÚMERO PARA A DIREITA\n",
        "          if re.findall('\\d+', s[start:end]) != []:\n",
        "            parte_direita_numero.append((start, end))\n",
        "            break\n",
        "          if end+k > len(s)-1:break\n",
        "          frase = s[start:end+k]\n",
        "          #print(\"frase 1\", frase)\n",
        "          if frase[-1] == '.' or frase[-1] == ',':break\n",
        "          if frase[-1].isspace() == False:\n",
        "            if frase[-1].isdigit() == True and s[start:end+k+1][-1].isdigit() == False:  #dorms 2\n",
        "              if end+k in indices_preco or end+k in indices_area: break\n",
        "              parte_direita_numero.append((start, end+k))\n",
        "            elif frase[-1].isdigit() == True and s[start:end+k+1][-1].isdigit() == True: #dorms 02\n",
        "              if end+k in indices_preco or end+k in indices_area: break\n",
        "              parte_direita_numero.append((start, end+k+1))\n",
        "            break\n",
        "        for j in range(1,50):\n",
        "          if start-j < 0:break\n",
        "          if start-j-1 < 0: j+=1\n",
        "          frase = s[start-j:end]\n",
        "          #print(\"frase2\", frase)\n",
        "          try:\n",
        "            if frase[0] == '.' or frase[0] == ',':break\n",
        "          except:break\n",
        "          if frase[0].isspace() == False:\n",
        "            if start-j in indices_preco or start-j in indices_area: break\n",
        "            if frase[0].isdigit() == True and s[start-j-1:end][0].isdigit() == False:  #2 dorms \n",
        "              parte_esquerda_numero.append( (start-j, end) )\n",
        "            if frase[0].isdigit() == True and s[start-j-1:end][0].isdigit() == True:  #02 dorms \n",
        "              parte_esquerda_numero.append( (start-j-2, end) )\n",
        "            break\n",
        "      #print(parte_direita_numero, parte_esquerda_numero)\n",
        "      if parte_direita_numero != [] and parte_esquerda_numero != []:\n",
        "        #s = s.replace(',','').replace('.','')\n",
        "        inicio = parte_esquerda_numero[0][0]\n",
        "        inicio_frase = inicio\n",
        "        fim = parte_esquerda_numero[0][1]\n",
        "        fim_parte_direita = parte_direita_numero[0][1]\n",
        "        fim2 = fim_parte_direita\n",
        "        #print(\"ref\", s[inicio:fim2])\n",
        "        indice_space, space = inicio, 0\n",
        "        inicio_frase = -1\n",
        "        #inicio_frase = inicio\n",
        "        k = 0\n",
        "        for _ in range(1,25):   #para trás\n",
        "          brk = 0\n",
        "          k +=1\n",
        "          if inicio-k < 0:break\n",
        "          if inicio_frase != -1:inicio = inicio_frase\n",
        "          if s[inicio-k].isspace():space +=1 #CONFERIR SE TEM UMA OUTRA PALAVRA ANTES DO NÚMERO ANTERIOR\n",
        "          if s[inicio-k]==',' or s[inicio-k] == '.':\n",
        "            inicio_frase = inicio - k\n",
        "            break\n",
        "          if verify_tags_and_numbers(s, s[inicio-k:inicio]) == 1:\n",
        "            space_ant = 0\n",
        "            for j in range(1,5):\n",
        "              if s[inicio-k-j].isdigit():\n",
        "                inicio_frase = inicio-k-j\n",
        "                #print(\"ENCONTROU NÚMERO\", s[inicio_frase:fim])\n",
        "                k = 0\n",
        "                break\n",
        "              if s[inicio-k-j].isspace():\n",
        "                space_ant +=1\n",
        "              if space_ant == 1 and re.findall('\\w', s[inicio-k-j]) != []:\n",
        "                #print(\"NÂO TEM NÚMERO ANTES\", s[inicio-k:fim])\n",
        "                inicio_frase = inicio-k\n",
        "                brk = 1\n",
        "                break\n",
        "            if brk == 1:\n",
        "              break\n",
        "        fim_frase = -1\n",
        "        digit_next, space_next, f, fim2 = 0, 0, -1, fim_parte_direita+1\n",
        "        #print(\"inicio\", s[inicio:fim2])\n",
        "        for _ in range(1,25): #para frente\n",
        "          f +=1\n",
        "          number_with_word = 0\n",
        "          if fim2+f > len(s) -1:\n",
        "            #print(fim2+1, len(s))\n",
        "            break\n",
        "          if s[fim2+f].isspace():\n",
        "            space_next +=1\n",
        "          \n",
        "          #print(\"--\", s[fim2:fim2+f+1])\n",
        "          #if ' ' in s[fim2:fim2+f+1]:\n",
        "          #  if verify_tags_and_numbers(s, s[fim2:fim2+f+1]) == 0:\n",
        "          #    print(fim_parte_direita, fim2)\n",
        "          #    while fim_parte_direita >= fim2:\n",
        "          #      fim2 -=1\n",
        "          #    print(s[start:end])\n",
        "          #    while  s[fim2] != s[start:end][-1]: fim2 -=1\n",
        "          #    print(\"break aqio\")\n",
        "          #    break\n",
        "          #if space_next >= 2 and verify_tags_and_numbers(s, s[fim2:fim2+f+1]) == 0:\n",
        "          #  fim2 -=1\n",
        "            #print(\"break aqui\")\n",
        "          #  break\n",
        "\n",
        "          if space_next == 1 and re.findall('\\d', s[fim2:fim2+f+1]) != []:\n",
        "            #print(\"aqui\", s[fim2:fim2+f+1])\n",
        "            number_with_word = 1\n",
        "          if (number_with_word == 1 or s[fim2+f].isdigit() ):#and verify_tags_and_numbers(s, s[fim2:fim2+f+1]) == 1:\n",
        "            if verify_tags_and_numbers(s, s[fim2:fim2+f+1]) == 1:\n",
        "              #print(\"encontrou\", s[fim2:fim2+f+1])\n",
        "              fim2 = fim2+f+1\n",
        "              f = 0 #carros 3\n",
        "              space_next = 0\n",
        "            else:break\n",
        "\n",
        "        #while re.findall('\\w', s[inicio_frase:fim2][-1]) != [] and s[inicio_frase:fim2].strip()[0].isdigit():\n",
        "        #  fim2 -=1\n",
        "        #if s[inicio_frase:fim2].strip()[0].isdigit():\n",
        "        #  space = 0\n",
        "        #  for p in range(1,20):\n",
        "        #    if fim2+p+1 > len(s)-1:break\n",
        "        #    if s[fim2+p].isspace(): space +=1\n",
        "        #    if space >= 1:\n",
        "        #      if verify_tags_and_numbers(s, s[fim2:fim2+p+1]) == 0:\n",
        "        #        fim2 -=1\n",
        "        #      break\n",
        "        #print(\"print(\", s[inicio_frase:fim2])\n",
        "        list_return_frase.append( ( s[inicio_frase:fim2], len(s[inicio_frase:fim2]) ) )\n",
        "      #break\n",
        "    #break\n",
        "\n",
        "  dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM'\n",
        "  if len(list_return_frase) == 0: return dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only\n",
        "  try:\n",
        "    list_return_frase.sort(reverse=True)\n",
        "    frase_correct = list_return_frase[0][0].strip()\n",
        "    #if frase_correct[0] == '':return dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only\n",
        "    #if frase_correct[0][0].isdigit() and frase_correct[0][-1].isdigit():\n",
        "      #frase_correct = frase_correct[1:].strip()\n",
        "  except:return dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only\n",
        "  #print(frase_correct)\n",
        "  try:\n",
        "    if frase_correct[0].isdigit():\n",
        "      if (re.findall('\\d{1,2}\\s*dorm', frase_correct)) != []:\n",
        "        dorms_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*dorm', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*suit', frase_correct)) != []:\n",
        "        suites_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*suit', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*vaga', frase_correct)) != []:\n",
        "        vagas_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*vaga', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*banheiro', frase_correct)) != []:\n",
        "        banheiro_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*banheiro', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*quarto', frase_correct)) != []:\n",
        "        quartos_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*quarto', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*garage', frase_correct)) != []:\n",
        "        garagem_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*garage', frase_correct)[0])[0]\n",
        "      if (re.findall('\\d{1,2}\\s*carro', frase_correct)) != []:\n",
        "        carros_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*carro', frase_correct)[0])[0]\n",
        "    else:\n",
        "      if (re.findall('dorm\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        dorms_rgx_only = re.findall('\\d{1,2}', re.findall('dorm\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('suit\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        suites_rgx_only = re.findall('\\d{1,2}', re.findall('suit\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('vaga\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        vagas_rgx_only = re.findall('\\d{1,2}', re.findall('vaga\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('banheiro\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        banheiro_rgx_only = re.findall('\\d{1,2}', re.findall('banheiro\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('quarto\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        quartos_rgx_only = re.findall('\\d{1,2}', re.findall('quarto\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('garage\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        garagem_rgx_only = re.findall('\\d{1,2}', re.findall('garage\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "      if (re.findall('carro\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "        carros_rgx_only = re.findall('\\d{1,2}', re.findall('carro\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "  except:pass\n",
        "  return dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGsGkAYuS0Pt"
      },
      "outputs": [],
      "source": [
        "#sentença extraída normal do list_ner = com espaço -> verificar qual número é mais próximo do cômodo\n",
        "def returnTagsContSpace(s):\n",
        "  s = unidecode(s.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0',''))\n",
        "  doc = nlp8(s)\n",
        "  dorm_rgx_tag_space, suite_rgx_tag_space, vaga_rgx_tag_space, banheiro_rgx_tag_space, quarto_rgx_tag_space, garagem_rgx_tag_space, carro_rgx_tag_space = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM'\n",
        "  for tag in ['dorm', 'suit', 'vaga', 'banheiro', 'quarto', 'garage', 'carro']:\n",
        "    regex_tag = tag+'\\w*'\n",
        "    list_values_rigth, list_values_left = [], []\n",
        "    for match in re.finditer(regex_tag, doc.text):\n",
        "      start, end = match.span()\n",
        "      span = doc.char_span(start, end, label=tag)\n",
        "      if span:\n",
        "        for i in range(1,100): #para a direita\n",
        "          string_frase = s[start:end+i+1]\n",
        "          if end+i >= len(s):break\n",
        "          if s[end+i].isdigit() and s[end+i+1].isdigit() == True:\n",
        "            #if end+i+1 <= len(s)-1:#.isdigit() == True: #vagas 50\n",
        "              #if s[end+i+1].isdigit() == True:\n",
        "                string_frase = s[start:end+i+2]\n",
        "                list_values_rigth.append(string_frase)\n",
        "                break\n",
        "          if s[end+i].isdigit():\n",
        "            string_frase = s[start:end+i+1]\n",
        "            list_values_rigth.append(string_frase)\n",
        "            break\n",
        "          #elif s[end+i].isdigit() and s[end+i+1].isdigit() == False: #vagas 5\n",
        "          elif s[end+i].isdigit() and s[end+i+1].isdigit() == False:\n",
        "            #if end+i+1 <= len(s)-1:#.isdigit() == True:\n",
        "              #if s[end+i+1].isdigit() == False:\n",
        "                string_frase = s[start:end+i+1]\n",
        "                list_values_rigth.append(string_frase)\n",
        "                break\n",
        "          elif s[end+i].isdigit() == False and s[end+i].isspace() == False: #quer dizer que é uma letra\n",
        "            #string_frase = s[start:end+i+1]\n",
        "            break\n",
        "        for i in range(1,100): #para a esquerda\n",
        "          if s[start-i].isdigit():\n",
        "            string_frase = s[start-i:end]\n",
        "            if s[start-i-1] != 'm' and s[start-i-1].isdigit() == False: #6 quartos\n",
        "              list_values_left.append(string_frase)\n",
        "              break\n",
        "            elif s[start-i-1] != 'm' and s[start-i-1].isdigit() == True: #36 quartos\n",
        "              list_values_left.append(s[start-i-1:end])\n",
        "              break\n",
        "          elif s[start-i].isdigit() == False and s[start-i].isspace() == False: #quer dizer que é uma letra\n",
        "            break\n",
        "    list_cont_space = []\n",
        "    list_values_rigth.extend(list_values_left)\n",
        "    #print(list_values_rigth)\n",
        "    if list_values_rigth != []:\n",
        "      for v in list_values_rigth:\n",
        "        list_cont_space.append( (len(re.findall('\\s', v)), v))\n",
        "\n",
        "      list_cont_space.sort(key=lambda x:x[0])\n",
        "      if len(list_cont_space) > 1:\n",
        "        if list_cont_space[0][0] == list_cont_space[1][0] and re.findall('\\d+', list_cont_space[0][1]) != re.findall('\\d+', list_cont_space[1][1]):\n",
        "          #return dorm_rgx_tag_space, suite_rgx_tag_space, vaga_rgx_tag_space, banheiro_rgx_tag_space, quarto_rgx_tag_space, garagem_rgx_tag_space, carro_rgx_tag_space        \n",
        "          continue\n",
        "      \n",
        "      if tag == 'dorm':dorm_rgx_tag_space = int(re.findall('\\d{1,2}', list_cont_space[0][1])[0])\n",
        "      elif tag == 'suit':suite_rgx_tag_space = int(re.findall('\\d{1,2}', list_cont_space[0][1])[0])\n",
        "      elif tag == 'vaga':vaga_rgx_tag_space = int(re.findall('\\d{1,2}', list_cont_space[0][1])[0])\n",
        "      elif tag == 'banheiro':banheiro_rgx_tag_space = int(re.findall('\\d{1,2}', list_cont_space[0][1])[0])\n",
        "      elif tag == 'quarto':quarto_rgx_tag_space = int(re.findall('\\d{1,2}', list_cont_space[0][1])[0])\n",
        "      elif tag == 'garage':garagem_rgx_tag_space = int(re.findall('\\d{1,2}', list_cont_space[0][1])[0])\n",
        "      elif tag == 'carro':carro_rgx_tag_space = int(re.findall('\\d{1,2}', list_cont_space[0][1])[0])\n",
        "      \n",
        "  return dorm_rgx_tag_space, suite_rgx_tag_space, vaga_rgx_tag_space, banheiro_rgx_tag_space, quarto_rgx_tag_space, garagem_rgx_tag_space, carro_rgx_tag_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bd3Weg5S7h4"
      },
      "outputs": [],
      "source": [
        "def returnAreaTerreno(url, s):\n",
        "  s = unidecode(re.sub('\\s+', ' ', s.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0','')))\n",
        "  doc = nlp8(s.lower())\n",
        "  expression = re.compile('terreno')\n",
        "  list_values_terreno = []\n",
        "  for match in re.finditer(expression, doc.text):\n",
        "    start, end = match.span()\n",
        "    span = doc.char_span(start, end, label='terreno')\n",
        "    if span:\n",
        "      ok2 = 0\n",
        "      for i in range(1,20):\n",
        "        string_verify = s[start:end+i][-2:]\n",
        "        if (string_verify == 'm2' or string_verify == 'm²' or s[start:end+i][-3:] == 'm ²' or s[start:end+i][-3:] == 'm 2') and (re.findall('\\d', s[start:end+i][-4]) != []):\n",
        "          frase_select = s[start:end+i]\n",
        "          if ('area' in frase_select or 'total' in frase_select or 'terreno' in frase_select) and ('priv' not in frase_select):\n",
        "            for v in re.findall('\\d+[,.]?\\d+',s[start:end+i] ):\n",
        "              list_values_terreno.append(v)\n",
        "            break\n",
        "          break\n",
        "      for i in range(1,30):\n",
        "        string_verify = s[start-i:end][:2]\n",
        "        if string_verify == 'm2' or string_verify == 'm²' or s[start-i:end][:3] == 'm ²' or s[start-i:end][:3] == 'm 2':\n",
        "          not_frase = 0\n",
        "          contn, cont_space = 0, 0\n",
        "          frase_select2 = s[start-i:end]\n",
        "          for k in range(1,30):\n",
        "            frase_select2 = s[start-i-k:end]\n",
        "            frasev = s[start-i-k]\n",
        "            if re.findall('\\d', frasev) != []:\n",
        "              contn +=1\n",
        "            elif re.findall('\\s', frasev) != [] and frasev[0:3] != 'com' and frasev[0:2] != 'de':\n",
        "              cont_space +=1\n",
        "            if cont_space > 1: \n",
        "              if ('priv' in frase_select2 or 'box' in frase_select2):\n",
        "                not_frase = 1\n",
        "                break\n",
        "              if not_frase == 0:\n",
        "                for v in re.findall('\\d+[,.]?\\d+',s[start-i-k:end] ):\n",
        "                  list_values_terreno.append(v)\n",
        "              break\n",
        "          break\n",
        "  return list_values_terreno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8R2Ib79TA3P"
      },
      "outputs": [],
      "source": [
        "def returnArea(url, s):\n",
        "  #print(url)\n",
        "  s = unidecode(re.sub('\\s+', ' ', s.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0','')))\n",
        "  doc = nlp8(s.lower())\n",
        "  expression = re.compile('area')\n",
        "  list_values_area = []\n",
        "  parte_1_direita = False\n",
        "  for match in re.finditer(expression, doc.text):\n",
        "    start, end = match.span()\n",
        "    span = doc.char_span(start, end, label='area')\n",
        "    if span:\n",
        "      for i in range(1,20):\n",
        "        string_verify = s[start:end+i][-2:]\n",
        "        if (string_verify == 'm2' or string_verify == 'm²' or s[start:end+i][-3:] == 'm ²' or s[start:end+i][-3:] == 'm 2') and (re.findall('\\d', s[start:end+i][-4]) != []):\n",
        "          frase_select = s[start:end+i]\n",
        "          if ('area' in frase_select or 'total' in frase_select or 'terreno' in frase_select) and ('privativ' not in frase_select):\n",
        "            #print(s[start:end+i])\n",
        "            for v in re.findall('\\d+[,.]?\\d+',s[start:end+i] ):\n",
        "              list_values_area.append(v)\n",
        "            parte_1_direita = True\n",
        "            break\n",
        "      for i in range(1,20):\n",
        "        string_verify = s[start-i:end][:2]\n",
        "        if string_verify == 'm2' or string_verify == 'm²' or s[start-i:end][:3] == 'm ²' or s[start-i:end][:3] == 'm 2':\n",
        "          not_frase = 0\n",
        "          contn, cont_space = 0, 0\n",
        "          frase_select2 = s[start-i:end]\n",
        "          for k in range(1,30):\n",
        "            #if (re.findall('[a-z]', s[start-i-k]) != []) and (s[start-i-k] !='.' or s[start-i-k] != ','):\n",
        "            #print(s[start-i-k:end])\n",
        "            frase_select2 = s[start-i-k:end]\n",
        "            frasev = s[start-i-k]\n",
        "            if re.findall('\\d', frasev) != []:\n",
        "              contn +=1\n",
        "            elif re.findall('\\s', frasev) != [] and frasev[0:3] != 'com' and frasev[0:2] != 'de':\n",
        "              cont_space +=1\n",
        "            if cont_space > 1: \n",
        "              #print(\"FRASE\", frase_select2)\n",
        "              if ('priv' in frase_select2 or 'box' in frase_select2) and (parte_1_direita == True):\n",
        "                not_frase = 1\n",
        "                break\n",
        "              if not_frase == 0:\n",
        "                for v in re.findall('\\d+[,.]?\\d+',s[start-i-k:end] ):\n",
        "                  list_values_area.append(v)\n",
        "              break\n",
        "          break\n",
        "    #print(list_values_area)\n",
        "  return list_values_area\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def returnAreaPrivativa(url, s):\n",
        "  s = unidecode(re.sub('\\s+', ' ', s.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0','')))\n",
        "  doc = nlp8(s.lower())\n",
        "  expression = re.compile('privativ\\w*')\n",
        "  list_values_area_priv = []\n",
        "  for match in re.finditer(expression, doc.text):\n",
        "    start, end = match.span()\n",
        "    span = doc.char_span(start, end, label='priv')\n",
        "    if span:\n",
        "      ok2 = 0\n",
        "      for i in range(1,20):\n",
        "        string_verify = s[start:end+i][-2:]\n",
        "        if (string_verify == 'm2' or string_verify == 'm²' or s[start:end+i][-3:] == 'm ²' or s[start:end+i][-3:] == 'm 2') and (re.findall('\\d', s[start:end+i][-4]) != []):\n",
        "          for v in re.findall('\\d+[,.]?\\d+',s[start:end+i] ):\n",
        "            list_values_area_priv.append(v)\n",
        "          ok2 +=1\n",
        "          break\n",
        "      #CONFERE PARA A ESQUERDA SE TEM OUTRO NÚMERO\n",
        "      for i in range(1,20):\n",
        "        string_verify = s[start-i:end][:2]\n",
        "        if string_verify == 'm2' or string_verify == 'm²' or s[start-i:end][:3] == 'm ²' or s[start-i:end][:3] == 'm 2':\n",
        "          not_frase = 0\n",
        "          contn, cont_space = 0, 0\n",
        "          frase_select2 = s[start-i:end]\n",
        "          for k in range(1,30):\n",
        "            frase_select2 = s[start-i-k:end]\n",
        "            frasev = s[start-i-k]\n",
        "            if re.findall('\\d', frasev) != []:\n",
        "              contn +=1\n",
        "            elif re.findall('\\s', frasev) != [] and frasev[0:3] != 'com' and frasev[0:2] != 'de':\n",
        "              cont_space +=1\n",
        "            if cont_space > 1: \n",
        "              if ('terreno' in frase_select2 or 'box' in frase_select2 or 'total' in frase_select2): #PARA DEIXAR MAIS FLEXÍVEL E ADICIONAR POSSÍVEIS MAIS VALORES -> ok2 == 0\n",
        "                not_frase = 1\n",
        "                break\n",
        "              if not_frase == 0:\n",
        "                for v in re.findall('\\d+[,.]?\\d+',s[start-i-k:end] ):\n",
        "                  #print(\"insere\", v)\n",
        "                  list_values_area_priv.append(v)\n",
        "              break\n",
        "          break\n",
        "  return list_values_area_priv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NriL7UWvTGQR"
      },
      "outputs": [],
      "source": [
        "def returnArea(url, s):\n",
        "  #print(url)\n",
        "  s = unidecode(re.sub('\\s+', ' ', s.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0','')))\n",
        "  doc = nlp8(s.lower())\n",
        "  expression = re.compile('area')\n",
        "  list_values_area = []\n",
        "  parte_1_direita = False\n",
        "  for match in re.finditer(expression, doc.text):\n",
        "    start, end = match.span()\n",
        "    span = doc.char_span(start, end, label='area')\n",
        "    if span:\n",
        "      for i in range(1,20):\n",
        "        string_verify = s[start:end+i][-2:]\n",
        "        if (string_verify == 'm2' or string_verify == 'm²' or s[start:end+i][-3:] == 'm ²' or s[start:end+i][-3:] == 'm 2') and (re.findall('\\d', s[start:end+i][-4]) != []):\n",
        "          frase_select = s[start:end+i]\n",
        "          if ('area' in frase_select or 'total' in frase_select or 'terreno' in frase_select) and ('privativ' not in frase_select):\n",
        "            #print(s[start:end+i])\n",
        "            for v in re.findall('\\d+[,.]?\\d+',s[start:end+i] ):\n",
        "              list_values_area.append(v)\n",
        "            parte_1_direita = True\n",
        "            break\n",
        "      for i in range(1,20):\n",
        "        string_verify = s[start-i:end][:2]\n",
        "        if string_verify == 'm2' or string_verify == 'm²' or s[start-i:end][:3] == 'm ²' or s[start-i:end][:3] == 'm 2':\n",
        "          not_frase = 0\n",
        "          contn, cont_space = 0, 0\n",
        "          frase_select2 = s[start-i:end]\n",
        "          for k in range(1,30):\n",
        "            #if (re.findall('[a-z]', s[start-i-k]) != []) and (s[start-i-k] !='.' or s[start-i-k] != ','):\n",
        "            #print(s[start-i-k:end])\n",
        "            frase_select2 = s[start-i-k:end]\n",
        "            frasev = s[start-i-k]\n",
        "            if re.findall('\\d', frasev) != []:\n",
        "              contn +=1\n",
        "            elif re.findall('\\s', frasev) != [] and frasev[0:3] != 'com' and frasev[0:2] != 'de':\n",
        "              cont_space +=1\n",
        "            if cont_space > 1: \n",
        "              #print(\"FRASE\", frase_select2)\n",
        "              if ('priv' in frase_select2 or 'box' in frase_select2) and (parte_1_direita == True):\n",
        "                not_frase = 1\n",
        "                break\n",
        "              if not_frase == 0:\n",
        "                for v in re.findall('\\d+[,.]?\\d+',s[start-i-k:end] ):\n",
        "                  list_values_area.append(v)\n",
        "              break\n",
        "          break\n",
        "    #print(list_values_area)\n",
        "  return list_values_area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L02yrjjJTOng"
      },
      "outputs": [],
      "source": [
        "def converteStringToDigito(string):\n",
        "  list_number_per_extenso =  ['zero', 'um', 'dois', 'tres', 'quatro', 'cinco', 'seis', 'sete', 'oito', 'nove', 'dez',\n",
        "           'onze', 'doze', 'treze', 'catorze', 'quinze', 'dezesseis', 'dezessete', 'dezoito', 'dezenove', 'vinte']\n",
        "  \n",
        "  for i in range(1,len(list_number_per_extenso)):\n",
        "    if list_number_per_extenso[i] == string:\n",
        "      return i\n",
        "\n",
        "\n",
        "def returnComodosOk(list_frases):\n",
        "  dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM'\n",
        "  list_frases.sort(key = lambda x: x[1], reverse=True)\n",
        "  list_frase_correct = []\n",
        "  for fr in list_frases:\n",
        "    dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM'\n",
        "    correct  = 0\n",
        "    fr = fr[0].strip()\n",
        "    frase_correct = fr\n",
        "    if len(fr) == 0 or fr == None:break\n",
        "    #print(fr)\n",
        "    if fr[-1].isdigit() == False:\n",
        "      frase_correct = fr\n",
        "      if (re.findall('\\d{1,2}\\s*dorm', frase_correct)) != []:\n",
        "        dorms_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*dorm', frase_correct)[0])[0]\n",
        "        correct +=1\n",
        "      if (re.findall('\\d{1,2}\\s*suit', frase_correct)) != []:\n",
        "        suites_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*suit', frase_correct)[0])[0]\n",
        "        correct +=1\n",
        "      if (re.findall('\\d{1,2}\\s*vaga', frase_correct)) != []:\n",
        "        vagas_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*vaga', frase_correct)[0])[0]\n",
        "        correct +=1\n",
        "      if (re.findall('\\d{1,2}\\s*banheiro', frase_correct)) != []:\n",
        "        banheiro_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*banheiro', frase_correct)[0])[0]\n",
        "        correct +=1\n",
        "      if (re.findall('\\d{1,2}\\s*quarto', frase_correct)) != []:\n",
        "        quartos_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*quarto', frase_correct)[0])[0]\n",
        "        correct +=1\n",
        "      if (re.findall('\\d{1,2}\\s*garage', frase_correct)) != []:\n",
        "        garagem_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*garage', frase_correct)[0])[0]\n",
        "        correct +=1\n",
        "      if (re.findall('\\d{1,2}\\s*carro', frase_correct)) != []:\n",
        "        carros_rgx_only = re.findall('\\d{1,2}', re.findall('\\d{1,2}\\s*carro', frase_correct)[0])[0]\n",
        "        correct +=1\n",
        "    elif fr[-1].isdigit():\n",
        "        #frase_correct = fr[fr.find(' '):]\n",
        "        frase_correct = fr\n",
        "        if (re.findall('dorm\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "          dorms_rgx_only = re.findall('\\d{1,2}', re.findall('dorm\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "          correct +=1\n",
        "        if (re.findall('suit\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "          suites_rgx_only = re.findall('\\d{1,2}', re.findall('suit\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "          correct +=1\n",
        "        if (re.findall('vaga\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "          vagas_rgx_only = re.findall('\\d{1,2}', re.findall('vaga\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "          correct +=1\n",
        "        if (re.findall('banheiro\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "          banheiro_rgx_only = re.findall('\\d{1,2}', re.findall('banheiro\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "          correct +=1\n",
        "        if (re.findall('quarto\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "          quartos_rgx_only = re.findall('\\d{1,2}', re.findall('quarto\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "          correct +=1\n",
        "        if (re.findall('garage\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "          garagem_rgx_only = re.findall('\\d{1,2}', re.findall('garage\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "          correct +=1\n",
        "        if (re.findall('carro\\w*\\s*\\d{1,2}', frase_correct)) != []:\n",
        "          carros_rgx_only = re.findall('\\d{1,2}', re.findall('carro\\w*\\s*\\d{1,2}', frase_correct)[0])[0]\n",
        "          correct +=1\n",
        "    if correct > 1:\n",
        "      #print(frase_correct)\n",
        "      list_frase_correct.append((frase_correct, correct, dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only))\n",
        "  if len(list_frase_correct) > 1:\n",
        "    #print(list_frase_correct)\n",
        "    list_frase_correct.sort(key = lambda x: x[1], reverse=True)\n",
        "    if list_frase_correct[0][1] == list_frase_correct[1][1]:\n",
        "      if len(list_frase_correct[0][0]) > len(list_frase_correct[1][0]):\n",
        "        list_frase_correct[0], list_frase_correct[1] = list_frase_correct[1], list_frase_correct[0]\n",
        "  #print(\"Frase\", list_frase_correct)\n",
        "  if len(list_frase_correct) > 0:\n",
        "    dorms_rgx_only, suites_rgx_only, vagas_rgx_only = list_frase_correct[0][2],list_frase_correct[0][3],list_frase_correct[0][4],\n",
        "    banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only = list_frase_correct[0][5],list_frase_correct[0][6],list_frase_correct[0][7],list_frase_correct[0][8]\n",
        "  return dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quartos_rgx_only, garagem_rgx_only, carros_rgx_only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqSlODC0TW-A"
      },
      "outputs": [],
      "source": [
        "def verify_tag_entry(word, tipo = 'tag'):\n",
        "  if tipo == 'tag': list_search = ['dorm', 'suit', 'vaga', 'banheiro', 'quarto', 'garage', 'carro', 'box']  \n",
        "  elif tipo == 'number_extense': list_search = ['zero', 'um', 'dois', 'tres', 'quatro', 'cinco', 'seis', 'sete', 'oito', 'nove', 'dez',\n",
        "           'onze', 'doze', 'treze', 'catorze', 'quinze', 'dezesseis', 'dezessete', 'dezoito', 'dezenove', 'vinte']\n",
        "  for tag in list_search:\n",
        "    if tag in word: \n",
        "      #print(\"word\", word, \"ok\")\n",
        "      return 1\n",
        "  return 0\n",
        "\n",
        "list_number_per_extenso =  ['zero', 'um', 'dois', 'tres', 'quatro', 'cinco', 'seis', 'sete', 'oito', 'nove', 'dez',\n",
        "           'onze', 'doze', 'treze', 'catorze', 'quinze', 'dezesseis', 'dezessete', 'dezoito', 'dezenove', 'vinte']\n",
        "\n",
        "def comodosJuntosNEW(s):\n",
        "  s = re.sub('\\s+', ' ', unidecode(s.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0','').replace('sendo', '').replace('demi','').replace('demi-','')).replace('ate', ''))\n",
        "  doc = nlp8(s)\n",
        "  pontuaction = \"\"\"!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\"\"\"\n",
        "  list_frases = []\n",
        "  indices_area = indicesArea(s)\n",
        "  indices_preco = indicesPreco(s)\n",
        "  for tag in ['dorm', 'suit', 'vaga', 'banheiro', 'quarto', 'garage', 'carro', 'box']:\n",
        "    tag_regex = tag+'\\w*'\n",
        "    for match in re.finditer(tag_regex, doc.text):\n",
        "      start, end = match.span()\n",
        "      span = doc.char_span(start, end, label='area')\n",
        "      if span:\n",
        "        #print(\"-----SPAN-----\", span)\n",
        "        #confere se tem pontuação para o lado direito\n",
        "        inicio, fim = start, end\n",
        "        if s[start:end+1][-1] in pontuaction or s[start:end+2][-1] in pontuaction:\n",
        "            #print(s[start:end+1])\n",
        "            #se tem pontuação, então confere para a esquerda se tem um número\n",
        "            space = 0\n",
        "            for j in range(1,10):\n",
        "              if s[start-j].isspace():space +=1\n",
        "              if space == 2:break\n",
        "              if s[start-j].isdigit():\n",
        "                inicio, fim = start-j, end\n",
        "                #print(s[start-j:end])\n",
        "        #confere para a ESQUERDA se tem outras tags procuradas\n",
        "        space = 0\n",
        "        inicio2 = inicio\n",
        "        inicio_final = -1\n",
        "        inicio_aux = -1\n",
        "        digit_aux = 0\n",
        "        list_ordens = []\n",
        "        for e in range(1,100):\n",
        "          if inicio-e < 0:\n",
        "            break\n",
        "          if inicio-e in indices_area or inicio-e in indices_preco:\n",
        "            inicio_final = inicio - e + 1\n",
        "            break\n",
        "          if s[inicio-e].isspace():\n",
        "            space +=1\n",
        "          if space == 1:\n",
        "            #if verify_tag_entry(s[inicio-e+digit_aux:inicio2], tipo = 'tag') == 1:\n",
        "            ##  print(\"OKKK\")\n",
        "            inicio_aux = inicio-e\n",
        "          if s[inicio-e] == ',' or s[inicio-e] == ';' or s[inicio-e] == '|' or s[inicio-e] == '-':\n",
        "            inicio_final = inicio-e + 1\n",
        "            break\n",
        "          elif s[inicio-e].isdigit():\n",
        "              #if s[inicio-e-1].isdigit():digit_aux -= 1\n",
        "              if len(list_ordens ) > 0: #verificar também se é de 2 dígitos, como 02 dois dorms\n",
        "                if s[inicio-e-1].isdigit() == False:\n",
        "                  if str(converteStringToDigito(list_ordens[-1].strip())) != s[inicio-e]:\n",
        "                    #print(\"NÃO\")\n",
        "                    inicio_final = inicio - e + 2\n",
        "                    break\n",
        "                elif s[inicio-e-1].isdigit() == True:\n",
        "                  if str(converteStringToDigito(list_ordens[-1].strip())) != s[inicio-e-1]:\n",
        "                    inicio_final = inicio - e + 2\n",
        "                    break\n",
        "              space = 0\n",
        "          if space >=2:\n",
        "            list_ordens = []\n",
        "            if verify_tag_entry(s[inicio-e+digit_aux:inicio2], tipo = 'tag') == 1 :#or verify_tag_entry(s[inicio-e+digit_aux:inicio2], tipo = 'number_extense') == 1:\n",
        "              #print(\"DENTROOOOOOOOOOOOO\", s[inicio-e+digit_aux:fim])\n",
        "              space = 1\n",
        "              inicio2 = inicio - e + digit_aux\n",
        "              continue\n",
        "            elif verify_tag_entry(s[inicio-e+digit_aux:inicio2], tipo = 'number_extense') == 1:\n",
        "              list_ordens.append(s[inicio-e+digit_aux:inicio2])\n",
        "              space = 1\n",
        "              inicio2 = inicio - e + digit_aux\n",
        "              continue\n",
        "            else: \n",
        "              #number_extense_ver = 0\n",
        "              #se não tem uma palavra anterior que é igual ou parecida com as tags procuradas, então sai fora:\n",
        "              #print(\"TESTEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\", s[inicio-e+digit_aux-5:inicio2],s[inicio-e+digit_aux:inicio2][-1], \"ANTES\", s[inicio-e+digit_aux-1:inicio2][0])\n",
        "              if s[inicio-e+digit_aux:inicio2][-1].isdigit():\n",
        "                inicio_final = inicio2 - 1 + digit_aux\n",
        "                #print(\"DIGIT\")\n",
        "                if s[inicio-e+digit_aux-1].isdigit():\n",
        "                  inicio_final -= 1\n",
        "              else:inicio_final = inicio2 + digit_aux\n",
        "              #print(\"FORAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\", s[inicio-e+digit_aux:inicio2])\n",
        "              break\n",
        "          #print(s[inicio-e:fim])\n",
        "        if s[inicio_final-1].isdigit():inicio_final -=1\n",
        "        #print(\"final\", s[inicio_final:fim])\n",
        "\n",
        "        #PARA A DIREITAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA ------------------------------\n",
        "\n",
        "        fim2 = fim\n",
        "        space = 0\n",
        "        fim_final = -1\n",
        "        fim_aux = -1\n",
        "        digit_aux = 0\n",
        "        list_ordens = []\n",
        "        #print(\"DIREITAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\", span)\n",
        "        for d in range(1,100):\n",
        "          if fim2+ d + 1 > len(s) - 1:\n",
        "            break\n",
        "          if fim2 + d in indices_area or fim2 + d in indices_preco:\n",
        "            fim_final = fim2 + d #+ 1\n",
        "            break\n",
        "          if s[fim2 + d].isspace():\n",
        "            space +=1\n",
        "          if space == 1:\n",
        "            fim_aux = fim2 + d + 1\n",
        "          #print(s[fim2:fim2+d+1])\n",
        "          if s[fim2+ d ] == ',' or s[fim2+ d ] == ';' or s[fim2+ d ] == '|' or s[fim2+ d ] == '-':\n",
        "            fim_final = fim2 + d + 1 \n",
        "            break\n",
        "          elif s[fim2 + d].isdigit():\n",
        "              #if s[fim + d + 1-1].isdigit():digit_aux -= 1\n",
        "              if len(list_ordens) > 0: #verificar também se é de 2 dígitos, como 02 dois dorms\n",
        "                if s[fim2+d+1].isdigit() == False:\n",
        "                  if str(converteStringToDigito(list_ordens[-1].strip())) != s[fim2 + d]:\n",
        "                    fim_final = fim2 + d\n",
        "                    break\n",
        "                elif s[fim2+d+1].isdigit() == True:\n",
        "                  if str(converteStringToDigito(list_ordens[-1].strip())) != s[fim2 + d + 1]:\n",
        "                    fim_final = fim2 + d + 1\n",
        "                    break\n",
        "              space = 2\n",
        "              fim2 +=d\n",
        "          if space >=2:\n",
        "            list_ordens = []\n",
        "            if verify_tag_entry(s[fim2:fim2+d+1], tipo = 'tag') == 1 :#or verify_tag_entry(s[fim2+d+1:fim2], tipo = 'number_extense') == 1:\n",
        "              #print(\"DENTROOOOOOOOOOOOO\", s[fim2:fim2+d+1])\n",
        "              space = 1\n",
        "              fim2 = fim2 + d +1\n",
        "              continue\n",
        "            elif verify_tag_entry(s[fim2:fim2+d+1], tipo = 'number_extense') == 1:\n",
        "              list_ordens.append(s[fim2:fim2+d+1])\n",
        "              space = 1\n",
        "              fim2 = fim2 + d + 1\n",
        "              continue\n",
        "            else: \n",
        "              #se não tem uma palavra anterior que é igual ou parecida com as tags procuradas, então sai fora:\n",
        "              if s[fim2:fim2+d+digit_aux][-1].isdigit():\n",
        "                fim_final = fim2 + 1 + digit_aux\n",
        "                if fim_final in indices_area: fim_final -=1\n",
        "              else:\n",
        "                fim_final = fim2 + digit_aux\n",
        "                if fim_final in indices_area: fim_final -=1\n",
        "              #print(\"FORAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\", s[fim2:fim2+d+1])\n",
        "              break\n",
        "          #print(s[inicio_final:fim_final])\n",
        "        #print(\"final\", s[inicio_final:fim_final])\n",
        "        frase_final = s[inicio_final:fim_final].strip()\n",
        "        #print(indices_area, fim_final)\n",
        "        #print(frase_final)\n",
        "        list_frases.append((frase_final, len(frase_final)))\n",
        "  return returnComodosOk(list_frases) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhqJwIScThh-"
      },
      "outputs": [],
      "source": [
        "\"\"\"BOM SE NÃO TEM NÚMERO NO INÍCIO E NO FINAL TAMBÉM. POR EXEMPLO: cod 1541 dorms 2 suites 3 banheiros 2\"\"\"\n",
        "def tagsShow(s, tipo):\n",
        "  s = re.sub('\\(?\\d{1,2}\\)?\\s*\\d{4,5}[.-]?\\d{3,5}', '', s)\n",
        "  if tipo == 'comodos':\n",
        "    s = unidecode(s.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0','').replace('de','').replace('com','').replace('demi-', '').replace('demi',''))  \n",
        "    #s = unidecode(s.lower().replace(':','').replace('\\xa0','').replace('de','').replace('com','').replace('demi-', '').replace('demi',''))  \n",
        "    list_variaveis = ['dorm', 'quarto', 'suit', 'vaga', 'garage', 'banheiro', 'carro']\n",
        "    vs1 = re.findall('[dormquartosvagasuitegaragembanheiroscarro\\s]+\\d{1,2}', s) \n",
        "    vs2 = re.findall('\\d{1,2}[dormquartosvagasuitegaragembanheiroscarro\\s]+', s)\n",
        "    indices_area = indicesArea(s)\n",
        "    indices_preco = indicesPreco(s)\n",
        "    indices_area +=indices_preco\n",
        "    indices_excess = indices_area\n",
        "  elif tipo == 'areas':\n",
        "    s = unidecode(s.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0','').replace('de','').replace('com','').replace('m²', '').replace('m2', '').replace('m 2','').replace('m ²','').replace('m22','').replace('casa', ''))\n",
        "    list_variaveis = ['area','priv','tot','terren','constr']\n",
        "    vs1 = re.findall('[areatotalconstruidprivativterrenoglobal\\s]+\\d+[,.]?\\d+', s) \n",
        "    vs2 = re.findall('\\d+[,.]?\\d+[areatotalconstruidprivativterrenoglobal\\s]+', s)\n",
        "    indices_preco = indicesPreco(s)\n",
        "    indices_excess = indices_preco\n",
        "  vs1.extend(vs2)\n",
        "  #print(vs1)\n",
        "  list_indices_and_phase = []\n",
        "  list_indices_iniciais = []\n",
        "  for v in vs1:\n",
        "    #print(s.find(v), s.find(v)+len(v), s[s.find(v):s.find(v)+len(v)])\n",
        "    #print(s.find(v.strip()), s.find(v.strip())+len(v.strip()), s[s.find(v.strip()):s.find(v.strip())+len(v.strip())])\n",
        "    for lv in list_variaveis:\n",
        "      if lv in v:\n",
        "        #print(v,  s.find(v), s.find(v)+len(v))\n",
        "        v = v.strip()\n",
        "        indice_v_inicial, indice_v_final = s.find(v), s.find(v)+len(v)\n",
        "        if indice_v_inicial in indices_excess or indice_v_final-1 in indices_excess:continue\n",
        "        if indice_v_inicial in indices_excess or indice_v_final-1 in indices_excess:continue\n",
        "        if indice_v_inicial in list_indices_iniciais:continue\n",
        "        list_indices_iniciais.append(indice_v_inicial)\n",
        "        list_indices_and_phase.append((indice_v_inicial, indice_v_final, s[indice_v_inicial:indice_v_final]))\n",
        "        break\n",
        "  list_indices_and_phase.append((2000000000,-1,-1))\n",
        "  #print(s)\n",
        "  list_indices_and_phase.sort(key=lambda x: x[0])\n",
        "  #print(\"LISTA\", list_indices_and_phase)\n",
        "  dorms, quartos, suites, vagas, banheiros, garagem, carros  = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM'\n",
        "  area, area_priv, area_terreno, area_construida, area_total, area_global, area_util = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM'\n",
        "  pula = 0\n",
        "  for l in range(len(list_indices_and_phase)):\n",
        "    if pula == 1:\n",
        "      pula = 0\n",
        "      continue\n",
        "    frase = ''\n",
        "    if l == len(list_indices_and_phase)-1 :break#or l == len(list_indices_and_phase)-1:break\n",
        "    if list_indices_and_phase[l][1] in list(range(list_indices_and_phase[l+1][0], list_indices_and_phase[l+1][1]+1)):\n",
        "      n = re.findall('\\d+', list_indices_and_phase[l][2])[0]\n",
        "      #print( \"IGUAL:\", re.findall('\\D+', list_indices_and_phase[l][2].strip())[0].strip(), re.findall('\\D+', list_indices_and_phase[l+1][2])[0].strip())\n",
        "      #print(list_indices_and_phase[l][2].split()[-1], list_indices_and_phase[l+1][2].split()[0], list_indices_and_phase[l][2].split()[-1] == list_indices_and_phase[l+1][2].split()[0])\n",
        "      if list_indices_and_phase[l+1][2].strip()[:len(n)] == n:\n",
        "        #print(\"PRIMEIRO\", list_indices_and_phase[l][2].strip())\n",
        "        pula = 1\n",
        "        frase = list_indices_and_phase[l][2].strip()\n",
        "      elif re.findall('\\D+', list_indices_and_phase[l][2])[0].strip() == re.findall('\\D+', list_indices_and_phase[l+1][2])[0].strip():\n",
        "        frase = list_indices_and_phase[l][2].strip()\n",
        "        #if tipo == 'areas':pula = 1\n",
        "        #pula = 1\n",
        "        #print(\"FRASE IGUAL\", frase)\n",
        "      #se a palavra final for diferente da palavra inicial a outra frase\n",
        "      elif list_indices_and_phase[l][2].split()[-1] != list_indices_and_phase[l+1][2].split()[0]:\n",
        "        frase = list_indices_and_phase[l][2].strip()\n",
        "        #print(\"FRASE DIFERENTE\", frase)\n",
        "      \n",
        "    else:\n",
        "      #print(list_indices_and_phase[l][2].strip())\n",
        "      frase = list_indices_and_phase[l][2]\n",
        "\n",
        "    if frase == '':continue\n",
        "    #print(frase)\n",
        "    if tipo == 'comodos':\n",
        "      if 'dorm' in frase: \n",
        "        if re.findall('\\d{1,2}\\s*dorm|dorm\\w*\\s*\\d{1,2}', frase) == []:\n",
        "          #print(\"não pertence\")\n",
        "          continue\n",
        "        if dorms == 'NENHUM':dorms = re.findall('\\d+', frase)[0]\n",
        "        else: dorms +='&-&'+ re.findall('\\d+', frase)[0]\n",
        "      if 'quarto' in frase: \n",
        "        if re.findall('\\d{1,2}\\s*quarto|quarto\\w*\\s*\\d{1,2}', frase) == []:continue\n",
        "        if quartos == 'NENHUM':quartos = re.findall('\\d+', frase)[0]\n",
        "        else: quartos +='&-&'+ re.findall('\\d+', frase)[0]\n",
        "      if 'suit' in frase: \n",
        "        if re.findall('\\d{1,2}\\s*suit|suit\\w*\\s*\\d{1,2}', frase) == []:continue\n",
        "        if suites == 'NENHUM':suites = re.findall('\\d+', frase)[0]\n",
        "        else: suites +='&-&'+ re.findall('\\d+', frase)[0]\n",
        "      if 'vaga' in frase: \n",
        "        if re.findall('\\d{1,2}\\s*vaga|vaga\\w*\\s*\\d{1,2}', frase) == []:continue\n",
        "        if vagas == 'NENHUM':vagas = re.findall('\\d+', frase)[0]\n",
        "        else: vagas +='&-&'+ re.findall('\\d+', frase)[0]\n",
        "      if 'banheiro' in frase: \n",
        "        if re.findall('\\d{1,2}\\s*banheiro|banheiro\\w*\\s*\\d{1,2}', frase) == []:continue\n",
        "        if banheiros == 'NENHUM':banheiros = re.findall('\\d+', frase)[0]\n",
        "        else: banheiros +='&-&'+ re.findall('\\d+', frase)[0]\n",
        "      if 'garag' in frase: \n",
        "        if re.findall('\\d{1,2}\\s*garag|garag\\w*\\s*\\d{1,2}', frase) == []:continue\n",
        "        if garagem == 'NENHUM':garagem = re.findall('\\d+', frase)[0]\n",
        "        else: garagem +='&-&'+ re.findall('\\d+', frase)[0]\n",
        "      if 'carro' in frase: \n",
        "        if re.findall('\\d{1,2}\\s*carro|carro\\w*\\s*\\d{1,2}', frase) == []:continue\n",
        "        if carros == 'NENHUM':carros = re.findall('\\d+', frase)[0]\n",
        "        else: carros +='&-&'+ re.findall('\\d+', frase)[0]\n",
        "    elif tipo == 'areas':\n",
        "      if len(re.findall('\\d', frase)) >=8: continue #NÚMERO DE TELEFONE\n",
        "      if 'priv' in frase: \n",
        "        if area_priv == 'NENHUM':\n",
        "          area_priv = re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "        else: area_priv +='&-&'+re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "      if 'terren' in frase: #area_terreno = re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "        if area_terreno == 'NENHUM':\n",
        "          area_terreno = re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "        else: area_terreno +='&-&'+re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "      if 'constru' in frase: #area_construida = re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "        if area_construida == 'NENHUM':\n",
        "          area_construida = re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "        else: area_construida +='&-&'+re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "      if 'util' in frase: #area_util = re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "        if area_util == 'NENHUM':\n",
        "          area_util = re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "        else: area_util +='&-&'+re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "      if 'global' in frase: #area_global = re.findall('\\d+[,.]?\\d+', list_indices_and_phase[l][2])[0]\n",
        "        if area_global == 'NENHUM':\n",
        "          area_global = re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "        else: area_global +='&-&'+re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "      if 'area' in frase and ('priv' not in frase or 'constru' not in frase or 'terren' not in frase or 'la' not in frase): #area = re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "        if area == 'NENHUM':\n",
        "          area = re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "        else: area +='&-&'+re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "      if 'total' in frase and ('priv' not in frase or 'constru' not in frase or 'terren' not in frase or 'la' not in frase): #area_total = re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "        if area_total == 'NENHUM':\n",
        "          area_total = re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "        else: area_total +='&-&'+re.findall('\\d+[,.]?\\d+', frase)[0]\n",
        "\n",
        "  if tipo == 'comodos':\n",
        "    return dorms, quartos, suites, vagas, banheiros, garagem, carros  \n",
        "  elif tipo == 'areas': return area, area_priv, area_terreno, area_construida, area_total, area_global, area_util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8-9gU9qXBxF"
      },
      "outputs": [],
      "source": [
        "def regexOU(sentence):\n",
        "  dorms_ou, quartos_ou, suites_ou, banheiros_ou, vagas_ou, garagem_ou, carros_ou, box_ou = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM'\n",
        "  sentence = re.sub('\\s+', ' ', unidecode(sentence.lower()))\n",
        "  for tag in ['dorm','quarto','suit','banheiro','vaga','garage','carro','box']:\n",
        "    regex1 = re.findall('\\d{1,2}\\s*[aou]+\\s*\\d{1,2}\\s*'+tag, sentence)\n",
        "    regex2 = re.findall('\\d{1,2},\\s*\\d{1,2}\\s*[aou]+\\s*\\d{1,2}\\s*'+tag, sentence)\n",
        "    str_regex, str_regex1, str_regex2 = 'NENHUM', 'NENHUM', 'NENHUM'\n",
        "    if regex2 != []:\n",
        "      #print(regex2, df_sentences['Sentence'].iloc[i])\n",
        "      for v in list(set(regex2)):\n",
        "        if str_regex == 'NENHUM':str_regex = v\n",
        "        else: str_regex +='&-&'+v\n",
        "      #print(str_regex)\n",
        "    elif regex1 != []:\n",
        "      for v in list(set(regex1)):\n",
        "        if str_regex == 'NENHUM':str_regex = v\n",
        "        else: str_regex +='&-&'+v\n",
        "      #print(str_regex)\n",
        "    if str_regex != 'NENHUM':\n",
        "      if tag == 'dorm':dorms_ou = str_regex.replace(tag, '').strip()\n",
        "      if tag == 'quarto':quartos_ou = str_regex.replace(tag, '').strip()\n",
        "      if tag == 'suit':suites_ou = str_regex.replace(tag, '').strip()\n",
        "      if tag == 'banheiro':banheiros_ou = str_regex.replace(tag, '').strip()\n",
        "      if tag == 'vaga':vagas_ou = str_regex.replace(tag, '').strip()\n",
        "      if tag == 'garage':garagem_ou = str_regex.replace(tag, '').strip()\n",
        "      if tag == 'carro':carros_ou = str_regex.replace(tag, '').strip()\n",
        "      if tag == 'box':boxs_ou = str_regex.replace(tag, '').strip()\n",
        "  return dorms_ou, quartos_ou, suites_ou, banheiros_ou, vagas_ou, garagem_ou, carros_ou, box_ou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GOtehp-poxN"
      },
      "source": [
        "# Funções Regex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo2BtmPh6ZWD"
      },
      "source": [
        "## Pré-processamento nlp6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8onKBN1I-rp"
      },
      "outputs": [],
      "source": [
        "def configuraTexto(value):\n",
        "  digt = re.findall('\\d+', value)\n",
        "  if digt != []:\n",
        "    digt = digt[0]\n",
        "    l = value.split(digt)\n",
        "    for k in range(len(l)):\n",
        "      if l[k]=='':l[k] = digt\n",
        "    word_rep = ' '.join(l)\n",
        "    return word_rep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG43mKG26cZ-"
      },
      "outputs": [],
      "source": [
        "def tokenize_sentence(sent):\n",
        "  list_tk = []\n",
        "  for token_sent in nltk.word_tokenize(sent):\n",
        "    list_tk.append(token_sent)\n",
        "  tokens_sent = [plv for plv in list_tk if plv not in stopwords and plv not in string.punctuation ]\n",
        "  return ' '.join([str(elemento) for elemento in tokens_sent])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXHLqveZKjpI"
      },
      "outputs": [],
      "source": [
        "#EEEEEEEEEEEEEEEEEEEEEEEEEESSSSSSSSSSSSSSSSSSSSSSSSSSSSEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
        "list_types_names_tags = []\n",
        "list_type_dorms = []\n",
        "list_type_vagas = []\n",
        "list_type_suites = []\n",
        "list_numbers = ['1','2','3','4','5','6','7','8','9']\n",
        "#df_sentence_and_tags = pd.read_csv('/content/gdrive/My Drive/COOP/Anotador/AnotadorSpacyO5.csv')\n",
        "def findWordsAndIndices(sentence_find, word_find, tag, sub_word_padrao, option_ner = '*', sentence_ner=''):\n",
        "  for i in range(1):\n",
        "    word_find_sub_not_replace = word_find\n",
        "    s, inicio, fim = '','', ''\n",
        "    if option_ner == 1 or option_ner == 2:\n",
        "      sentence_find = sentence_ner\n",
        "    if word_find_sub_not_replace in sentence_find:\n",
        "      s = sentence_find\n",
        "      if option_ner != 1:\n",
        "        s = s.replace('ormit ori', 'ormitóri')\n",
        "        s = s.replace('su ite', 'suíte')\n",
        "        s = s.replace(word_find_sub_not_replace, sub_word_padrao)\n",
        "        s = s.replace(sub_word_padrao+'.', sub_word_padrao)\n",
        "        s = s.replace('dormitoriosór', 'dormitorios')\n",
        "        s = s.replace('dormitoriosio', 'dormitorios')\n",
        "        s = s.replace('dormitoriosorios', 'dormitorios')\n",
        "        s = s.replace('dormitoriositorios', 'dormitorios')\n",
        "        s = s.replace('dormitoriositorio(s)', 'dormitorios')\n",
        "        s = s.replace('dormitoriositório(s)', 'dormitorios')\n",
        "        s = s.replace('dormitoriositórios', 'dormitorios')\n",
        "        s = s.replace('Vaga(s):', 'vagas')\n",
        "        s = s.replace('vaga(s):', 'vagas')\n",
        "        s = s.replace('suítestestes', 'suítes')\n",
        "        regex = '[0-9]{1}'+word_find_sub_not_replace+'\\w*'\n",
        "        value = re.findall(regex, s)\n",
        "        if value != []: \n",
        "          text = configuraTexto(value[0])\n",
        "          s = s.replace(value[0], text)\n",
        "        regex = word_find_sub_not_replace+'\\w*'+'[0-9]{1}'\n",
        "        value = re.findall(regex, s)\n",
        "        if value != []: \n",
        "          text = configuraTexto(value[0])\n",
        "          s = s.replace(value[0], text)\n",
        "      word_find = sub_word_padrao\n",
        "      #02 dormitorios\n",
        "      if s[s.find(word_find)-1] == ' ' and s[s.find(word_find)-2] != '0' and re.findall('\\d',s[s.find(word_find)-2]) != [] and s[s.find(word_find)-3] == '0' and s.find(word_find) != -1:\n",
        "        for x in range(18):\n",
        "          try:\n",
        "            #print(s[s.find(word_find)+x])\n",
        "            if s[s.find(word_find)+x]==' ' or s[s.find(word_find)+x]=='.':               \n",
        "              inicio = s.find(word_find)-3\n",
        "              fim = s.find(word_find)+x\n",
        "              if option_ner == 1: return inicio, fim\n",
        "              word_sub = s[inicio:fim].replace(s[inicio+3:fim], sub_word_padrao)\n",
        "              s = s.replace(s[inicio+3:fim], sub_word_padrao)\n",
        "              s = s.replace(sub_word_padrao+'s', sub_word_padrao)\n",
        "              init = s.find(s[s.find(word_sub)])\n",
        "              init = s.find(word_sub)\n",
        "              end = init + len(sub_word_padrao) + 3\n",
        "              #print(\"2 ------\",s[init:end], \"2.1--\",s[inicio:fim])\n",
        "              inicio, fim = init, end\n",
        "              break\n",
        "          except: break\n",
        "      #2 dormitórios\n",
        "      elif s[s.find(word_find_sub_not_replace)-1] == ' ' and s[s.find(word_find_sub_not_replace)-2] != '0' and re.findall('\\d',s[s.find(word_find_sub_not_replace)-2]) != [] and s.find(word_find_sub_not_replace) != -1 :\n",
        "        for x in range(18):\n",
        "          try: \n",
        "            if s[s.find(word_find_sub_not_replace)+x]==' ' or s[s.find(word_find_sub_not_replace)+x]==';' or s[s.find(word_find_sub_not_replace)+x]=='.': \n",
        "              #print(\"opa\",s[s.find(word_find_sub_not_replace)-2:s.find(word_find_sub_not_replace)+x])\n",
        "              inicio = s.find(word_find_sub_not_replace)-2\n",
        "              fim = s.find(word_find_sub_not_replace)+x\n",
        "              if option_ner == 1: return inicio, fim\n",
        "              word_sub = s[inicio:fim].replace(s[inicio+2:fim], sub_word_padrao)\n",
        "              s = s.replace(s[inicio+2:fim], sub_word_padrao)\n",
        "              #if s[fim-2:fim+1] == 'ss': s=s.replace(s[fim-2:fim+1], sub_word_padrao)\n",
        "              s = s.replace(sub_word_padrao+'s', sub_word_padrao)\n",
        "              init = s.find(word_sub)\n",
        "              end = init + len(sub_word_padrao) + 2\n",
        "              #print(\"2 ------\",s[init:end], \"2.1--\",s[inicio:fim])              \n",
        "              inicio, fim = init, end              \n",
        "              break\n",
        "          except:break\n",
        "      #dormitórios 2 e dormitórios2\n",
        "      else:\n",
        "        space = 0\n",
        "        for j in range(30):\n",
        "          if s.find(word_find) != -1:\n",
        "            try:\n",
        "              if s[s.find(word_find)+j] == ',':break\n",
        "              if s[s.find(word_find)+j] == ' ': space +=1\n",
        "              if space == 2:break\n",
        "              if s[s.find(word_find)+j] in list_numbers:\n",
        "                inicio = s.find(word_find)\n",
        "                fim = s.find(word_find)+j\n",
        "                if option_ner == 1:return inicio, fim+1\n",
        "                if s[fim] == '0' or re.findall('\\d', s[fim]) == []: fim -=1            \n",
        "                if s[fim] == ' ': fim -=1\n",
        "                s = s.replace(sub_word_padrao+'s', sub_word_padrao)\n",
        "                init = s.find(word_find)\n",
        "                if s.find(sub_word_padrao+'s') != -1:\n",
        "                  end = init + len(sub_word_padrao) + 2\n",
        "                else: end = fim+1\n",
        "                inicio, fim = init, end\n",
        "                if s[end] == ' ':end -=1\n",
        "                break\n",
        "            except: break\n",
        "      if option_ner == 1 or option_ner == 2:\n",
        "        break\n",
        "    else: \n",
        "      if option_ner == 2:return sentence_find\n",
        "      else: return 0,0\n",
        "    s = s.replace('dormitoriositórios', 'dormitorios')\n",
        "  if option_ner == 2: return s\n",
        "  if option_ner == 1 and inicio != '' and fim != '' and inicio !=-1 and fim != -1:\n",
        "    return inicio, fim, tag, s\n",
        "  return 0,0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UIn5hBVqH6t"
      },
      "outputs": [],
      "source": [
        "def removeTags(text, list_tags_more_spaces = 'null', imob_excec='null', list_regex_excec='null'):\n",
        "\n",
        "  #scripts = re.compile(r'<script.*?/script>')\n",
        "  scripts = re.compile(r'<(script).*?</\\1>(?s)')\n",
        "  css = re.compile(r'<style.*?/style>')\n",
        "  header = re.compile(r'<header.*?/header>')\n",
        "  footer = re.compile(r'<footer.*?/footer>')\n",
        "  tags = re.compile(r'<.*?>')\n",
        "  exc = re.compile(r'\\xa0')\n",
        "  nbsp = re.compile(r'&nbsp;')\n",
        "  nbsp1 = re.compile(r'&nbsp')\n",
        "  nbsp2 = re.compile(r'nbsp')\n",
        "  amp = re.compile(r'&amp;')\n",
        "  ampliar = re.compile(r'Ampliar')\n",
        "  direcao = re.compile(r' -->')\n",
        "\n",
        "  if list_tags_more_spaces != 'null':\n",
        "    for regex_more_space in list_tags_more_spaces:\n",
        "      text = re.sub(regex_more_space, '     [SPACE]     ', text)\n",
        "\n",
        "  if list_regex_excec != 'null':\n",
        "    for regex_excec in list_regex_excec:\n",
        "      text = re.sub(regex_excec, '', text)\n",
        "\n",
        "  text = scripts.sub('', text)\n",
        "  text = css.sub('', text)\n",
        "  text = header.sub(' ', text)\n",
        "  text = tags.sub(' ', text)\n",
        "  text = exc.sub(' ', text)\n",
        "  text = nbsp.sub(' ', text)\n",
        "  text = nbsp1.sub(' ', text)\n",
        "  text = nbsp2.sub(' ', text)\n",
        "  text = amp.sub(' ', text)\n",
        "  text = ampliar.sub('', text)\n",
        "  text = direcao.sub(' ', text)\n",
        "  text = footer.sub('', text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW01a9of6e3C"
      },
      "source": [
        "## Processos textos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2D-NEGAqz7za"
      },
      "outputs": [],
      "source": [
        "def regexDormsNumberBefore(text, tag):\n",
        "    list_words = []\n",
        "    label = \"TAGS\"\n",
        "    doc = nlp(text)\n",
        "    ex = r'\\d{1,2}\\s*?'+tag+'[\\w]*[(s)]*?[-:)(]*?'\n",
        "    expression = re.compile(ex)\n",
        "    for match in re.finditer(expression, doc.text):\n",
        "        already = 0\n",
        "        start, end = match.span()\n",
        "        span = doc.char_span(start, end, label=label)\n",
        "        if span:\n",
        "          for i in range(0,40):\n",
        "            if end+i >= len(text):break\n",
        "            if text[end+i] != ' ':\n",
        "              if re.findall('\\d', text[end+i]) != []:\n",
        "                already = 1\n",
        "              break\n",
        "        if already == 0 and start != -1:\n",
        "          list_words.append({'palavra':span, 'inicio':start, 'fim':end})\n",
        "    return list_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1_McIEozwNw"
      },
      "outputs": [],
      "source": [
        "def regexDormsNumberNext(text, tag):\n",
        "    list_words = []\n",
        "    label = \"TAGS\"\n",
        "    doc = nlp(text)\n",
        "    ex = tag+'[\\w]*[(s)]*?[-:)(]*?\\s*?\\d{1,2}'\n",
        "    already = 0\n",
        "    expression = re.compile(ex)\n",
        "    for match in re.finditer(expression, doc.text):\n",
        "        start, end = match.span()\n",
        "        span = doc.char_span(start, end, label=label)\n",
        "        if span:\n",
        "          #se tiver um número antes, não adiciona\n",
        "          for i in range(1,40):\n",
        "            if text[start-i] != ' ':\n",
        "              if re.findall('\\d', text[start-i]) != []:\n",
        "                already = 1\n",
        "              break\n",
        "        if already == 0 and start != -1:\n",
        "          list_words.append({'palavra':span, 'inicio':start, 'fim':end})\n",
        "    return list_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgLSTB-tznH8"
      },
      "outputs": [],
      "source": [
        "#list_entities = [(132, 140, 'TIPO'), (237, 277, 'ADDRESS'), (224, 229, 'COD'), (294, 306, 'VALOR'), (311, 323, 'VALOR'), (766, 779, 'VALOR'), (790, 803, 'VALOR'), (828, 840, 'VALOR'), (766, 779, 'VALOR'), (1379, 1392, 'VALOR'), (1379, 1392, 'VALOR'), (1644, 1657, 'VALOR'), (1644, 1657, 'VALOR'), (1924, 1937, 'VALOR'), (2055, 2068, 'VALOR'), (2179, 2192, 'VALOR'), (2349, 2362, 'VALOR'), (1644, 1657, 'VALOR'), (2055, 2068, 'VALOR'), (2786, 2799, 'VALOR'), (3427, 3438, 'VALOR'), (486, 495, 'AREA'), (524, 532, 'AREA'), (1333, 1340, 'AREA'), (1333, 1340, 'AREA'), (1609, 1618, 'AREA'), (1742, 1751, 'AREA'), (1888, 1897, 'AREA'), (2020, 2029, 'AREA'), (2143, 2152, 'AREA'), (2301, 2310, 'AREA'), (2457, 2466, 'AREA'), (2604, 2613, 'AREA'), (2750, 2759, 'AREA'), (2869, 2878, 'AREA')]\n",
        "def verifyUniquesEntities(list_entities, tuple_tags, modo):\n",
        "  if modo == 'insert':\n",
        "    list_entities2 = []\n",
        "    list_entities2.append(list_entities)\n",
        "    for tag_word in tuple_tags:\n",
        "      ai = tag_word[0]\n",
        "      af = tag_word[1]\n",
        "      l = [x for x in range(ai, af+1)]\n",
        "      for j in range(len(list_entities)):\n",
        "          if list_entities[j][0] not in l and list_entities[j][1] not in l:\n",
        "            list_entities2.append(tag_word)\n",
        "    return list_entities2\n",
        "  #se não tiver outra tag entre o índice de outra tag\n",
        "  elif modo == 'certify':\n",
        "    for tag_word in tuple_tags:\n",
        "      ai = tag_word[0]\n",
        "      af = tag_word[1]\n",
        "      l = [x for x in range(ai, af+1)]\n",
        "      for j in range(len(list_entities)):\n",
        "          if list_entities[j][0] in l or list_entities[j][1] in l:\n",
        "            return 0 #já tem uma tag entre os índices\n",
        "    return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv65XQfTdfCL"
      },
      "outputs": [],
      "source": [
        "def removeTags(text, list_tags_more_spaces = 'null', list_remove_tags = 'null', imob_excec='null', list_regex_excec='null'):\n",
        "\n",
        "  #scripts = re.compile(r'<script.*?/script>')\n",
        "  scripts = re.compile(r'<(script).*?</\\1>(?s)')\n",
        "  css = re.compile(r'<style.*?/style>')\n",
        "  header = re.compile(r'<header.*?/header>')\n",
        "  footer = re.compile(r'<footer.*?/footer>')\n",
        "  tags = re.compile(r'<.*?>')\n",
        "  exc = re.compile(r'\\xa0')\n",
        "  nbsp = re.compile(r'&nbsp;')\n",
        "  nbsp1 = re.compile(r'&nbsp')\n",
        "  nbsp2 = re.compile(r'nbsp')\n",
        "  amp = re.compile(r'&amp;')\n",
        "  ampliar = re.compile(r'Ampliar')\n",
        "  direcao = re.compile(r' -->')\n",
        "\n",
        "  if list_remove_tags != 'null':\n",
        "    for r in list_remove_tags:\n",
        "      classe = r['Class']\n",
        "      id = r['id']\n",
        "      tag = r['TAG']\n",
        "      soup_r = BeautifulSoup(text,\"html5\")\n",
        "      text = soup_r\n",
        "      try:\n",
        "        if classe != '':\n",
        "          soup_r.find(class_ = classe).decompose()\n",
        "        if id != '':\n",
        "          soup_r.find(id = id).decompose()\n",
        "        if tag != '':\n",
        "          soup_r.find(tag).decompose()\n",
        "      except:\n",
        "        print(\"ERROR DECOMPOSE TAG REMOVE\")\n",
        "      text = str(text)\n",
        "\n",
        "  if list_tags_more_spaces != 'null':\n",
        "    for regex_more_space in list_tags_more_spaces:\n",
        "      text = re.sub(regex_more_space, '     [SPACE]     ', text)\n",
        "\n",
        "  if list_regex_excec != 'null':\n",
        "    for regex_excec in list_regex_excec:\n",
        "      text = re.sub(regex_excec, '', text)\n",
        "\n",
        "  text = scripts.sub('', text)\n",
        "  text = css.sub('', text)\n",
        "  text = header.sub(' ', text)\n",
        "  text = tags.sub(' ', text)\n",
        "  text = exc.sub(' ', text)\n",
        "  text = nbsp.sub(' ', text)\n",
        "  text = nbsp1.sub(' ', text)\n",
        "  text = nbsp2.sub(' ', text)\n",
        "  text = amp.sub(' ', text)\n",
        "  text = ampliar.sub('', text)\n",
        "  text = direcao.sub(' ', text)\n",
        "  text = footer.sub('', text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIMMA3TaqH6p"
      },
      "outputs": [],
      "source": [
        "def regexDormsNumberBefore(text, tag):\n",
        "    list_words = []\n",
        "    label = \"TAGS\"\n",
        "    doc = nlp(text)\n",
        "    ex = r'\\d{1,2}\\s*?'+tag+'[\\w]*[(s)]*?[-:)(]*?'\n",
        "    expression = re.compile(ex)\n",
        "    for match in re.finditer(expression, doc.text):\n",
        "        already = 0\n",
        "        start, end = match.span()\n",
        "        span = doc.char_span(start, end, label=label)\n",
        "        if span:\n",
        "          for i in range(0,40):\n",
        "            if end+i >= len(text):break\n",
        "            if text[end+i] != ' ':\n",
        "              if re.findall('\\d', text[end+i]) != []:\n",
        "                already = 1\n",
        "              break\n",
        "        if already == 0 and start != -1:\n",
        "          list_words.append({'palavra':span, 'inicio':start, 'fim':end})\n",
        "    return list_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvUEOs4uqH6q"
      },
      "outputs": [],
      "source": [
        "def regexDormsNumberNext(text, tag):\n",
        "    list_words = []\n",
        "    label = \"TAGS\"\n",
        "    doc = nlp(text)\n",
        "    ex = tag+'[\\w]*[(s)]*?[-:)(]*?\\s*?\\d{1,2}'\n",
        "    already = 0\n",
        "    expression = re.compile(ex)\n",
        "    for match in re.finditer(expression, doc.text):\n",
        "        start, end = match.span()\n",
        "        span = doc.char_span(start, end, label=label)\n",
        "        if span:\n",
        "          #se tiver um número antes, não adiciona\n",
        "          for i in range(1,40):\n",
        "            if text[start-i] != ' ':\n",
        "              if re.findall('\\d', text[start-i]) != []:\n",
        "                already = 1\n",
        "              break\n",
        "        if already == 0 and start != -1:\n",
        "          list_words.append({'palavra':span, 'inicio':start, 'fim':end})\n",
        "    return list_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meBVBJAeqH6s"
      },
      "outputs": [],
      "source": [
        "#list_entities = [(132, 140, 'TIPO'), (237, 277, 'ADDRESS'), (224, 229, 'COD'), (294, 306, 'VALOR'), (311, 323, 'VALOR'), (766, 779, 'VALOR'), (790, 803, 'VALOR'), (828, 840, 'VALOR'), (766, 779, 'VALOR'), (1379, 1392, 'VALOR'), (1379, 1392, 'VALOR'), (1644, 1657, 'VALOR'), (1644, 1657, 'VALOR'), (1924, 1937, 'VALOR'), (2055, 2068, 'VALOR'), (2179, 2192, 'VALOR'), (2349, 2362, 'VALOR'), (1644, 1657, 'VALOR'), (2055, 2068, 'VALOR'), (2786, 2799, 'VALOR'), (3427, 3438, 'VALOR'), (486, 495, 'AREA'), (524, 532, 'AREA'), (1333, 1340, 'AREA'), (1333, 1340, 'AREA'), (1609, 1618, 'AREA'), (1742, 1751, 'AREA'), (1888, 1897, 'AREA'), (2020, 2029, 'AREA'), (2143, 2152, 'AREA'), (2301, 2310, 'AREA'), (2457, 2466, 'AREA'), (2604, 2613, 'AREA'), (2750, 2759, 'AREA'), (2869, 2878, 'AREA')]\n",
        "def verifyUniquesEntities(list_entities, tuple_tags, modo):\n",
        "  if modo == 'insert':\n",
        "    list_entities2 = []\n",
        "    list_entities2.append(list_entities)\n",
        "    for tag_word in tuple_tags:\n",
        "      ai = tag_word[0]\n",
        "      af = tag_word[1]\n",
        "      l = [x for x in range(ai, af+1)]\n",
        "      for j in range(len(list_entities)):\n",
        "          if list_entities[j][0] not in l and list_entities[j][1] not in l:\n",
        "            list_entities2.append(tag_word)\n",
        "    return list_entities2\n",
        "  #se não tiver outra tag entre o índice de outra tag\n",
        "  elif modo == 'certify':\n",
        "    for tag_word in tuple_tags:\n",
        "      ai = tag_word[0]\n",
        "      af = tag_word[1]\n",
        "      l = [x for x in range(ai, af+1)]\n",
        "      for j in range(len(list_entities)):\n",
        "          if list_entities[j][0] in l or list_entities[j][1] in l:\n",
        "            return 0 #já tem uma tag entre os índices\n",
        "    return 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah72vDEGqfuy"
      },
      "source": [
        "## Bairros e Imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNGiKqk1qhdu"
      },
      "outputs": [],
      "source": [
        "#ESSE\n",
        "def getBairros(page):\n",
        "  doc = nlp(page.lower())\n",
        "  expression = re.compile('bairro')\n",
        "  len_bairro = len('bairro')\n",
        "  caracteres_especiais = [',', '.', '-', '|','(',')']\n",
        "  list_bairros = []\n",
        "  for match in re.finditer(expression, doc.text):\n",
        "    start, end = match.span()\n",
        "    span = doc.char_span(start, end, label='BAIRRO')\n",
        "    if span:\n",
        "      cont_letras, cont_space = 0,0\n",
        "      for i in range(0,60):\n",
        "        try:\n",
        "          if page[end+len_bairro+i] == ' ' and cont_letras > 3:\n",
        "            cont_space +=1\n",
        "          elif page[end+len_bairro+i] != ' ':\n",
        "            cont_space = 0\n",
        "            cont_letras +=1\n",
        "          else: cont_space = 0\n",
        "        except:break\n",
        "        if cont_space >= 2 or page[end+len_bairro+i] in caracteres_especiais:\n",
        "          #print(page[start+len_bairro: end+len_bairro+i])\n",
        "          list_bairros.append(page[start+len_bairro: end+len_bairro+i].strip())\n",
        "          break\n",
        "  return list_bairros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsYzaH50qiyx"
      },
      "outputs": [],
      "source": [
        "def getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao = []):\n",
        "  imgs = soup.find_all(tag_img)\n",
        "  dict_imgs = {}\n",
        "  for url_check in urls_checks:\n",
        "    for attr_check in attrs_check:\n",
        "      list_imgs = []\n",
        "      for img in imgs:\n",
        "        try:\n",
        "          url = img.get(attr_check)\n",
        "          if url_check in url.lower() and url_check != '':\n",
        "            list_imgs.append(url)\n",
        "          elif url_check in url.lower():\n",
        "            list_imgs.append(url)\n",
        "        except:pass\n",
        "      list_imgs = list(set(list_imgs))\n",
        "      list_imgs_return = []\n",
        "      for ig in list_imgs:\n",
        "        ops = 0\n",
        "        for check in check_excessao:\n",
        "          if check in ig:\n",
        "            ops = 1\n",
        "        if ops == 0:\n",
        "          list_imgs_return.append(ig)\n",
        "        dict_imgs[attr_check] = list_imgs_return\n",
        "  return dict_imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOCmvnRDqj-H"
      },
      "outputs": [],
      "source": [
        "def getImagesAriotti(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao = []):\n",
        "  imgs = soup.find_all(tag_img)\n",
        "  dict_imgs = {}\n",
        "  list_imgs_return = []\n",
        "  for url_check in urls_checks:\n",
        "    for attr_check in attrs_check:\n",
        "      list_imgs = []\n",
        "      for img in imgs:\n",
        "        try:\n",
        "          url = img.get(attr_check)\n",
        "          #print(url)\n",
        "          if url_check in url.lower() and url_check != '':\n",
        "            list_imgs.append(url)\n",
        "          elif url_check in url.lower():\n",
        "            list_imgs.append(url)\n",
        "        except:pass\n",
        "      list_imgs = list(set(list_imgs))\n",
        "      for ig in list_imgs:\n",
        "        ops = 0\n",
        "        for check in check_excessao:\n",
        "          if check in ig:\n",
        "            ops = 1\n",
        "        if ops == 0:\n",
        "          list_imgs_return.append(ig)\n",
        "      dict_imgs[attr_check] = list(set(list_imgs_return))\n",
        "  return dict_imgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCPsRNgjToA2"
      },
      "source": [
        "#Run 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZbtZGZTuJ4s"
      },
      "outputs": [],
      "source": [
        "def regexSpacy1(page):\n",
        "  page = page.strip()\n",
        "\n",
        "  human_verify = False\n",
        "\n",
        "  suites_regex, dorms_regex, vagas_regex, banheiros_regex = [], [], [], []\n",
        "  for name_tag in ['quarto', 'dorm', 'vaga', 'suít', 'suit', 'banheiro']:\n",
        "    tag_insert = ''\n",
        "    l = regexDormsNumberNext(page.lower(), name_tag)\n",
        "    if name_tag == 'quarto' or name_tag == 'dorm': tag_insert = 'DORMS'\n",
        "    if name_tag == 'vaga': tag_insert = 'VAGAS'\n",
        "    if name_tag == 'suít' or name_tag == 'suit': tag_insert = 'SUITES'\n",
        "    if name_tag == 'banheiro': tag_insert = 'BANHEIRO'\n",
        "    for j in range(len(l)):\n",
        "      cert = verifyUniquesEntities([], [(l[j]['inicio'], l[j]['fim'], tag_insert)], modo='certify')\n",
        "      if cert == 1:\n",
        "        if tag_insert == 'SUITES': suites_regex.append(page[l[j]['inicio']:l[j]['fim']])\n",
        "        if tag_insert == 'DORMS': dorms_regex.append(page[l[j]['inicio']:l[j]['fim']])\n",
        "        if tag_insert == 'VAGAS': vagas_regex.append(page[l[j]['inicio']:l[j]['fim']])\n",
        "        if tag_insert == 'BANHEIRO': banheiros_regex.append(page[l[j]['inicio']:l[j]['fim']])\n",
        "\n",
        "  for name_tag in ['quarto', 'dorm', 'vaga', 'suít', 'suit', 'banheiro']:\n",
        "    tag_insert = ''\n",
        "    l = regexDormsNumberBefore(page.lower(), name_tag)\n",
        "    if name_tag == 'quarto' or name_tag == 'dorm': tag_insert = 'DORMS'\n",
        "    if name_tag == 'vaga': tag_insert = 'VAGAS'\n",
        "    if name_tag == 'suít' or name_tag == 'suit': tag_insert = 'SUITES'\n",
        "    if name_tag == 'banheiro': tag_insert = 'BANHEIRO'\n",
        "    for j in range(len(l)):\n",
        "      cert = verifyUniquesEntities([], [(l[j]['inicio'], l[j]['fim'], tag_insert)], modo='certify')\n",
        "      if cert == 1:\n",
        "        if tag_insert == 'SUITES': suites_regex.append(page[l[j]['inicio']:l[j]['fim']])\n",
        "        if tag_insert == 'DORMS': dorms_regex.append(page[l[j]['inicio']:l[j]['fim']])\n",
        "        if tag_insert == 'VAGAS': vagas_regex.append(page[l[j]['inicio']:l[j]['fim']])\n",
        "        if tag_insert == 'BANHEIRO': banheiros_regex.append(page[l[j]['inicio']:l[j]['fim']])\n",
        "\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "  pattern_dorms = [ {\"LIKE_NUM\": True}, {\"TEXT\":{\"REGEX\":\"\\s?dorm\"}}]\n",
        "  pattern_suit = [ {\"LIKE_NUM\": True}, {\"TEXT\":{\"REGEX\":\"\\s?suít\"}} ]\n",
        "  pattern_suit2 = [ {\"LIKE_NUM\": True}, {\"TEXT\":{\"REGEX\":\"\\s?suit\"}} ]\n",
        "  pattern_banheiro = [ {\"LIKE_NUM\": True}, {\"TEXT\":{\"REGEX\":\"\\s?banheiro\"}} ]\n",
        "  pattern_vagas = [ {\"LIKE_NUM\": True}, {\"TEXT\":{\"REGEX\":\"\\s?vagas\"}} ]\n",
        "  pattern_vagas_garag = [ {\"LIKE_NUM\": True}, {\"TEXT\":{\"REGEX\":\"\\s?garagem\"}} ]\n",
        "\n",
        "  matcher.add(\"Dorms\", [pattern_dorms])\n",
        "  matcher.add(\"Suites\", [pattern_suit])\n",
        "  matcher.add(\"Suites\", [pattern_suit2])\n",
        "  matcher.add(\"Banheiro\", [pattern_banheiro])\n",
        "  matcher.add(\"Vagas\", [pattern_vagas])\n",
        "  matcher.add(\"Vagas\", [pattern_vagas_garag])\n",
        "\n",
        "  doc = nlp(page.lower())\n",
        "  matches = matcher(doc)\n",
        "  dorms_spacy_rg,suites_spacy_rg,banheiro_spacy_rg,vagas_spacy_rg = [],[],[],[]\n",
        "  for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  \n",
        "    span = doc[start:end] \n",
        "    if string_id == 'Dorms': dorms_spacy_rg.append(span.text)\n",
        "    if string_id == 'Suites': suites_spacy_rg.append(span.text)\n",
        "    if string_id == 'Banheiro': banheiro_spacy_rg.append(span.text)\n",
        "    if string_id == 'Vagas': vagas_spacy_rg.append(span.text)\n",
        "\n",
        "  return suites_regex, dorms_regex, vagas_regex, banheiros_regex, dorms_spacy_rg,suites_spacy_rg,banheiro_spacy_rg,vagas_spacy_rg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDBz9PpqwCai"
      },
      "outputs": [],
      "source": [
        "def coordenadasRegex(soup_regex, imob):\n",
        "  address_regex_iframe = ''\n",
        "  list_coords, list_lngs, list_lats = [], [], []\n",
        "  try:\n",
        "    r = requests.get(soup_regex.find('iframe').get('src'), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    #list_coords = list(set(re.findall('-\\d{1,3}[.]\\d+', r.text)))\n",
        "    list_lngs.append(list(set(re.findall('-52[.]\\d+', r.text))))\n",
        "    list_lats.append(list(set(re.findall('-28[.]\\d+', r.text))))\n",
        "    text = r.text\n",
        "    end = text.find('Brazil')+len('Brazil')\n",
        "    for i in range(end,1,-1):\n",
        "      if text[i-1] == '\"':\n",
        "        address_regex_iframe = text[i:end]\n",
        "        break\n",
        "  except:pass\n",
        "\n",
        "  lat_maior, lng_maior = 'null','null'\n",
        "\n",
        "  list_lngs.append(list(set(re.findall('-52[.]\\d+', res.text))))\n",
        "  list_lats.append(list(set(re.findall('-28[.]\\d+', res.text))))\n",
        "\n",
        "  tam = 0\n",
        "  for lng in list_lngs:\n",
        "    if len(lng) > tam:\n",
        "      tam = len(lng)\n",
        "      lng_maior = lng[0]\n",
        "\n",
        "  tam = 0\n",
        "  for lat in list_lats:\n",
        "    if len(lat) > tam:\n",
        "      tam = len(lat)\n",
        "      lat_maior = lat[0]\n",
        "  if lat_maior == 'null' or lng_maior == 'null':\n",
        "    list_coords = []\n",
        "  else:list_coords = [lat_maior, lng_maior]\n",
        "  if list_coords == ['-28.2529169', '-52.4044902'] and imob == 'R. Classic': list_coords = []\n",
        "\n",
        "  return list_coords, address_regex_iframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F9qeCKyw_3k"
      },
      "outputs": [],
      "source": [
        "def findWordPerIndice(desc):\n",
        "    desc_ner = desc.lower()\n",
        "\n",
        "    suites1, suites2, dorms, vagas, quartos, banheiros_r = [],[],[],[],[],[]\n",
        "    init, fim = findWordsAndIndices(desc_ner, 'sui', 'SUITES', sub_word_padrao='suí', option_ner = 1, sentence_ner=desc_ner)\n",
        "    if desc_ner[init:fim] == '':suites1 = []\n",
        "    else:suites1.append(desc_ner[init:fim])\n",
        "\n",
        "    init, fim = findWordsAndIndices(desc_ner, 'sui', 'SUITES', sub_word_padrao='suí', option_ner = 1, sentence_ner=desc_ner)\n",
        "    #suites2 = desc_ner[init:fim]\n",
        "    if desc_ner[init:fim] == '':suites2 = []\n",
        "    else:suites2.append(desc_ner[init:fim])\n",
        "\n",
        "    init, fim = findWordsAndIndices(desc_ner, 'dorm', 'DORMS', sub_word_padrao='dorm', option_ner = 1, sentence_ner=desc_ner)\n",
        "    #dorms = desc_ner[init:fim]\n",
        "    if desc_ner[init:fim] == '':dorms = []\n",
        "    else:dorms.append(desc_ner[init:fim])\n",
        "\n",
        "    init, fim = findWordsAndIndices(desc_ner, 'vaga', 'VAGAS', sub_word_padrao='vaga', option_ner = 1, sentence_ner=desc_ner)\n",
        "    #vagas = desc_ner[init:fim]\n",
        "    if desc_ner[init:fim] == '':vagas = []\n",
        "    else:vagas.append(desc_ner[init:fim])\n",
        "  \n",
        "    init, fim = findWordsAndIndices(desc_ner, 'quarto', 'DORMS', sub_word_padrao='dorm', option_ner = 1, sentence_ner=desc_ner)\n",
        "    #quartos = desc_ner[init:fim]\n",
        "    if desc_ner[init:fim] == '':quartos = []\n",
        "    else:quartos.append(desc_ner[init:fim])\n",
        "    \n",
        "    init, fim = findWordsAndIndices(desc_ner, 'banheiro', 'BANHEIROS', sub_word_padrao='banheiro', option_ner = 1, sentence_ner=desc_ner)\n",
        "    #banheiros_r = desc_ner[init:fim]\n",
        "    if desc_ner[init:fim] == '':banheiros_r = []\n",
        "    else:banheiros_r.append(desc_ner[init:fim])\n",
        "\n",
        "    area_m = re.findall('\\d+[,.]?\\d+\\s?m²', desc_ner)\n",
        "    area_ha = re.findall('\\d+[,.]?\\d+\\s?ha', desc_ner)\n",
        "    if area_ha == []:area_ha = re.findall('\\d+[,.]?\\d+\\s?hect', desc_ner)\n",
        "    \n",
        "    desc_ner = desc_ner.replace('_', ' ')\n",
        "    if findWordsAndIndices(desc_ner, 'sui', 'SUITES', sub_word_padrao='suítes', option_ner = 2, sentence_ner=desc_ner) != (0, 0): desc_ner = findWordsAndIndices(desc_ner, 'sui', 'SUITES', sub_word_padrao='suítes', option_ner = 2, sentence_ner=desc_ner)\n",
        "    if findWordsAndIndices(desc_ner, 'suí', 'SUITES', sub_word_padrao='suítes', option_ner = 2, sentence_ner=desc_ner) != (0, 0): desc_ner = findWordsAndIndices(desc_ner, 'sui', 'SUITES', sub_word_padrao='suítes', option_ner = 2, sentence_ner=desc_ner)\n",
        "    if findWordsAndIndices(desc_ner, 'vaga', 'VAGAS', sub_word_padrao='vagas', option_ner = 2, sentence_ner=desc_ner) != (0, 0): desc_ner = findWordsAndIndices(desc_ner, 'vaga', 'VAGAS', sub_word_padrao='vagas', option_ner = 2, sentence_ner=desc_ner)\n",
        "    if findWordsAndIndices(desc_ner, 'dorm', 'DORMS', sub_word_padrao='dormitorios', option_ner = 2, sentence_ner=desc_ner) != (0, 0): desc_ner = findWordsAndIndices(desc_ner, 'dorm', 'DORMS', sub_word_padrao='dormitorios', option_ner = 2, sentence_ner=desc_ner)\n",
        "    if findWordsAndIndices(desc_ner, 'banheiro', 'BANHEIROS', sub_word_padrao='banheiros', option_ner = 2, sentence_ner=desc_ner) != (0,0): desc_ner = findWordsAndIndices(desc_ner, 'banheiro', 'BANHEIROS', sub_word_padrao='banheiros', option_ner = 2, sentence_ner=desc_ner)\n",
        "\n",
        "    return suites1, suites2, dorms, vagas, quartos, banheiros_r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWJpjel-tS3h"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = requests.get('https://www.benvegnuimoveis.com.br/imovel/locacao-anual-casas-04-dormitorio-em-passo-fundo/casa-4-dorm-(1-suite)--vila-luiza/14405',\n",
        "    headers= {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'})"
      ],
      "metadata": {
        "id": "zoRtGMLTaHMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_bruto = res.text\n",
        "page = re.sub('\\s+', ' ', page_bruto)\n",
        "page = cleaner.clean_html(page)\n",
        "soup = BeautifulSoup(res.text)"
      ],
      "metadata": {
        "id": "UHjvMie5aSmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page.find('cookies para personal')\n",
        "page[50100:50600]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "e-bK6xDIbxLq",
        "outputId": "38fb42fd-9f06-4d18-92b5-748cb9fde8c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' background: #2BA7B2; } .fecharIframe{ background-color: #2BA7B2 !important; }</style><div class=\"modal fade modalCookies\" id=\"modal-cookies\" tabindex=\"-1\"><div class=\"modal-dialog modal-lg\"><div class=\"modal-content\"></div></div></div><div class=\"avisoCookiesFixo cookiesmodal cookiesBottom\" id=\"cookiesmodal\"><div class=\"container\"><div class=\"row\"><div class=\"pull-left txtCookies\"><p>Usamos cookies para personalizar e melhorar a sua experiência. Ao navegar neste site, você concorda com a nossa '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = removeTags(page, list_tags_more_spaces=['</li>'], list_remove_tags = [({'Class':'master-slider', 'id':'', 'TAG':''})], imob_excec='Benvegnú', list_regex_excec=['<div id=\"footer.*','<div class=\"modal fade modalCookies.*','id=\"modal-cookies.*','<div class=\"avisoCookiesFixo.*', '<div class=\"master-slider ms-skin-light-3 ms-wk.*<div class=\"clearfix\">', '<div class=\"imovel__detalhe__simulador\".*', '<div id=\"similares.*', '<section class=\"imovel__similar produto\".*'])\n",
        "page = sentence.lower()\n",
        "list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "page"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "sRhIOBESZ0s9",
        "outputId": "f9e86d98-b643-4da3-b3c0-0f9a07ad3499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR DECOMPOSE TAG REMOVE\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'    casa 4 dorm (1 suíte) - vila luiza | vila luiza | passo fundo                         (54) 99688.6198                  alugar imoveis    »     casas passo fundo    »     casa 4 dormitorios        casa 4 dorm (1 suíte) - vila luiza      14    cód. l14405        4 dorm. sendo 1 suíte        rua coronel gervasio annes , vila luiza - passo fundo / rs             r$1.120.000,00                                      vaga(s):   6       [space]        área do terreno:  690.00 m²      [space]        privativa casa:  426.00 m²      [space]        mobília:  sim      [space]         área total do terreno  690.00m²  :  11.50m  de largura x  60.00m  de comprimento.             características acabamentos           acabamento em gesso     [space]      laje     [space]      piso cerâmico     [space]      piso de porcelanato     [space]      rebaixamento em gesso     [space]               características da casa           alarme     [space]      alarme monitorado     [space]      ar condicionado     [space]      área de serviço     [space]      churrasqueira     [space]      cozinha americana     [space]      cozinha fechada     [space]      depósito     [space]      escritório     [space]      garagem p/ 4 carros     [space]      interfone     [space]      ótima posição solar     [space]      patio frente     [space]      patio fundos     [space]      portão eletrônico     [space]      sacada     [space]      sala de estar     [space]      salão de festas     [space]      suíte c/ closet     [space]                      linda casa 4 dorm com 1 suíte! vila luiza!            telhado shingle  importado      [space]       aquecimento solar para água      [space]       interfone      [space]       câmera      [space]       abertura de alumínio gold (aljesa esquadrias)      [space]       forro gesso acartonado      [space]       piso térreo porcelanato italiano      [space]       piso 3º pavimento lâmina de tabuão      [space]       aberturas internas portas laqueadas      [space]       cozinha projetada      [space]       closed      [space]       aberturas espera para automação      [space]       escritório      [space]       suíte com sacada, closed      [space]       imóvel com ótima ventilação e iluminação natural      [space]       muito espaço para viver com conforto e comodidade      [space]               pavimentos:           3º pavimento:  3 dormitórios (ou 1sala/escritório) banheiro social      2º pavimento  escritório acesso individual cozinha americana sala estar/jantar banheiro área de serviço laje externa de 10 x 6 mts      térreo:  churrasqueiras salão de festas banheiro escritório/quarto empregada depósito garagem para 6 carros cobertos churrasqueira externa coberta depósito em baixo da escada      contate-nos para maiores informações!                 forma de pagamento          estudam-se propostas!  aceita imóvel de menor valor como parte do pagamento!                                             avise-nos de seu interesse.  vamos negociar!                 compartilhe esse imóvel          [space]           [space]               [space]                 [space]               [space]               [space]                          localização - casa 4 dorm (1 suíte) - vila luiza    ver rua    ver mapa     '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05TA85McEYoi"
      },
      "outputs": [],
      "source": [
        "df_regex_extract = pd.DataFrame(columns=['Dorms Show','Quartos Show','Suites Show','Vagas Show','Banheiros Show','Garagem Show','Carros Show',\n",
        "    'Area Show','Area Priv Show','Area Terreno Show','Area Global Show','Area Útil Show','Area Construída Show','Area Total Show',\n",
        "    'dorms REGEX sentence','only dorms REGEX','only dorms REGEX 2','dorms unique REGEX','only dorms REGEX 3',\n",
        "    'quartos REGEX sentence','only quartos REGEX','only quartos REGEX 2','only quartos REGEX 3','quartos unique REGEX',\n",
        "    'vagas REGEX sentence', 'carro REGEX sentence', 'garagem REGEX sentence','only vagas REGEX','only vagas REGEX 2',\n",
        "    'only vagas REGEX 3','only garagem REGEX','only garagem REGEX 2','only garagem REGEX 3','only carros REGEX 2',\n",
        "    'only carros REGEX 3','vagas unique REGEX','garagem unique REGEX','carros unique REGEX',\n",
        "    'suites REGEX sentence','only suites REGEX','only suites REGEX 2','only suites REGEX 3','suites unique REGEX',\n",
        "    'banheiros REGEX sentence','only banheiros REGEX','only banheiros REGEX 2','only banheiros REGEX 3','banheiros unique REGEX',\n",
        "    'Dorms OU','Quartos OU','Suites OU','Banheiros OU','Vagas OU','Garagem OU','Carros OU','Box OU',\n",
        "    'only area REGEX', 'only area priv REGEX', 'only area terreno REGEX', 'area REGEX',\n",
        "    'banheiros NER', 'suites NER', 'area NER', 'vagas NER', 'quartos NER',\n",
        "    'dorms NER', 'preço NER', 'dorms REGEX', 'dorms REGEX correct','banheiros REGEX', 'banheiros REGEX correct', 'suites REGEX' , 'suites REGEX correct','vagas REGEX', \n",
        "    'vagas REGEX correct', 'quartos REGEX', 'quartos REGEX correct',\n",
        "    'area REGEX correct', 'area priv REGEX correct', 'preco REGEX correct', 'Sentence', 'Preço All', 'preco REGEX','Imobiliária','Tipo', 'Situacao', 'URL', 'Endereco Soup', 'Pagina'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n75LFN97iYUs"
      },
      "outputs": [],
      "source": [
        "def headerImmobile(dominio_imob_def):\n",
        "  return {\n",
        "      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "      'Accept-Encoding': 'gzip, deflate',\n",
        "      'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "      'Connection': 'keep-alive',\n",
        "      'Host': f'{dominio_imob_def}',\n",
        "      'Origin': f'{dominio_imob_def}',\n",
        "      'Referer': f'https://{dominio_imob_def}',\n",
        "      'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"99\", \"Google Chrome\";v=\"99\"',\n",
        "      'sec-ch-ua-mobile': '?0',\n",
        "      'sec-ch-ua-platform': '\"Windows\"',\n",
        "      'Sec-Fetch-Dest': 'document',\n",
        "      'Sec-Fetch-Mode': 'navigate',\n",
        "      'Sec-Fetch-Site': 'same-origin',\n",
        "      'Sec-Fetch-User': '?1',\n",
        "      'Upgrade-Insecure-Requests': '1',\n",
        "      'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36',\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pvCD3L9nGvA"
      },
      "outputs": [],
      "source": [
        "list_ner = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBUr-nMvkK9-"
      },
      "outputs": [],
      "source": [
        "df_urls_already_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWC1axpyU2eO"
      },
      "outputs": [],
      "source": [
        "#df_urls_extracao_parte_2_ok = pd.DataFrame(columns=['URL', 'Situacao', 'Parte'])\n",
        "#df_urls_extracao_parte_2_ok.to_csv('/content/gdrive/My Drive/NER/DataFrames/URLS/urls_config-26-04.csv', sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U79-S3vJlWr_"
      },
      "outputs": [],
      "source": [
        "name_urls_extract_todos = '/content/gdrive/My Drive/NER/DataFrames/URLS/urls_extract_TODOS-26-04.csv'\n",
        "name_urls_config = '/content/gdrive/My Drive/NER/DataFrames/URLS/urls_config-26-04.csv'\n",
        "name_csv = 'urls_extract_TODOS-26-04.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEIh62mXsjWH"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "  try:\n",
        "    df_urls = pd.read_csv(name_urls_extract_todos, sep='\\t')\n",
        "    df_urls.drop_duplicates('URL', inplace = True)\n",
        "    break\n",
        "  except:\n",
        "    print(\"Dataframe das urls ainda não existe\")\n",
        "    time.sleep(30)\n",
        "    continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxQK-OLfta8b"
      },
      "outputs": [],
      "source": [
        "#arquivo = diretorio/name_csv\n",
        "#timestamp_att_df = arquivo.stat().st_mtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4G4KeRiNDBc"
      },
      "outputs": [],
      "source": [
        "arquivo_urls_config = diretorio/name_urls_config\n",
        "timestamp_att_end = arquivo_urls_config.stat().st_mtime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp_att_end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UU7f1OsApYE",
        "outputId": "af9b414d-6ab3-4dbc-8a6e-3516cd5c0a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1651327902.0"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr98e281yVa_"
      },
      "outputs": [],
      "source": [
        "gerar_error_next_except = 'opa'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name_save_file_csv_and_json = '/content/gdrive/My Drive/NER/DataFrames/Extracao/teste-26-04-parte-1'\n",
        "name_save_file_csv_and_json = '/content/gdrive/My Drive/NER/DataFrames/Extracao/teste-26-04-parte-1-2'\n",
        "name_save_file_csv_and_json = '/content/gdrive/My Drive/NER/DataFrames/Extracao/teste-26-04-parte-1-3'\n",
        "name_save_file_csv_and_json = '/content/gdrive/My Drive/NER/DataFrames/Extracao/teste-26-04-parte-1-4'"
      ],
      "metadata": {
        "id": "5XZJU-l0BDVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZx45B-yMdbk"
      },
      "outputs": [],
      "source": [
        "df_regex_extract = pd.read_csv(f'{name_save_file_csv_and_json}.csv', sep='\\t')\n",
        "list_ner = open(f'{name_save_file_csv_and_json}.json')\n",
        "list_ner = json.load(list_ner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNc4o-PGObh6"
      },
      "outputs": [],
      "source": [
        "def checkTimesIsOpenFile(arquivo_urls_config, timestamp_att_df_start):\n",
        "  timestamp_att_df_now = arquivo_urls_config.stat().st_mtime #ultima modificação no arquivo\n",
        "  seconds_difference_is_chenge_file = (datetime.fromtimestamp(timestamp_att_df_now) - datetime.fromtimestamp(timestamp_att_df_start)).seconds\n",
        "\n",
        "  timestamp_att_df_now_is_open = arquivo_urls_config.stat().st_atime #última vez aberto o arquivo\n",
        "  seconds_difference_is_open_file = (datetime.fromtimestamp(timestamp_att_df_now_is_open) - datetime.fromtimestamp(timestamp_att_df_start)).seconds\n",
        "\n",
        "  print(\"TEMPO DESDE A ÚLTIMA ABERTURA\", seconds_difference_is_open_file, seconds_difference_is_chenge_file)\n",
        "  while seconds_difference_is_open_file < 20 or seconds_difference_is_chenge_file < 20:\n",
        "    print(\"loading seconds urls_config\")\n",
        "    if seconds_difference_is_chenge_file < 20:\n",
        "      time.sleep(20)\n",
        "      timestamp_att_df_now = arquivo_urls_config.stat().st_mtime\n",
        "      seconds_difference_is_chenge_file = (datetime.fromtimestamp(timestamp_att_df_now) - datetime.fromtimestamp(timestamp_att_df_start)).seconds\n",
        "    else:\n",
        "      time.sleep(20)\n",
        "      timestamp_att_df_now_is_open = arquivo_urls_config.stat().st_atime\n",
        "      seconds_difference_is_open_file = (datetime.fromtimestamp(timestamp_att_df_now_is_open) - datetime.fromtimestamp(timestamp_att_df_start)).seconds\n",
        "  return timestamp_att_df_now"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def checkTimesIsOpenFile(arquivo_urls_config):\n",
        "  timestamp_att_df_start = time.time()\n",
        "  timestamp_att_df_now = arquivo_urls_config.stat().st_mtime #ultima modificação no arquivo\n",
        "  seconds_difference_is_change_file = (datetime.fromtimestamp(timestamp_att_df_now) - datetime.fromtimestamp(timestamp_att_df_start)).seconds\n",
        "  \n",
        "  print(\"TEMPO DESDE O ÚLTIMO\", seconds_difference_is_change_file)\n",
        "  while seconds_difference_is_change_file < 20:\n",
        "    print(\"loading seconds urls_config\")\n",
        "    if seconds_difference_is_change_file < 20:\n",
        "      time.sleep(20)\n",
        "      timestamp_att_df_now = arquivo_urls_config.stat().st_mtime\n",
        "      seconds_difference_is_change_file = (datetime.fromtimestamp(timestamp_att_df_now) - datetime.fromtimestamp(timestamp_att_df_start)).seconds\n",
        "  return timestamp_att_df_now"
      ],
      "metadata": {
        "id": "gg4KPdWzAoJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "def checkTimesIsOpenFile(arquivo_urls_config):\n",
        "  timestamp_att_df_start = time.time()\n",
        "  timestamp_att_df_now = arquivo_urls_config.stat().st_mtime #ultima modificação no arquivo\n",
        "  seconds_difference_is_change_file = timestamp_att_df_start - timestamp_att_df_now\n",
        "  \n",
        "  print(\"TEMPO DESDE O ÚLTIMO\", seconds_difference_is_change_file)\n",
        "  while seconds_difference_is_change_file < 30:\n",
        "    print(\"loading seconds urls_config\")\n",
        "    if seconds_difference_is_change_file < 30:\n",
        "      time.sleep(30)\n",
        "      timestamp_att_df_start = time.time()\n",
        "      timestamp_att_df_now = arquivo_urls_config.stat().st_mtime\n",
        "      seconds_difference_is_change_file = timestamp_att_df_start - timestamp_att_df_now\n",
        "  return timestamp_att_df_now"
      ],
      "metadata": {
        "id": "_zvKOASbcuHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_regex_extract.shape, len(list_ner)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNfeBmeo8qme",
        "outputId": "36b47b71-cfab-4133-b754-86b18fbcc929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((0, 89), 0)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urls_config = pd.read_csv(name_urls_config, sep='\\t')"
      ],
      "metadata": {
        "id": "gBaxR4iE1_ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_urls = pd.read_csv(name_urls_extract_todos, sep='\\t')\n",
        "list_urls_config = urls_config[(urls_config['Situacao'] == 'OK') | (urls_config['Situacao'] == 'Em extracao')]['URL'].to_list()\n",
        "#df_urls = df_urls.loc[[x not in list_urls_config for x in df_urls['URL'].tolist()]]\n",
        "#rows = np.random.choice(df_urls.index.values, 50 if df_urls.shape[0] >= 50 else df_urls.shape[0])\n",
        "#df_urls = df_urls.loc[rows]\n",
        "len(list_urls_config), df_urls.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt1v7UbH1Gue",
        "outputId": "5136351e-3ff7-489b-b53e-6b32b406da0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20543, (21109, 7))"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(urls_config[(urls_config['Situacao'] == 'OK') | (urls_config['Situacao'] == 'Em extracao')]['URL'].to_list())\n",
        "[x not in list_urls_config for x in df_urls['URL'].tolist()]"
      ],
      "metadata": {
        "id": "afKt9H0X2EZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_urls.duplicated(subset=['URL']).sum()"
      ],
      "metadata": {
        "id": "c7gotXT83Xod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls_config['Situacao'].value_counts()"
      ],
      "metadata": {
        "id": "txVqCY8108RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "res = requests.get('https://arnelimoveis.com.br/imovel/7127/casa-3-quartos-santa-maria-ii-passo-fundo/')"
      ],
      "metadata": {
        "id": "be1iT4nb6j4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_bruto = res.text\n",
        "\n",
        "page = re.sub('\\s+', ' ', page_bruto)\n",
        "page = cleaner.clean_html(page)\n",
        "soup = BeautifulSoup(res.text)\n",
        "sentence = removeTags(page, list_tags_more_spaces=['</small>'], imob_excec='Arnel', list_regex_excec=['<h2>Semelhantes.*', '<section class=\"pgl-featured pgl-properties card-semelhantes.*\"', \n",
        "                                                                                                            '<div class=\"formEnvia.*', 'card-semelhantes.*' ])\n",
        "page = sentence.lower()"
      ],
      "metadata": {
        "id": "FxhshomE6npP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "osNZHx-v7BnF",
        "outputId": "cb387b86-a205-4e95-f6c4-3a7261a51ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'               casa com 3 quartos, santa maria ii, passo fundo – r$ 430.000,00 – cod. 7127 – arnel imóveis.                                                                                                                                                                                                                                                                                                                                                                                         cod 7127     [space]          casa de 03 dormitórios no bairro santa maria ii para comprar                                  compartilhe                                                             home     busca imóveis    casa de 03 dormitórios no bairro santa maria ii para comprar               casa em alvenaria, disponibilidade para guardar um segundo carro, suíte master no segundo piso.espaços amplos, bem arejados ótima posição solar.               santa maria ii -    passo fundo/   rs             r$ 430.000,00        condomínio: r$ 0,00      iptu: r$ 0,00                                dormitórios   3     3          banheiros   3     3          vaga   1     1                 outras características        area total: 300      area privativa: 65.52      aceita fgts      aceita permuta imóvel      ar condicionado      área de serviço      banheiro social      churrasqueira      cozinha      dimensoes terreno: 12x25      espera para split      conservacao imovel: bom      pátio      piso: cerâmico      piso area intima: cerâmico      sacada      suíte master           infraestrutura do condomínio        gradeado                                 deseja ver ao vivo?              quando prefiro visitar este imóvel                    você pode escolher mais de um periodo e dia, conforme sua disponibilidade.           dia da semana      segunda        terça        quarta        quinta        sexta        sábado        domingo         período      manhã        tarde        noite            próximo           seus dados para contato                      como prefere ser contatado?      whatsapp        e-mail        ligação              voltar                            proposta          nome        telefone        email        mensagem          li e aceito a  política de privacidade                                       visualizações    35  pessoas viram esse anúncio.                                  >                         '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "fFQ6dDYbyPEC",
        "outputId": "d2a9f907-32e8-4213-9ace-528b8d4f362a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEMPO 60s\n",
            "(21109, 5) - URLS CONFIG: 21425\n",
            "Selecionou (0,)\n",
            "TEMPO DESDE O ÚLTIMO 266.3738708496094\n",
            "ADICIONOU 0\n",
            "TEMPO DESDE O ÚLTIMO 0.5146193504333496\n",
            "loading seconds urls_config\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-87017dff8004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m   \u001b[0;31m#salva em permanente os links que foram extraídos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m   \u001b[0mtimestamp_att_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckTimesIsOpenFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marquivo_urls_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#checa o tempo antes de abrir e salvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m   \u001b[0murls_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_urls_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0murl_new\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_urls_novos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-1593ef4ac522>\u001b[0m in \u001b[0;36mcheckTimesIsOpenFile\u001b[0;34m(arquivo_urls_config)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading seconds urls_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseconds_difference_is_change_file\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mtimestamp_att_df_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mtimestamp_att_df_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marquivo_urls_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cont = 1\n",
        "cont_imobs_extraction_now = 1\n",
        "\n",
        "while True:\n",
        "  print(\"TEMPO 60s\")\n",
        "  time.sleep(5)\n",
        "  #arquivo = diretorio/name_csv\n",
        "  #tempo2 = arquivo.stat().st_mtime\n",
        "  #if tempo2 > timestamp_att_df:\n",
        "  #  timestamp_att_df = tempo2\n",
        "  #else:\n",
        "  #  print(\"TEMPO NÃO VÁLIDO DE ATUAZLIAÇÃO DO DATAFRAME DE TODAS AS URLS\")\n",
        "  #  continue\n",
        "\n",
        "  df_urls = pd.read_csv(name_urls_extract_todos, sep='\\t')\n",
        "  if df_urls.shape[0] == 0:\n",
        "    time.sleep(30)\n",
        "    continue\n",
        "\n",
        "  df_urls = df_urls.loc[:, ~df_urls.columns.str.contains('^Unnamed')]\n",
        "\n",
        "  if cont_imobs_extraction_now != 1:\n",
        "    timestamp_att_end = checkTimesIsOpenFile(arquivo_urls_config)\n",
        "  \n",
        "  urls_config = pd.read_csv(name_urls_config, sep='\\t')\n",
        "  urls_config = urls_config.append({'URL':'aberto/grava', 'Situacao': 'aberto/grava', 'Parte':0}, ignore_index = True)\n",
        "\n",
        "  urls_config = urls_config.loc[:, ~urls_config.columns.str.contains('^Unnamed')]\n",
        "  print(df_urls.shape, \"- URLS CONFIG:\", urls_config.shape[0])\n",
        "\n",
        "  if urls_config[urls_config['Situacao'] == 'OK'].shape[0] == df_urls.shape[0]:break\n",
        "\n",
        "  if urls_config.shape[0] > 0:\n",
        "    list_urls_config = urls_config[(urls_config['Situacao'] == 'OK') | (urls_config['Situacao'] == 'Em extracao')]['URL'].to_list()\n",
        "    df_urls = df_urls.loc[[x not in list_urls_config for x in df_urls['URL'].tolist()]]\n",
        "    rows = np.random.choice(df_urls.index.values, 50 if df_urls.shape[0] >= 50 else df_urls.shape[0])\n",
        "    df_urls = df_urls.loc[rows]\n",
        "  else:\n",
        "    rows = np.random.choice(df_urls.index.values, 50 if df_urls.shape[0] >= 50 else df_urls.shape[0])\n",
        "    df_urls = df_urls.loc[rows]\n",
        "        \n",
        "  print(\"Selecionou\", df_urls['URL'].shape)\n",
        "  timestamp_att_end = checkTimesIsOpenFile(arquivo_urls_config)\n",
        "  contilk = 0\n",
        "  for ilk in range(df_urls.shape[0]):\n",
        "    #urls_config = urls_config.append({'URL':df_urls['URL'].iloc[ilk], 'Situacao': 'OK'}, ignore_index = True)\n",
        "    contilk +=1\n",
        "    urls_config = urls_config.append({'URL':df_urls['URL'].iloc[ilk], 'Situacao': 'Em extracao', 'Parte':0}, ignore_index = True)\n",
        "  urls_config.to_csv(name_urls_config, sep='\\t')\n",
        "  print(\"ADICIONOU\", contilk)\n",
        "\n",
        "  list_urls_novos = []\n",
        "  indice_df = -1\n",
        "  list_urls_already_in_df = df_regex_extract['URL'].to_list()\n",
        "  for imob, url, tipo, situacao in zip(df_urls['Imobiliária'], df_urls['URL'], df_urls['Tipo'], df_urls['Situacao']):\n",
        "    cont +=1\n",
        "    if url in list_urls_already_in_df:continue\n",
        "    print(cont_imobs_extraction_now, imob, url, tipo, situacao)\n",
        "    indice_df +=1\n",
        "    #if indice_df <= df_regex_extract_aux.shape[0]:continue\n",
        "    dominio_imob = df_urls['Dominio'].iloc[indice_df]\n",
        "\n",
        "    if dominio_imob == '' or dominio_imob == None or type(dominio_imob) == float:\n",
        "      header_immobile = {'User-Agent': 'Mozilla/5.0'}\n",
        "    else:\n",
        "      header_immobile = headerImmobile(dominio_imob_def = dominio_imob)\n",
        "    header_immobile = {'User-Agent': 'Mozilla/5.0'}\n",
        "    imob = imob.strip()  \n",
        "\n",
        "    try:\n",
        "      res = requests.get(url, headers = header_immobile) #headers={'User-Agent': 'Mozilla/5.0'})\n",
        "      res.encoding = 'UTF-8'\n",
        "    except:\n",
        "      print(\"ERRO URL\")\n",
        "      try:\n",
        "        df_urls['Situacao'].iloc[df_urls[df_urls['URL'] == url].index.to_list()] = 'ERROR URL'\n",
        "        print(\"deu certo ERROR URL\")\n",
        "      except:print(\"ERROR SITUACAO URL\")\n",
        "      continue\n",
        "    page_bruto = res.text\n",
        "\n",
        "    page = re.sub('\\s+', ' ', page_bruto)\n",
        "    try:\n",
        "      page = cleaner.clean_html(page)\n",
        "    except:\n",
        "      print(\"ERROR CLEANER\")\n",
        "      continue\n",
        "    soup = BeautifulSoup(res.text)\n",
        "\n",
        "    #if '404' in res.text or 'página não encontrada' in res.text.lower():\n",
        "      #print(\"ERROR 404\", url)\n",
        "      #continue\n",
        "    address_soup, sentence = 'NENHUM', 'NENHUM'\n",
        "    if imob == 'Bortolini':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['<div class=\"item\">'],  imob_excec='Bortolini', list_regex_excec=['<div class=\"imovel__detalhe__simulador\".*', '<section class=\"imovel__similar produto\".*'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout', 'uploads'])\n",
        "      try:\n",
        "        address_soup = soup.find(class_='imovel__detalhe__header').find(class_='local').text.strip()\n",
        "      except:pass\n",
        "    elif imob == 'Associadas' or imob == 'Ativa':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['</li>'], list_remove_tags = [({'Class':'', 'id':'sigaGaleria', 'TAG':''})], imob_excec='Associadas', list_regex_excec=['<p class=\"rodape.*', '<h3 class=\"similares no-border padding-bottom-15 padding-top-10\".*', '<div id=\"similares.*', '<div class=\"col-lg-4 col-md-5 stickyForm.*class=\"fimRolagemContato\">', '<div class=\"modal fade modalCookies.*','id=\"modal-cookies.*','<div class=\"avisoCookiesFixo.*'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "      try:\n",
        "        address_soup = soup.find(class_='sp-address-property').text.strip()\n",
        "      except:pass\n",
        "    elif imob == 'Benvegnú Imóveis' or imob == 'Benvegnu' or imob == 'Imóveis endres' or imob == 'Imobiliárias Coligadas' or imob == 'Bellotti' or imob == 'Ativa' or imob == 'Ceolin Imóveis' or imob == 'Bins Imobiliária' or imob == 'Medina' or imob == 'Bérgamo Imobiliária' or imob == 'Espendlor Imóveis' or imob == 'Casa Tua Transações Imobiliárias':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['</li>'], list_remove_tags = [({'Class':'master-slider', 'id':'', 'TAG':''})], imob_excec='Benvegnú', list_regex_excec=['<div class=\"master-slider ms-skin-light-3 ms-wk.*<div class=\"clearfix\">', '<div id=\"footer.*', '<div class=\"imovel__detalhe__simulador\".*', '<div id=\"similares.*', '<section class=\"imovel__similar produto\".*', '<div class=\"modal fade modalCookies.*','id=\"modal-cookies.*','<div class=\"avisoCookiesFixo.*'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "      try:\n",
        "        address_soup = soup.find(class_='sp-address-property').text.strip()\n",
        "      except:pass\n",
        "    elif imob == 'Finardi Carrão Imóveis' or imob == 'Finardi Carrão' or imob == 'Aluga Administradora de Bens LTDA' or imob == 'Fragomeni Imóveis' or imob == 'Gilberto Pedott Corretor de Imóveis' or imob == 'Cau Imóveis' or imob == 'Bertolini e Schneider Imóveis' or imob == 'Graffer Negócios Imobiliários' or imob == 'Área Nobre Imóveis':\n",
        "      #sentence = removeTags(page, list_tags_more_spaces=['</li>'], imob_excec='BenvegnuEndress', list_regex_excec=['<div id=\"similares\".*', '<div class=\"col-lg-4 col-md-5 stickyForm.*class=\"fimRolagemContato\">', '<div class=\"col-lg-4 col-md-5.*class=\"fimRolagemContato\">'])\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['</li>'], list_remove_tags = [({'Class':'', 'id':'sigaGaleria', 'TAG':''})], imob_excec='Aluga', list_regex_excec=['<div id=\"footer.*', '<div class=\"imovel__detalhe__simulador\".*', '<div id=\"similares.*', '<section class=\"imovel__similar produto\".*', '<div class=\"modal fade modalCookies.*','id=\"modal-cookies.*','<div class=\"avisoCookiesFixo.*'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "      try:\n",
        "        address_soup = soup.find(class_='sp-address-property').text.strip()\n",
        "      except:pass\n",
        "    elif imob == 'Casa Moove Imobiliária' or imob == 'Casa Moove' or imob == 'Ellen Negócios Imobiliários' or imob == 'Formighieri' or imob == 'Costa Lemes Imobiliária' or imob == 'Xarao':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['</li>'], list_remove_tags = [({'Class':'', 'id':'sigaGaleria', 'TAG':''})],  imob_excec='CasaMoove', list_regex_excec=['<div id=\"footer.*', '<div id=\"similares\".*', '<div class=\"col-lg-4 col-md-5 stickyForm.*class=\"fimRolagemContato\">', '<div class=\"col-lg-4 col-md-5.*class=\"fimRolagemContato\">', '<div class=\"modal fade modalCookies.*','id=\"modal-cookies.*','<div class=\"avisoCookiesFixo.*'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "      try:\n",
        "        address_soup = soup.find(class_='sp-address-property').text.strip()\n",
        "      except:pass\n",
        "    elif imob == 'Aporte Imóveis':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['</li>'], list_remove_tags = [({'Class':'', 'id':'sigaGaleria', 'TAG':''})], imob_excec='Aporte', list_regex_excec=['<div id=\"similares\".*', '<div class=\"col-lg-4 col-md-5 stickyForm.*class=\"fimRolagemContato\">', '<div class=\"col-lg-4 col-md-5.*class=\"fimRolagemContato\">', '<div class=\"modal fade modalCookies.*','id=\"modal-cookies.*','<div class=\"avisoCookiesFixo.*'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImagesAriotti(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "      try:\n",
        "        address_soup = soup.find(class_='sp-address-property').text.strip()\n",
        "      except:pass\n",
        "    elif imob == 'Imobiliária Ariotti':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['</li>'], list_remove_tags = [({'Class':'', 'id':'pg_galeria', 'TAG':''})], imob_excec='ariotti', list_regex_excec=['<div id=\"similares\".*', '<div class=\"col-lg-4 col-md-5 stickyForm.*</button>', '<div class=\"modal fade modalCookies.*','id=\"modal-cookies.*','<div class=\"avisoCookiesFixo.*'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImagesAriotti(soup, tag_img='div', attrs_check=['data-src', 'data-exthumbimage'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout', 'uploads', 'g1'])\n",
        "      try:\n",
        "        address_soup = soup.find(class_='sp-address-property').text.strip()\n",
        "      except:pass\n",
        "    elif imob == 'Arnel':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['</small>'], imob_excec='Arnel', list_regex_excec=['<h2>Semelhantes.*', '<section class=\"pgl-featured pgl-properties card-semelhantes.*\"', \n",
        "                                                                                                            '<div class=\"formEnvia.*', 'card-semelhantes.*' ])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "      try:\n",
        "        address_soup = soup.find('address').text\n",
        "      except:pass\n",
        "    elif imob == 'Verdi' or imob == 'Predial':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['</small>'], imob_excec='Verdi', list_regex_excec=['<section id=\"imovel-form-fixed.*', '<section id=\"section-indicados\".*', '<div class=\"col-lg-4 col-md-5 stickyForm.*class=\"fimRolagemContato\">', '<div class=\"col-lg-4 col-md-5.*class=\"fimRolagemContato\">'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "      try:\n",
        "        address_soup = soup.find(class_='endereco').text\n",
        "      except:pass\n",
        "    elif imob == 'Coratto':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['</tbody>'], imob_excec='Coratto', list_regex_excec=['<section id=\"imoveis-semelhantes\".*', '<p class=\"titulo\"></p>.*</aside>'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "\n",
        "    elif imob.strip() == 'Clube House Petrópolis':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['<div class=\"item-info\">'], imob_excec='Clube House', list_regex_excec=['<div class=\"listing-related text-xs-center\".*', '<div class=\"reference hidden-lg-up\">.*'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "      #VERIFICAR O ENDEREÇO DEPOIS, POIS ESTÁ JUNTO COM O TÍTULO E NÃO TEM UM PADRÃO PARA SEPARAR O ENDEREÇO DO TÍTULO\n",
        "      try:\n",
        "        address_soup = soup.find(class_='header-title hidden-lg-up').text\n",
        "      except:pass\n",
        "    #elif imob == 'Predial':\n",
        "    #  sentence = removeTags(page, list_tags_more_spaces=['</small>'], imob_excec='Predial', list_regex_excec=['<section id=\"imovel-form-fixed.*', '<ul class=\"list-unstyled main-menu.*</ul>', '<a class=\"btn-favorito\".*</a>',\n",
        "    #                                                               '<div class=\"navbar navbar-inverse.*<script>', #'<p class=\"iterative-notify-random-imovelpage.*</p>',\n",
        "    #                                                                '<footer.*','<h2 style=\"line-height: 45px;\".*', '<div id=\"disponivel-credpago.*\">'])\n",
        "    #  page = sentence.lower()\n",
        "    #  list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "    #  try:\n",
        "    #    address_soup = soup.find(class_='endereco').text\n",
        "    #  except:pass\n",
        "    elif imob == 'R. Classic':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['<div class=\"item-info\">'], imob_excec='R.Classic', list_regex_excec=['<div id=\"similares\".*', '<div class=\"class=\"card-broker border-color-contrast\".*</div></div>',\n",
        "                                                                          '<div class=\"cookies-component-lgpd.*</div>', '<footer.*'])\n",
        "      page = sentence.lower()\n",
        "      page = page.replace(' Veja mais  Veja mais                                   Qual o melhor dia e horário para você?                   Invalid date      Invalid date    Invalid date         O local será confirmado pelo corretor               Tenho interesse neste imóvel        Ao enviar concordo com os  termos de uso  e  política de privacidade , para contatar os próximos anunciantes e afirmo ter mais de 18 anos                   Tenho interesse neste imóvel        Ao enviar concordo com os  termos de uso  e  política de privacidade , para contatar os próximos anunciantes e afirmo ter mais de 18 anos','')\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "      try:\n",
        "        address_soup = soup.find(class_='header-title hidden-lg-up').find(class_='sub').text.strip()\n",
        "        if address_soup == '' or type(address_soup) == float:\n",
        "          float(gerar_error_next_except)\n",
        "      except:\n",
        "        try:\n",
        "          address_soup = soup.find(class_='header-title hidden-lg-up').text.strip()\n",
        "        except:pass\n",
        "    elif imob == 'Fabiane Imóveis':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['</tr>'], imob_excec='Fabianeimoveis', list_regex_excec=['<h1 class=\"header-text__title\">Imóveis semelhantes.*', '<form.*</form>', '<footer.*','<ul class=\"l-nav__container js-nav js-menu--close.*</ul>','<div class=\"header-img-box js-mainBanner.*</div>'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "      try:\n",
        "        table = soup.find_all('table', class_='table-detail')\n",
        "        for tr in table:\n",
        "          trs = tr.find_all('tr')\n",
        "          for tr in trs:\n",
        "            if tr.find('td', class_='color-text').text.strip().lower() == 'bairro':\n",
        "              address_soup = tr.find('td', class_='color-title').text.strip()\n",
        "      except:pass\n",
        "    elif imob == 'Grupo Imob':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['<div class=\"item-info\">'], imob_excec='GrupoImob', list_regex_excec=['<div class=\"listing-related\".*', '<header.*</header>', '<div class=\"class=\"card-broker border-color-contrast\".*</div></div>',\n",
        "                                                                          '<div class=\"cookies-component-lgpd.*</div>', '<div class=\"col-md-7 hidden-sm-down box-phones-menu.*</div>','<footer.*'])\n",
        "      page = sentence.lower()\n",
        "      page = page.replace(' Veja mais  Veja mais                                   Qual o melhor dia e horário para você?                   Invalid date      Invalid date    Invalid date         O local será confirmado pelo corretor               Tenho interesse neste imóvel        Ao enviar concordo com os  termos de uso  e  política de privacidade , para contatar os próximos anunciantes e afirmo ter mais de 18 anos                   Tenho interesse neste imóvel        Ao enviar concordo com os  termos de uso  e  política de privacidade , para contatar os próximos anunciantes e afirmo ter mais de 18 anos','')\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "      try:\n",
        "        address_soup = soup.find(class_='header-title hidden-lg-up').find(class_='sub').text.strip()\n",
        "        if address_soup == '' or type(address_soup) == float:\n",
        "          float(gerar_error_next_except)\n",
        "      except:\n",
        "        try:\n",
        "          address_soup = soup.find(class_='header-title hidden-lg-up').text.strip()\n",
        "        except:pass\n",
        "    elif imob == 'Casa Nostra':\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['<span class=\"feature\">'], imob_excec='CasaNostra', list_regex_excec=['<footer.*', '<aside id=\"sidebar.*</aside>', '<form.*</form>'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImagesAriotti(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "      try:\n",
        "        address_soup = soup.find('p', class_='address').text.strip()\n",
        "      except:pass\n",
        "    elif imob == 'Castilhos Nazari' or imob == 'Prima Casa': ###########VERIFICAR PRIMA CASA\n",
        "      #page = removeTags(page, imob_excec='BenvegnuEndress', list_regex_excec=['<form.*</form>','<ul class=\"impostos.*','<footer.*', '<ul class=\"menu-main-menu-1.*</ul>','<section class=\"rh_property__similar_properties.*</section>','<section class=\"rh_property__comments.*</section>', '<aside class=\"rh_sidebar.*</aside>'])\n",
        "      #page = removeTags(page, list_tags_more_spaces=['<span class=\"rh_meta_titles\">'],  imob_excec='PrimaCasa', list_regex_excec=['<div class=\"imovel__detalhe__simulador\".*', '<section class=\"imovel__similar produto\".*'])\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['<span class=\"rh_meta_titles\">'],  imob_excec='PrimaCasa', list_regex_excec=['<div class=\"imovel__detalhe__simulador\".*', '<section class=\"rh_property__similar_properties.*', '<section class=\"imovel__similar produto\".*'])\n",
        "      sentence = sentence.replace('Comprar     Alugar     Quem Somos      Nossa história     Serviços         Fale Conosco                (54) 99936-4363             atendimento@castilhosnazari.com.br','')\n",
        "      sentence = sentence.replace('Tipo de Imóvel              Localização               Preço Máximo              Qualquer              Preço Máximo              Código do Imóvel          Quartos              Banheiros              Vagas de Garagem','')\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "\n",
        "    elif imob == 'Remax':\n",
        "      #page = removeTags(page, imob_excec='Remax', list_regex_excec=['<footer.*', '<form.*</form>','<div class=\"single-contato sidebar.*</div>', '<div class=\"agente.*</small>'])\n",
        "      precos = re.findall(\"\\d+, 0, ',', '\\.', 'R\\$'\", page_bruto)\n",
        "      sentence = removeTags(page, list_tags_more_spaces=['<div class=\"box-inf-det\">', '</li>'], imob_excec='Remax', list_regex_excec=['<footer.*', '<form.*</form>','<div class=\"single-contato sidebar.*</div>', '<div class=\"agente.*</small>'])\n",
        "      sentence += '    '\n",
        "      for preco in precos:\n",
        "        sentence += 'R$ '+preco\n",
        "        sentence += '    '\n",
        "      sentence = sentence.replace(' Menu                                   Buscar imóveis     Venda seu imóvel      Agências     Seja corretor RE/MAX     Seja Parceiro     Seja um franqueado     Notícias                            Fechar        Buscar imóveis        Venda seu imóvel        Agências        Seja corretor RE/MAX        Seja Parceiro        Seja um franqueado        Notícias','')\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImagesAriotti(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout'])\n",
        "      try:\n",
        "        address_soup = soup.find(class_='container-ficha').find(class_='box-inf-det').text.strip()\n",
        "      except:pass\n",
        "    elif imob == 'BP Negócios imobiliários':\n",
        "      try:\n",
        "        page = str(soup.find(id='imovel-exibir').find_all(class_='container', recursive=False)[1]).replace('\\n',' ').replace('\\t',' ')\n",
        "        page = cleaner.clean_html(page)\n",
        "      except:\n",
        "        print(\"ERROR PAGE\", url)\n",
        "        continue\n",
        "      sentence = removeTags(page, imob_excec='BP', list_regex_excec=['<footer.*', '<form.*</form>', '<div id=\"imovel_contato.*</form>'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImagesAriotti(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout', 'no-image'])\n",
        "      ps = soup.find(class_='imovel_endereco').find_all('p')\n",
        "      for p in ps:\n",
        "        if address_soup == 'NENHUM':address_soup = p.text.strip()\n",
        "        else:address_soup +='&-&'+ p.text.strip()\n",
        "    elif imob == 'Martins Imóveis':\n",
        "      try:\n",
        "        page = str(soup.find(id='imovel-exibir').find_all(class_='container', recursive=False)[0]).replace('\\n',' ').replace('\\t',' ')\n",
        "        page = cleaner.clean_html(page)\n",
        "      except:\n",
        "        print(\"ERROR PAGE\", url)\n",
        "        continue\n",
        "      sentence = removeTags(page, imob_excec='Martins', list_regex_excec=['<footer.*', '<form.*</form>', '<div id=\"imovel_contato.*</form>'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImagesAriotti(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout', 'no-image'])\n",
        "      ps = soup.find(class_='imovel_endereco').find_all('p')\n",
        "      for p in ps:\n",
        "        if address_soup == 'NENHUM':address_soup = p.text.strip()\n",
        "        else:address_soup +='&-&'+ p.text.strip()\n",
        "    elif imob == 'Rosana Lorenzoni Corretora de imóveis':\n",
        "      try:\n",
        "        page = str(soup.find(id='imovel-exibir').find_all(class_='container', recursive=False)[0]).replace('\\n',' ').replace('\\t',' ')\n",
        "        page = cleaner.clean_html(page)\n",
        "      except:\n",
        "        print(\"ERROR PAGE\", url)\n",
        "        continue\n",
        "      sentence = removeTags(page, imob_excec='Rosana', list_regex_excec=['<footer.*', '<form.*</form>', '<div id=\"imovel_contato.*</div>', '<div id=\"social-media.*</h4>'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImagesAriotti(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout', 'no-image'])\n",
        "      ps = []\n",
        "      try:\n",
        "        ps = soup.find(class_='imovel_endereco').find_all('p')\n",
        "      except:\n",
        "        for script in range(len(soup.find_all('script'))):\n",
        "          try:\n",
        "            ps = BeautifulSoup(soup.find_all('script')[script].text).find(class_='imovel_endereco').find_all('p')\n",
        "            break\n",
        "          except:continue\n",
        "      for p in ps:\n",
        "        if address_soup == 'NENHUM':address_soup = p.text.strip()\n",
        "        else:address_soup +='&-&'+ p.text.strip()\n",
        "\n",
        "    elif imob == 'Tedesco Imóveis':\n",
        "      try:\n",
        "        page = str(soup.find(id='building-display')).replace('\\n',' ').replace('\\t',' ')\n",
        "        page = cleaner.clean_html(page)\n",
        "      except:\n",
        "        print(\"ERROR PAGE\", url)\n",
        "        continue\n",
        "      sentence = removeTags(page, imob_excec='Tedesco', list_regex_excec=['<footer.*','<div id=\"f-aux.*', '<div class=\"clearfix\" id=\"building-contact\".*</div>', '<form.*</form>', '<div id=\"imovel_contato.*</div>', '<div id=\"social-media.*</h4>'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','corretor', 'logo', 'layout','no-image'])\n",
        "      try:\n",
        "        address_soup = soup.find('div', class_='layout-row layout-align-center-center').find('h1').find_next_sibling('div').text.replace('\\n',' ').replace('\\t','').strip()\n",
        "      except:pass\n",
        "    elif imob == 'Corretora de Imóveis Camargo':\n",
        "      try:\n",
        "        page = str(soup.find(id='building-display')).replace('\\n',' ').replace('\\t',' ')\n",
        "        page = cleaner.clean_html(page)\n",
        "      except:#coratto\n",
        "        print(\"ERROR PAGE\", url)\n",
        "        continue\n",
        "      sentence = removeTags(page, imob_excec='Camargo', list_regex_excec=['<footer.*', '<div class=\"col-xs-12\" id=\"building-contact\">.*', '<form.*</form>', '<div id=\"imovel_contato.*</div>'])\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','logo', 'layout','no-image','logo_rodape'])\n",
        "      try:\n",
        "        ps = soup.find(class_='col-xs-6 col-md-8').find_all('p')\n",
        "        address_soup = ps[0].text.replace('\\t','').strip().replace('\\n','')\n",
        "      except:pass\n",
        "    elif imob == 'Carlotto Imóveis':\n",
        "      try:\n",
        "        page = str(soup.find(id='building-display')).replace('\\n',' ').replace('\\t',' ')\n",
        "        page = cleaner.clean_html(page)\n",
        "      except:\n",
        "        print(\"ERROR PAGE\", url)\n",
        "        continue\n",
        "      sentence = removeTags(page, imob_excec='IMOB', list_regex_excec=['<footer.*', '<div class=\"col-xs-12\" id=\"building-contact\">.*', '<form.*</form>', '<div id=\"imovel_contato.*</div>'])\n",
        "      sentence = sentence.replace('Contato   Entre em contato conosco, preenchendo o formulário abaixo.          Mensagem enviada com sucesso. Agradecemos o Contato.       Ocorreu um erro ao enviar a sua mensagem. Por favor tente novamente.       Nome        Telefone        E-mail        Mensagem        Quero ser contatado por       E-mail      Telefone      Whatsapp       Qualquer uma das opções           Ao clicar em enviar, eu concordo em receber comunicações da Imobiliária, utilizando os dados que forneci neste formulário','')\n",
        "      page = sentence.lower()\n",
        "      list_imgs = getImages(soup, tag_img='img', attrs_check=['data-src', 'src'], urls_checks=['.jpg', '.jpeg'], check_excessao=['usua','logo', 'layout','no-image','logo_rodape'])\n",
        "      ps = soup.find(class_='grid-body').find_all('p')\n",
        "      for p in ps:\n",
        "        try:\n",
        "          if 'endereco' in unidecode(p.find('strong').text.strip().lower()):\n",
        "            if address_soup == 'NENHUM':address_soup = p.text.replace(p.find('strong').text, '').replace('()','').strip().replace('\\n',' ')\n",
        "            else:address_soup +='&-&'+p.text.replace(p.find('strong').text, '').replace('()','').strip().replace('\\n',' ')\n",
        "        except:pass\n",
        "        try:\n",
        "          if 'bairro' in unidecode(p.find('strong').text.strip().lower()):\n",
        "            if address_soup == 'NENHUM':address_soup = p.text.strip()\n",
        "            else:address_soup +='&-&'+p.text.strip()\n",
        "            break\n",
        "        except:pass\n",
        "    else:\n",
        "      print(\"IMOB NÂO TEM\")\n",
        "      continue\n",
        "  \n",
        "    \"\"\"\n",
        "      FUNÇÕES DO NER - PRIMEIRAS TENTATIVAS COM REGEX E O NER 4...7\n",
        "    \"\"\"\n",
        "    if sentence == 'NENHUM':\n",
        "      print(\"OPAAAA SENTENCE\")\n",
        "      break\n",
        "    \n",
        "    suites_regex, dorms_regex, vagas_regex, banheiros_regex, dorms_spacy_rg,suites_spacy_rg,banheiro_spacy_rg,vagas_spacy_rg = regexSpacy1(page)\n",
        "    list_coordenadas_regex, address_regex_iframe_return = coordenadasRegex(soup_regex = soup, imob = imob)\n",
        "    suites1, suites2, dorms, vagas, quartos, banheiros_r = findWordPerIndice(page)\n",
        "\n",
        "    desc_ner = sentence.lower()\n",
        "    desc_ner45 = sentence\n",
        "\n",
        "    area_m = re.findall('\\d+[,.]?\\d+\\s?m²', desc_ner)\n",
        "    area_ha = re.findall('\\d+[,.]?\\d+\\s?ha', desc_ner)\n",
        "    if area_ha == []:area_ha = re.findall('\\d+[,.]?\\d+\\s?hect', desc_ner)\n",
        "\n",
        "    desc_ner = desc_ner.replace('_', ' ')\n",
        "    if findWordsAndIndices(desc_ner, 'sui', 'SUITES', sub_word_padrao='suítes', option_ner = 2, sentence_ner=desc_ner) != (0, 0): desc_ner = findWordsAndIndices(desc_ner, 'sui', 'SUITES', sub_word_padrao='suítes', option_ner = 2, sentence_ner=desc_ner)\n",
        "    if findWordsAndIndices(desc_ner, 'suí', 'SUITES', sub_word_padrao='suítes', option_ner = 2, sentence_ner=desc_ner) != (0, 0): desc_ner = findWordsAndIndices(desc_ner, 'sui', 'SUITES', sub_word_padrao='suítes', option_ner = 2, sentence_ner=desc_ner)\n",
        "    if findWordsAndIndices(desc_ner, 'vaga', 'VAGAS', sub_word_padrao='vagas', option_ner = 2, sentence_ner=desc_ner) != (0, 0): desc_ner = findWordsAndIndices(desc_ner, 'vaga', 'VAGAS', sub_word_padrao='vagas', option_ner = 2, sentence_ner=desc_ner)\n",
        "    if findWordsAndIndices(desc_ner, 'dorm', 'DORMS', sub_word_padrao='dormitorios', option_ner = 2, sentence_ner=desc_ner) != (0, 0): desc_ner = findWordsAndIndices(desc_ner, 'dorm', 'DORMS', sub_word_padrao='dormitorios', option_ner = 2, sentence_ner=desc_ner)\n",
        "    if findWordsAndIndices(desc_ner, 'banheiro', 'BANHEIROS', sub_word_padrao='banheiros', option_ner = 2, sentence_ner=desc_ner) != (0,0): desc_ner = findWordsAndIndices(desc_ner, 'banheiro', 'BANHEIROS', sub_word_padrao='banheiros', option_ner = 2, sentence_ner=desc_ner)\n",
        "\n",
        "    #dados estão apenas proporcionais ------------------------------------------------------------------------\n",
        "    #print(\"\\n-------SPACY NER 4-------\")\n",
        "    cod4, tipo4, price4, dorms4, suites4, area4, vagas4, banheiros4, address4, sentences = [],[],[],[],[],[],[],[],[],[]\n",
        "    for model_type in [desc_ner45, desc_ner45.lower(), re.sub('\\s+',' ',desc_ner45), re.sub('\\s+',' ',desc_ner), re.sub('\\s+',' ',desc_ner.lower()), desc_ner.lower(), tokenize_sentence(page.lower()), tokenize_sentence(desc_ner.lower())]:\n",
        "        doc4 = nlp4(model_type)\n",
        "        sentences.append(doc4)\n",
        "        for ent in doc4.ents:\n",
        "          if ent.label_ == 'TIPO':tipo4.append(ent.text)\n",
        "          if ent.label_ == 'COD':cod4.append(ent.text)\n",
        "          if ent.label_ == 'PRICE':price4.append(ent.text)\n",
        "          if ent.label_ == 'DORMS':dorms4.append(ent.text)\n",
        "          if ent.label_ == 'SUITES':suites4.append(ent.text)\n",
        "          if ent.label_ == 'AREA':area4.append(ent.text)\n",
        "          if ent.label_ == 'VAGAS':vagas4.append(ent.text)\n",
        "          if ent.label_ == 'BANHEIROS':vagas4.append(ent.text)\n",
        "          if ent.label_ == 'ADDRESS':address4.append(ent.text)\n",
        "          #print(ent.text, ent.label_)\n",
        "\n",
        "    #dados proporcionais e ajustados (dorm = dormitorios) --------------------------------------------------------------------------\n",
        "      #print(\"\\n-------SPACY NER 5-------\")\n",
        "    cod5, tipo5, price5, dorms5, suites5, area5, vagas5, banheiros5, address5 = [],[],[],[],[],[],[],[],[]\n",
        "    for model_type in [desc_ner45, desc_ner45.lower(), re.sub('\\s+',' ',desc_ner45), re.sub('\\s+',' ',desc_ner), re.sub('\\s+',' ',desc_ner.lower()), desc_ner.lower(), tokenize_sentence(page.lower()), tokenize_sentence(desc_ner.lower())]:\n",
        "        doc5 = nlp5(model_type)\n",
        "        #doc5 = nlp5(re.sub('\\s+',' ',desc_ner45)) #padrão testado antes\n",
        "        for ent in doc5.ents:\n",
        "          if ent.label_ == 'TIPO':tipo5.append(ent.text)\n",
        "          if ent.label_ == 'COD':cod5.append(ent.text)\n",
        "          if ent.label_ == 'PRICE':price5.append(ent.text)\n",
        "          if ent.label_ == 'DORMS':dorms5.append(ent.text)\n",
        "          if ent.label_ == 'SUITES':suites5.append(ent.text)\n",
        "          if ent.label_ == 'AREA':area5.append(ent.text)\n",
        "          if ent.label_ == 'VAGAS':vagas5.append(ent.text)\n",
        "          if ent.label_ == 'BANHEIROS':vagas5.append(ent.text)\n",
        "          if ent.label_ == 'ADDRESS':address5.append(ent.text)\n",
        "\n",
        "\n",
        "    #remove stop words e padroniza os dados --------------------------------------------------------------------------------------\n",
        "    desc_ner = page.replace('_', ' ')\n",
        "    if findWordsAndIndices(desc_ner, 'sui', 'SUITES', sub_word_padrao='suítes', option_ner = 2, sentence_ner=desc_ner) != (0,0): desc_ner = findWordsAndIndices(desc_ner, 'sui', 'SUITES', sub_word_padrao='suítes', option_ner = 2, sentence_ner=desc_ner)\n",
        "    if findWordsAndIndices(desc_ner, 'suí', 'SUITES', sub_word_padrao='suítes', option_ner = 2, sentence_ner=desc_ner) != (0,0): desc_ner = findWordsAndIndices(desc_ner, 'sui', 'SUITES', sub_word_padrao='suítes', option_ner = 2, sentence_ner=desc_ner)\n",
        "    if findWordsAndIndices(desc_ner, 'vaga', 'VAGAS', sub_word_padrao='vagas', option_ner = 2, sentence_ner=desc_ner) != (0,0): desc_ner = findWordsAndIndices(desc_ner, 'vaga', 'VAGAS', sub_word_padrao='vagas', option_ner = 2, sentence_ner=desc_ner)\n",
        "    if findWordsAndIndices(desc_ner, 'dorm', 'DORMS', sub_word_padrao='dormitorios', option_ner = 2, sentence_ner=desc_ner) != (0,0): desc_ner = findWordsAndIndices(desc_ner, 'dorm', 'DORMS', sub_word_padrao='dormitorios', option_ner = 2, sentence_ner=desc_ner)\n",
        "    if findWordsAndIndices(desc_ner, 'banheiro', 'BANHEIRO', sub_word_padrao='banheiro', option_ner = 2, sentence_ner=desc_ner) != (0,0): desc_ner = findWordsAndIndices(desc_ner, 'banheiro', 'BANHEIRO', sub_word_padrao='banheiro', option_ner = 2, sentence_ner=desc_ner)\n",
        "\n",
        "    cod6, tipo6, price6, dorms6, suites6, area6, vagas6, banheiros6, address6, =  [],[],[],[],[],[],[],[],[]\n",
        "    for model_type in [desc_ner45, desc_ner45.lower(), re.sub('\\s+',' ',desc_ner45), re.sub('\\s+',' ',desc_ner), re.sub('\\s+',' ',desc_ner.lower()), desc_ner.lower(), tokenize_sentence(page.lower()), tokenize_sentence(desc_ner.lower())]:\n",
        "        doc6 = nlp6(model_type)\n",
        "        for ent in doc6.ents:\n",
        "          if ent.label_ == 'TIPO':tipo6.append(ent.text)\n",
        "          if ent.label_ == 'COD':cod6.append(ent.text)\n",
        "          if ent.label_ == 'PRICE':price6.append(ent.text)\n",
        "          if ent.label_ == 'DORMS':dorms6.append(ent.text)\n",
        "          if ent.label_ == 'SUITES':suites6.append(ent.text)\n",
        "          if ent.label_ == 'AREA':area6.append(ent.text)\n",
        "          if ent.label_ == 'VAGAS':vagas6.append(ent.text)\n",
        "          if ent.label_ == 'BANHEIROS':vagas6.append(ent.text)\n",
        "          if ent.label_ == 'ADDRESS':address6.append(ent.text)\n",
        "\n",
        "    cod7, tipo7, price7, dorms7, suites7, area7, vagas7, banheiros7, address7 = [],[],[],[],[],[],[],[],[]\n",
        "    for model_type in [page, desc_ner45, desc_ner45.lower(), re.sub('\\s+',' ',desc_ner45), re.sub('\\s+',' ',desc_ner), re.sub('\\s+',' ',desc_ner.lower()), desc_ner.lower(), tokenize_sentence(page.lower()), tokenize_sentence(desc_ner.lower())]:\n",
        "      doc7 = nlp7(model_type) \n",
        "      for ent in doc7.ents:\n",
        "        if ent.label_ == 'TIPO':tipo7.append(ent.text)\n",
        "        if ent.label_ == 'COD':cod7.append(ent.text)\n",
        "        if ent.label_ == 'PRICE':price7.append(ent.text)\n",
        "        if ent.label_ == 'DORMS':dorms7.append(ent.text)\n",
        "        if ent.label_ == 'SUITES':suites7.append(ent.text)\n",
        "        if ent.label_ == 'AREA':area7.append(ent.text)\n",
        "        if ent.label_ == 'VAGAS':vagas7.append(ent.text)\n",
        "        if ent.label_ == 'BANHEIROS':vagas7.append(ent.text)\n",
        "        if ent.label_ == 'ADDRESS':address7.append(ent.text)\n",
        "\n",
        "    #print('\\n NER 7.1----------------')\n",
        "    cod71, tipo71, price71, dorms71, suites71, area71, vagas71, banheiros71, address71 = [],[],[],[],[],[],[],[],[]\n",
        "    #for ent in doc.ents:\n",
        "    for model_type in [page, desc_ner45, desc_ner45.lower(), re.sub('\\s+',' ',desc_ner45), re.sub('\\s+',' ',desc_ner), re.sub('\\s+',' ',desc_ner.lower()), desc_ner.lower(), tokenize_sentence(page.lower()), tokenize_sentence(desc_ner.lower())]:\n",
        "      doc71 = nlp71(model_type)\n",
        "      for ent in doc71.ents:\n",
        "        #print(ent.text, ent.label_)\n",
        "        if ent.label_ == 'TIPO':tipo71.append(ent.text)\n",
        "        if ent.label_ == 'COD':cod71.append(ent.text)\n",
        "        if ent.label_ == 'PRICE':price71.append(ent.text)\n",
        "        if ent.label_ == 'DORMS':dorms71.append(ent.text)\n",
        "        if ent.label_ == 'SUITES':suites71.append(ent.text)\n",
        "        if ent.label_ == 'AREA':area71.append(ent.text)\n",
        "        if ent.label_ == 'VAGAS':vagas71.append(ent.text)\n",
        "        if ent.label_ == 'BANHEIROS':vagas71.append(ent.text)\n",
        "        if ent.label_ == 'ADDRESS':address71.append(ent.text)\n",
        "\n",
        "    list_bairros_regex = getBairros(page)\n",
        "\n",
        "    list_ner.append([{'URL':url, 'Imobiliária': imob, 'Sentence': sentence, 'Imagens': list_imgs}, {'Regex': {'Suites':suites2, 'Banheiros': banheiros_r, 'COORDS': list_coordenadas_regex, 'ADDRESS': address_regex_iframe_return,\n",
        "                                                'Dorms':dorms, 'Vagas':vagas, 'Quartos':quartos, 'AREA m2':area_m, 'AREA ha': area_ha}},\n",
        "                                      {'REGEX spacy':{'Suítes':suites_regex,  'Bairro':list_bairros_regex,\n",
        "                                          'Dorms':dorms_regex, 'Vagas':vagas_regex, 'Banheiros':banheiros_regex}},\n",
        "                                      {'REGEX spacy 2':{'Suítes':suites_spacy_rg,\n",
        "                                                  'Dorms':dorms_spacy_rg, 'Vagas':vagas_spacy_rg, 'Banheiros':banheiro_spacy_rg}},\n",
        "                                      {'NER 4':\n",
        "                                            {'Suites':suites4, 'COD':cod4,\n",
        "                                            'Tipo':tipo4,'Preco':price4, 'Dorms':dorms4, 'Vagas':vagas4, 'AREA':area4, 'BANHEIRO':banheiros4, 'ADDRESS':address4}},\n",
        "                                      {'NER 5':\n",
        "                                            {'Suites':suites5, 'COD':cod5,\n",
        "                                            'Tipo':tipo5,'Preco':price5, 'Dorms':dorms5, 'Vagas':vagas5, 'AREA':area5, 'BANHEIRO':banheiros5, 'ADDRESS':address5}},\n",
        "                                      {'NER 6':\n",
        "                                            {'Suites':suites6, 'COD':cod6,\n",
        "                                            'Tipo':tipo6,'Preco':price6, 'Dorms':dorms6, 'Vagas':vagas6, 'AREA':area6, 'BANHEIRO':banheiros6, 'ADDRESS':address6}},\n",
        "                                      {'NER 7':\n",
        "                                            {'Suites':suites7, 'COD':cod7,\n",
        "                                            'Tipo':tipo7,'Preco':price7, 'Dorms':dorms7, 'Vagas':vagas7, 'AREA':area7, 'BANHEIRO':banheiros7, 'ADDRESS':address7}},\n",
        "                                      {'NER 7.1':\n",
        "                                            {'Suites':suites71, 'COD':cod71,\n",
        "                                            'Tipo':tipo71,'Preco':price71, 'Dorms':dorms71, 'Vagas':vagas71, 'AREA':area71, 'BANHEIRO':banheiros71, 'ADDRESS':address71}},\n",
        "                                      #{'Humano Verificar':human_verify}\n",
        "                    ])\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "      FUNÇÕES NOVAS DO REGEX E CARACTER POR CARACTER\n",
        "    \"\"\"\n",
        "    #print(indice_df, sentence)\n",
        "    sentence_not_space =  unidecode(sentence.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0',''))\n",
        "    sentence_mod = re.sub(',', ' , ', re.sub('\\s+', ' ', unidecode(sentence.lower().replace('(','').replace(')','').replace(':','').replace('\\xa0',''))))\n",
        "\n",
        "\n",
        "    dorms, quartos, suites, vagas, banheiros, garagem, carros = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM',\n",
        "    area, area_priv, area_terreno, area_contruida, area_total, area_global, area_util = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM',\n",
        "\n",
        "    dorms, quartos, suites, vagas, banheiros, garagem, carros = tagsShow(sentence, tipo = 'comodos')\n",
        "    area, area_priv, area_terreno, area_contruida, area_total, area_global, area_util = tagsShow(sentence, tipo = 'areas')\n",
        "    \n",
        "\n",
        "    dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quarto_rgx_only, garagem_rgx_only = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM'\n",
        "    dorms_rgx_only2, suites_rgx_only2, vagas_rgx_only2, banheiro_rgx_only2, quartos_rgx_only2, garagem_rgx_only2, carros_rgx_only2 = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM'\n",
        "\n",
        "    \"\"\"alternativa regex que pega qualquer expressão que tenha número e palavras juntos: 2 dorms 3 suítes 4 banheiros\"\"\"\n",
        "    _, _, dorms_rgx_only, suites_rgx_only, vagas_rgx_only, banheiro_rgx_only, quarto_rgx_only, garagem_rgx_only = returnComodosJuntos(sentence)\n",
        "\n",
        "    \"\"\"quando há vários cômodos juntos, que é especificado (precisa saber qual cômodo quer, como dorms, suítes, banheiros ...), e não se sabe qual número é de qual cômodo. \n",
        "      comodosChavesJuntos tenta solucionar isso. Em muitos casos é efetivo. Sua melhoria é o comodosJuntosNEW\"\"\"\n",
        "    dorms_rgx_only2, suites_rgx_only2, vagas_rgx_only2, banheiro_rgx_only2, quartos_rgx_only2, garagem_rgx_only2, carros_rgx_only2 = comodosChavesJuntos(sentence)\n",
        "\n",
        "    \"\"\"quando há na sentença o cômodo que procurado e ele está sozinho com o número, seja do lado esquerdo, seja do lado direito. Por exemplo, 2 dorms ou dorms 2, não havendo outro número\n",
        "      que se possa juntar-se ao cômodo pesquisado\"\"\"\n",
        "    dorms_unique, quartos_unique, suites_unique, vagas_unique, banheiros_unique, garagem_unique, carros_unique, box_unique = comodosUnique(sentence)\n",
        "\n",
        "    \"\"\" a partir da extração, é colocado um espaço em cada tag que tiver. Logo, em muitas vezes, nessas extrações, quanto maior o número de espaços, quer dizer que teve mais \n",
        "      tags, logo, havia uma separação entre dorms e suítes, por exemplo. Isso significa que quanto menor o número de espaços que tinha entre um cômodo específico e um número, \n",
        "      em muitos casos, seria o valor correto para aquele cômodo. Entretanto, houve alguns casos em que isso não ocorre, como na imobiliária coratto. Dessa forma, é preciso na extração\n",
        "      melhorar essa parte, colocar mais espaços nas tags específicas que separam um cômodo do outro\n",
        "    \"\"\"\n",
        "    dorm_rgx_tag_space, suite_rgx_tag_space, vaga_rgx_tag_space, banheiro_rgx_tag_space, quarto_rgx_tag_space, garagem_rgx_tag_space, carro_rgx_tag_space = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM'   \n",
        "    dorm_rgx_tag_space, suite_rgx_tag_space, vaga_rgx_tag_space, banheiro_rgx_tag_space, quarto_rgx_tag_space, garagem_rgx_tag_space, carro_rgx_tag_space = returnTagsContSpace(sentence_not_space)\n",
        "\n",
        "    \"\"\" quando tem cômodos juntos e não sabe qual número é de qual cômodo. Dessa forma, o comodosJuntosNEW tenta solucionar esse problema - mais novo\"\"\"\n",
        "    dorms_rgx_only3, suites_rgx_only3, vagas_rgx_only3, banheiro_rgx_only3, quartos_rgx_only3, garagem_rgx_only3, carros_rgx_only3 = comodosJuntosNEW(sentence)\n",
        "\n",
        "    #'only carros REGEX':carros_rgx_only,\n",
        "\n",
        "    dorms_or, quartos_or, suites_or, banheiros_or, vagas_or, garagem_or, carros_or, box_or = regexOU(sentence)\n",
        "\n",
        "\n",
        "    return_areas = returnArea(url, sentence)\n",
        "    str_area = 'NENHUM'\n",
        "    for ar in return_areas:\n",
        "      if str_area =='NENHUM':str_area = ar  \n",
        "      else: str_area +='&-&'+ar\n",
        "    #df_sentences['only area REGEX'].iloc[indice_df] = str_area\n",
        "\n",
        "    return_areas_priv = returnAreaPrivativa(url, sentence)\n",
        "    str_area_priv = 'NENHUM'\n",
        "    for ar in return_areas_priv:\n",
        "      if str_area_priv =='NENHUM':str_area_priv = ar  \n",
        "      else: str_area_priv +='&-&'+ar\n",
        "    #df_sentences['only area priv REGEX'].iloc[indice_df] = str_area_priv\n",
        "\n",
        "    return_areas_terreno = returnAreaTerreno(url, sentence)\n",
        "    str_area_priv = 'NENHUM'\n",
        "    for ar in return_areas_terreno:\n",
        "      if str_area_priv =='NENHUM':str_area_priv = ar  \n",
        "      else: str_area_priv +='&-&'+ar\n",
        "    #df_sentences['only area terreno REGEX'].iloc[indice_df] = str_area_priv\n",
        "\n",
        "    #NLP8 - PRECO\n",
        "    str_preco_predict_ner = 'NENHUM'\n",
        "    doc = nlp8(sentence)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == 'PRECO':\n",
        "        #print(ent.text)\n",
        "        if str_preco_predict_ner == 'NENHUM':str_preco_predict_ner = ent.text\n",
        "        else:str_preco_predict_ner +='&-&'+ent.text\n",
        "    #df_sentences['preço NER'].iloc[indice_df] = str_preco_predict_ner\n",
        "\n",
        "    #NLP9 - DORMS\n",
        "    str_dorms_predict_ner = 'NENHUM'\n",
        "    doc = nlp9(sentence_mod)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == 'DORMS':\n",
        "        if str_dorms_predict_ner == 'NENHUM':str_dorms_predict_ner = ent.text\n",
        "        else:str_dorms_predict_ner +='&-&'+ent.text\n",
        "    #df_sentences['dorms NER'].iloc[indice_df] = str_dorms_predict_ner\n",
        "\n",
        "    #NLP9 - QUARTOS\n",
        "    str_quartos_predict_ner = 'NENHUM'\n",
        "    doc = nlp9(sentence_mod)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == 'QUARTOS':\n",
        "        if str_quartos_predict_ner == 'NENHUM':str_quartos_predict_ner = ent.text\n",
        "        else:str_quartos_predict_ner +='&-&'+ent.text\n",
        "    #df_sentences['quartos NER'].iloc[indice_df] = str_quartos_predict_ner\n",
        "      \n",
        "    #NLP10 - VAGAS\n",
        "    str_vagas_predict_ner = 'NENHUM'\n",
        "    doc = nlp10(sentence_mod)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == 'VAGAS':\n",
        "        if str_vagas_predict_ner == 'NENHUM':str_vagas_predict_ner = ent.text\n",
        "        else:str_vagas_predict_ner +='&-&'+ent.text\n",
        "    #df_sentences['vagas NER'].iloc[indice_df] = str_vagas_predict_ner\n",
        "\n",
        "    #NLP11 - AREA\n",
        "    str_area_predict_ner = 'NENHUM'\n",
        "    doc = nlp11(sentence_mod)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == 'AREA':\n",
        "        if str_area_predict_ner == 'NENHUM':str_area_predict_ner = ent.text\n",
        "        else:str_area_predict_ner +='&-&'+ent.text\n",
        "    #df_sentences['area NER'].iloc[indice_df] = str_area_predict_ner\n",
        "\n",
        "    #NLP12 - SUITES\n",
        "    str_suites_predict_ner = 'NENHUM'\n",
        "    doc = nlp12(sentence_mod)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == 'SUITES':\n",
        "        if str_suites_predict_ner == 'NENHUM':str_suites_predict_ner = ent.text\n",
        "        else:str_suites_predict_ner +='&-&'+ent.text\n",
        "    #df_sentences['suites NER'].iloc[indice_df] = str_suites_predict_ner\n",
        "\n",
        "    #NLP12 - BANHEIROS\n",
        "    str_banheiros_predict_ner = 'NENHUM'\n",
        "    doc = nlp12(sentence_mod)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ == 'BANHEIROS':\n",
        "        if str_banheiros_predict_ner == 'NENHUM':str_banheiros_predict_ner = ent.text\n",
        "        else:str_banheiros_predict_ner +='&-&'+ent.text\n",
        "    #df_sentences['banheiros NER'].iloc[indice_df] = str_banheiros_predict_ner\n",
        "\n",
        "    value_suites_regex, value_correct_suites_regex, value_banheiro_regex, value_correct_banheiro_regex, value_vagas_regex, value_correct_vagas_regex, value_quartos_regex, value_correct_quartos_regex='NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM','NENHUM',\n",
        "    value_dorms_regex, value_correct_dorms_regex, value_regex_price_reais, value_area_total_regex, value_correct_area_priv_correct_regex = 'NENHUM','NENHUM','NENHUM','NENHUM','NENHUM',\n",
        "\n",
        "    value_preco_regex, str_preco_regex_end = 'NENHUM', 'NENHUM'\n",
        "    str_area_regex_area = 'NENHUM'\n",
        "\n",
        "    for TAG in ['DORMS', 'QUARTOS', 'VAGAS', 'SUITES', 'BANHEIROS', 'AREA', 'PRECO']:\n",
        "      tag_regex, value ='NULL', []\n",
        "\n",
        "      if TAG == 'PRECO':\n",
        "        s = re.sub('mil', '.000,00', sentence_mod)\n",
        "        value_preco_regex = re.findall('\\(?r\\$\\)?[:\\s*]*\\d+[.,]?[\\d+]*?[.,]?[\\d+]*[.,]?[\\d+]*[.,]?[\\d+]*', s.lower())\n",
        "        #if value_preco_regex != []:value_preco_regex = value_preco_regex[0]\n",
        "        if value_preco_regex != []:\n",
        "          for vp in value_preco_regex:\n",
        "            if str_preco_regex_end == 'NENHUM':\n",
        "              str_preco_regex_end = vp\n",
        "            else: str_preco_regex_end +='&-&'+vp\n",
        "          #df_sentences['Preço All'].iloc[indice_df] = str_preco_regex_end\n",
        "        #df_sentences['preco REGEX'].iloc[indice_df] = value_preco_regex\n",
        "      elif TAG == 'AREA':\n",
        "        value = re.findall('\\S*\\d+[,.]?\\d+?\\s*[m2|m²|m 2|m ²]\\S*', sentence_not_space)\n",
        "        #value_test_correct = value\n",
        "        #value2 = re.findall('\\S*\\d+[,.]?\\d+m?\\s+?m?x\\s+?\\d+[,.]?\\d+m?\\s?m?\\s', sentence_not_space)\n",
        "        #value += value2\n",
        "        value = re.findall('\\d+.?,?\\d+?\\s*?m²', sentence_not_space)\n",
        "        value12 = re.findall('\\d+.?,?\\d+?\\s*?m2', sentence_not_space)\n",
        "        if value12 != []:value +=value12\n",
        "        value13 = re.findall('\\d+.?,?\\d+?\\s*?m ²', sentence_not_space)\n",
        "        if value13 != []:value +=value13\n",
        "        value14 = re.findall('\\d+.?,?\\d+?\\s*?m m', sentence_not_space)\n",
        "        if value14 != []:value +=value14\n",
        "        for vp in value:\n",
        "          if str_area_regex_area == 'NENHUM':\n",
        "            str_area_regex_area = vp\n",
        "          else: str_area_regex_area +='&-&'+vp\n",
        "        #df_sentences['area REGEX'].iloc[indice_df] = str_area_regex_area\n",
        "        continue\n",
        "      else:\n",
        "        if TAG == 'DORMS': tag_regex = 'dorm'\n",
        "        elif TAG == 'QUARTOS': tag_regex = 'quarto'\n",
        "        elif TAG == 'VAGAS': tag_regex = 'vaga'\n",
        "        elif TAG == 'SUITES': tag_regex = 'suite'\n",
        "        elif TAG == 'BANHEIROS': tag_regex = 'banheiro'\n",
        "\n",
        "        value = re.findall('\\S*\\d{1,2}'+f'\\s*?{tag_regex}\\S*', unidecode(sentence_not_space)) #2 dorms\n",
        "        value2 = re.findall(f'\\S*{tag_regex}[\\w]*\\s+?'+'\\d{1,2}\\S*', unidecode(sentence_not_space)) #dorms 2\n",
        "\n",
        "        value += value2\n",
        "    \n",
        "      values = 'NENHUM'\n",
        "      if value != [] and TAG != 'PRECO' and TAG != 'AREA':\n",
        "        for v in value:\n",
        "          v = v.strip()\n",
        "          ns = re.findall('\\d+', v)\n",
        "          if values == 'NENHUM':\n",
        "            values = ns[0]\n",
        "          else:values +='&-&'+ns[0]\n",
        "\n",
        "      area_priv, area_total = 'NENHUM', 'NENHUM'\n",
        "      value_reais = 'NENHUM'\n",
        "      value_correct = 'NENHUM'\n",
        "      if TAG != 'PRECO' and TAG != 'AREA':\n",
        "        list_vs = []\n",
        "        for v in value:\n",
        "          v = v.strip()\n",
        "          list_vs.append((  len(re.findall('\\s', v)), v))\n",
        "        if len(list_vs) > 0:\n",
        "          list_vs.sort()\n",
        "          for vs in list_vs:\n",
        "            val = re.findall('\\d+', vs[1])\n",
        "            if len(val) > 1:\n",
        "              value_correct = 'REVISÃO'\n",
        "              break\n",
        "            else:\n",
        "              if ('m2' not in val[0] or 'm²' not in val[0]) and ('r$' not in val[0] and TAG != 'AREA'): value_correct = val[0]\n",
        "              else: value_correct = 'REVISÃO'\n",
        "              break\n",
        "\n",
        "      value_reais = 'NENHUM'\n",
        "      if TAG == 'PRECO':\n",
        "          list_reais = []\n",
        "          for r in value:\n",
        "            r2 = re.sub('\\s+','',r.replace('r$','').strip())\n",
        "            if len(r2) >= 3:\n",
        "              if r2[-3] != ',': r2 +=',00'\n",
        "              r2 = r2.replace(',','').replace('.','')\n",
        "              list_reais.append((float(r2), r))\n",
        "            else:\n",
        "              try:\n",
        "                r2 +=',00'\n",
        "                r2 = r2.replace(',','').replace('.','')\n",
        "                list_reais.append((float(r2), r))\n",
        "              except:pass\n",
        "          if len(list_reais) > 0:\n",
        "            list_reais.sort()\n",
        "            value_reais = list_reais[-1][1]\n",
        "\n",
        "      elif TAG == 'AREA':\n",
        "          list_area = []\n",
        "          for r in value:\n",
        "            a2 = re.findall('\\d+[,.]?\\d+', r)\n",
        "            if len(re.findall('[.,]', a2[0])) > 1:pass\n",
        "              #list_area = 'REVISÃO'\n",
        "            else:\n",
        "              a2 = a2[0].replace(',','').replace('.','')\n",
        "              a2 = float(a2)\n",
        "              list_area.append(a2)        \n",
        "          if len(list_area) > 0:\n",
        "            list_area.sort()\n",
        "          if len(list_area) == 1:\n",
        "            area_total = list_area[-1]\n",
        "            area_priv = list_area[-1]\n",
        "          elif len(list_area) > 1:\n",
        "            area_total = list_area[-1]\n",
        "            for ap in list_area:\n",
        "              if ap > 0 and ap > 22:\n",
        "                area_priv = ap\n",
        "                break\n",
        "\n",
        "      if TAG == 'DORMS': \n",
        "        value_dorms_regex = values\n",
        "        value_correct_dorms_regex = value_correct\n",
        "      elif TAG == 'PRECO':\n",
        "        value_regex_price_reais = value_reais\n",
        "      elif TAG == 'AREA':\n",
        "        value_area_total_regex = values\n",
        "        value_correct_area_priv_correct_regex = area_priv\n",
        "      elif TAG == 'QUARTOS':\n",
        "        value_quartos_regex = values\n",
        "        value_correct_quartos_regex = value_correct\n",
        "      elif TAG == 'VAGAS':\n",
        "        value_vagas_regex = values\n",
        "        value_correct_vagas_regex = value_correct\n",
        "      elif TAG == 'SUITES':\n",
        "        value_suites_regex = values\n",
        "        value_correct_suites_regex = value_correct\n",
        "      elif TAG == 'BANHEIROS':\n",
        "        value_banheiro_regex = values\n",
        "        value_correct_banheiro_regex = value_correct\n",
        "\n",
        "    df_regex_extract = df_regex_extract.append({'Dorms Show':dorms,'Quartos Show': quartos,'Suites Show':suites,'Vagas Show':vagas,'Banheiros Show':banheiros,'Garagem Show':garagem,'Carros Show':carros,\n",
        "      'Area Show':area,'Area Priv Show':area_priv,'Area Terreno Show':area_terreno,'Area Global Show':area_global,'Area Útil Show':area_util,'Area Construída Show':area_contruida,'Area Total Show':area_total,\n",
        "      'dorms REGEX sentence':dorm_rgx_tag_space,'only dorms REGEX':dorms_rgx_only,'only dorms REGEX 2':dorms_rgx_only2,'dorms unique REGEX':dorms_unique,'only dorms REGEX 3':dorms_rgx_only3,\n",
        "      'quartos REGEX sentence':quarto_rgx_tag_space,'only quartos REGEX':quarto_rgx_only,'only quartos REGEX 2':quartos_rgx_only2,'only quartos REGEX 3':quartos_rgx_only3,'quartos unique REGEX':quartos_unique,\n",
        "      'vagas REGEX sentence':vaga_rgx_tag_space, 'carro REGEX sentence':carro_rgx_tag_space, 'garagem REGEX sentence':garagem_rgx_tag_space,'only vagas REGEX':vagas_rgx_only,'only vagas REGEX 2':vagas_rgx_only2,\n",
        "      'only vagas REGEX 3':vagas_rgx_only3,'only garagem REGEX':garagem_rgx_only,'only garagem REGEX 2':garagem_rgx_only2,'only garagem REGEX 3':garagem_rgx_only3,'only carros REGEX 2':carros_rgx_only2,\n",
        "      'only carros REGEX 3':carros_rgx_only3,'vagas unique REGEX':vagas_unique,'garagem unique REGEX':garagem_unique,'carros unique REGEX':carros_unique,\n",
        "      'suites REGEX sentence':suite_rgx_tag_space,'only suites REGEX':suites_rgx_only,'only suites REGEX 2':suites_rgx_only2,'only suites REGEX 3':suites_rgx_only3,'suites unique REGEX':suites_unique,\n",
        "      'banheiros REGEX sentence':banheiro_rgx_tag_space,'only banheiros REGEX':banheiro_rgx_only,'only banheiros REGEX 2':banheiro_rgx_only2,'only banheiros REGEX 3':banheiro_rgx_only3,'banheiros unique REGEX':banheiros_unique,\n",
        "      'Dorms OU': dorms_or,'Quartos OU': quartos_or,'Suites OU': suites_or,'Banheiros OU': banheiros_or,'Vagas OU': vagas_or,'Garagem OU': garagem_or,'Carros OU': carros_or,'Box OU': box_or,\n",
        "      'only area REGEX':str_area, 'only area priv REGEX':str_area_priv, 'only area terreno REGEX':str_area_priv, 'area REGEX':str_area_regex_area,\n",
        "      'banheiros NER':str_banheiros_predict_ner, 'suites NER':str_suites_predict_ner, 'area NER':str_area_predict_ner, 'vagas NER':str_vagas_predict_ner, 'quartos NER':str_quartos_predict_ner,\n",
        "      'dorms NER':str_dorms_predict_ner, 'preço NER': str_preco_predict_ner, 'dorms REGEX': value_dorms_regex, 'dorms REGEX correct':value_correct_dorms_regex,\n",
        "      'banheiros REGEX':value_banheiro_regex, 'banheiros REGEX correct': value_correct_banheiro_regex, 'suites REGEX': value_suites_regex, 'suites REGEX correct':value_correct_suites_regex,\n",
        "      'vagas REGEX':value_vagas_regex, 'vagas REGEX correct': value_correct_vagas_regex, 'quartos REGEX': value_quartos_regex, 'quartos REGEX correct':value_correct_quartos_regex,\n",
        "      'area REGEX correct':value_area_total_regex, 'area priv REGEX correct': value_correct_area_priv_correct_regex, 'preco REGEX correct':value_regex_price_reais, \n",
        "      'Preço All': str_preco_regex_end, 'preco REGEX': value_preco_regex,\n",
        "      'Sentence':sentence, 'Imobiliária':imob,\n",
        "      'Tipo':tipo, 'Situacao':situacao, 'URL': url, 'Endereco Soup': address_soup, 'Pagina': page_bruto}, ignore_index = True)\n",
        "    list_urls_novos.append(url)\n",
        "    cont_imobs_extraction_now += 1\n",
        "\n",
        "    if cont_imobs_extraction_now % 80 == 0:\n",
        "      print(\"save\")\n",
        "      df_regex_extract.to_csv(f'{name_save_file_csv_and_json}.csv', sep='\\t')\n",
        "      with open(f'{name_save_file_csv_and_json}.json', 'w') as f:\n",
        "        json.dump(list_ner, f)\n",
        "      #data = df_regex_extract.copy()\n",
        "      #data = data[np.isin(data['URL'].to_numpy(), list_urls_novos)]\n",
        "      #data.loc[[x in list_urls_novos for x in data['URL'].tolist()]]\n",
        "      #data = pd.read_csv('./DataFrames/2-df_final_regex-2202-02.csv', sep = '\\t', lineterminator = '\\n')\n",
        "      #client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
        "      #db = client['coop']\n",
        "      #collection = db['imoveis3']\n",
        "      #data.reset_index(inplace=True)\n",
        "      #data_dict = data.to_dict(\"records\")\n",
        "      #collection.insert_many(data_dict)\n",
        "\n",
        "    time.sleep(randint(4,8))\n",
        "  df_regex_extract.to_csv(f'{name_save_file_csv_and_json}.csv', sep='\\t')\n",
        "  with open(f'{name_save_file_csv_and_json}.json', 'w') as f:\n",
        "    json.dump(list_ner, f)\n",
        "  \n",
        "  #salva em permanente os links que foram extraídos\n",
        "  timestamp_att_end = checkTimesIsOpenFile(arquivo_urls_config) #checa o tempo antes de abrir e salvar\n",
        "  urls_config = pd.read_csv(name_urls_config, sep='\\t')\n",
        "  for url_new in list_urls_novos:\n",
        "    for ic in range(urls_config.shape[0]):\n",
        "      if url_new == urls_config['URL'].iloc[ic] and (urls_config['Situacao'].iloc[ic] == 'Em extracao' or urls_config['Situacao'].iloc[ic] == 'Revisao'):\n",
        "        urls_config['Situacao'].iloc[ic] = 'OK'\n",
        "        break\n",
        "  urls_config.to_csv(name_urls_config, sep='\\t')\n",
        "df_regex_extract.to_csv(f'{name_save_file_csv_and_json}.csv', sep='\\t')\n",
        "with open(f'{name_save_file_csv_and_json}.json', 'w') as f:\n",
        "  json.dump(list_ner, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_regex_extract.to_csv(f'{name_save_file_csv_and_json}.csv', sep='\\t')\n",
        "with open(f'{name_save_file_csv_and_json}.json', 'w') as f:\n",
        "  json.dump(list_ner, f)"
      ],
      "metadata": {
        "id": "bYMXFvLn7dc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len('sdfsd fsda fsaddsfgd5sa 0f456sa 0df5as d5f4sd4f as4d5f 4sd5f048f 04sd80f4d f4sd fs d4f 4d5f as50d4fs5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc-zKWQP7Xzb",
        "outputId": "e8109c15-090a-471b-cea5-d0e7189a0c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvyeBanMas29"
      },
      "outputs": [],
      "source": [
        "df_regex_extract.to_csv('/content/gdrive/My Drive/NER/DataFrames/Extracao/df_extraction_new_Parte-1.csv', sep='\\t')\n",
        "with open('/content/gdrive/My Drive/NER/DataFrames/Extracao/json_extraction_new_Parte-1.json', 'w') as f: #17-03;17-03-2;22-03;25-03\n",
        "  json.dump(list_ner, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EktlSO82SWPs",
        "6GOtehp-poxN",
        "oo2BtmPh6ZWD",
        "kW01a9of6e3C",
        "Ah72vDEGqfuy",
        "sCPsRNgjToA2"
      ],
      "name": "2.2-0 - Extração Conteúdo dos Links.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}